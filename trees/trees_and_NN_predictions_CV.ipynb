{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decision trees and NN tryouts on SPR data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and validation data as \n",
    "    month : [ Features | Targets| Difference | Last Choice Targets  ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.getLogger().handlers = []\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../common\")\n",
    "\n",
    "from dataset import load_trainval, LC_TARGET_LABELS, TARGET_LABELS_FRQ, TARGET_LABELS_DIFF\n",
    "from utils import to_yearmonth, TARGET_LABELS, TARGET_LABELS2\n",
    "from utils import target_str_to_labels, decimal_to_dummies, targets_str_to_indices, targets_dec_to_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    u'ind_empleado', u'pais_residencia',\n",
    "    u'sexo', u'age', u'ind_nuevo', u'antiguedad', u'indrel',\n",
    "    u'ult_fec_cli_1t', u'indrel_1mes', u'tiprel_1mes', u'indresi',\n",
    "    u'indext', u'conyuemp', u'canal_entrada', u'indfall', u'nomprov',\n",
    "    u'ind_actividad_cliente', u'renta', u'segmento'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # train_yearmonths_list = [201504, 201505, 201604]\n",
    "# train_yearmonths_list = [201505, 201605]\n",
    "# # train_yearmonths_list = [201505]\n",
    "# #val_yearmonth = [201605]\n",
    "# train_nb_clients = 15000\n",
    "# # train_nb_clients = 1500\n",
    "# #train_df, val_df = load_trainval(train_yearmonths_list, val_yearmonth, train_nb_clients, val_nb_clients=1500)\n",
    "# train_df = load_trainval(train_yearmonths_list, train_nb_clients=train_nb_clients)\n",
    "\n",
    "filename = \"trainval_201505+201602+201605__150000.csv\"\n",
    "train_df = pd.read_csv('../data/generated/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_df[['fecha_dato', 'ncodpers'] + TARGET_LABELS_FRQ.tolist()].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_clients(df1, mask1, mask2, df2=None):\n",
    "    active_clients1 = df1[mask1]['ncodpers'].unique()\n",
    "    if df2 is not None:\n",
    "        active_clients2 = df2[mask2]['ncodpers'].unique()\n",
    "    else:\n",
    "        active_clients2 = df1[mask2]['ncodpers'].unique()\n",
    "    active_clients = list(set(active_clients1) & set(active_clients2)) \n",
    "    \n",
    "    if df2 is not None:\n",
    "        return df1['ncodpers'].isin(active_clients), df2['ncodpers'].isin(active_clients)\n",
    "    return df1['ncodpers'].isin(active_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months_ym_map = {}\n",
    "# months = list(set(train_df['fecha_dato'].unique()) | set(val_df['fecha_dato'].unique()))\n",
    "months = train_df['fecha_dato'].unique()\n",
    "for m in months:\n",
    "    months_ym_map[to_yearmonth(m)] = m\n",
    "\n",
    "        \n",
    "train_months = train_df['fecha_dato'].unique()\n",
    "# val_months = val_df['fecha_dato'].unique()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import get_added_products, remove_last_choice, apk, map7_score\n",
    "from visualization import visualize_train_test, visualize_folds, compare_two_datasets, compare_folds, compare_folds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_features = ['targets_diff', 'targets_logdiff', 'targets_logcount2_diff', 'targets_logcount2', 'targets_logcount1', 'targets_logDec']\n",
    "TARGET_LABELS_FRQ_PREV = [c + '_prev' for c in TARGET_LABELS_FRQ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_XY(current_month, df1, next_year_month, df2, months_ym_map):\n",
    "    month_mask = df1['fecha_dato'] == months_ym_map[current_month]\n",
    "    next_year_month_mask = df2['fecha_dato'] == months_ym_map[next_year_month]\n",
    "    next_year_prev_month_mask = df2['fecha_dato'] == months_ym_map[next_year_month - 1]\n",
    "    \n",
    "    # get common clients from df1 at this month and df2 at next year month\n",
    "    common_clients_mask1, common_clients_mask2 = get_common_clients(df1, month_mask, next_year_month_mask, df2)\n",
    "    common_clients_mask2, common_clients_mask3 = get_common_clients(df2, common_clients_mask2 & next_year_month_mask, next_year_prev_month_mask, df2)\n",
    "        \n",
    "    c1 = df1[common_clients_mask1 & month_mask]['ncodpers'].values\n",
    "    c2 = df2[common_clients_mask2 & next_year_month_mask]['ncodpers'].values\n",
    "    c3 = df2[common_clients_mask3 & next_year_prev_month_mask]['ncodpers'].values\n",
    "    assert (c1 == c2).all() and (c2 == c3).all(), \"Problem with common clients\" \n",
    "    \n",
    "    X = df1[common_clients_mask1 & month_mask][['ncodpers', 'fecha_dato'] + target_features + features + TARGET_LABELS_FRQ.tolist()]            \n",
    "   \n",
    "    if TARGET_LABELS[0] in df2.columns and TARGET_LABELS_DIFF[0] in df2.columns and not df2[next_year_month_mask][TARGET_LABELS].isnull().all().all():\n",
    "        Y = df2[common_clients_mask2 & next_year_month_mask][['ncodpers', 'fecha_dato', 'targets_str', 'lc_targets_str', 'targets_diff'] + TARGET_LABELS + TARGET_LABELS_DIFF.tolist()]    \n",
    "        assert (X['ncodpers'].values == Y['ncodpers'].values).all(), \"There is a problem in alignment\"\n",
    "        Y.index = X.index                \n",
    "    else:\n",
    "        Y = None\n",
    "        \n",
    "    if TARGET_LABELS_FRQ[0] in df2.columns and not df2[next_year_prev_month_mask][TARGET_LABELS].isnull().all().all():\n",
    "        # Add TARGET_LABELS_FRQ from previous month to X:\n",
    "        target_labels_frq = df2[common_clients_mask3 & next_year_prev_month_mask][['ncodpers'] + TARGET_LABELS_FRQ.tolist()]\n",
    "        assert (X['ncodpers'].values == target_labels_frq['ncodpers'].values).all(), \"There is a problem in alignment\"\n",
    "        target_labels_frq = target_labels_frq[TARGET_LABELS_FRQ]\n",
    "        target_labels_frq.columns = TARGET_LABELS_FRQ_PREV\n",
    "        target_labels_frq.index = X.index\n",
    "        X = pd.concat([X, target_labels_frq], axis=1)        \n",
    "\n",
    "    \n",
    "    if LC_TARGET_LABELS[0] in df2.columns:\n",
    "        clients_last_choice = df2[common_clients_mask2 & next_year_month_mask][['ncodpers', 'fecha_dato', 'targets_str'] + LC_TARGET_LABELS.tolist()]\n",
    "    else:\n",
    "        clients_last_choice = None\n",
    "        \n",
    "    return X, Y, clients_last_choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201505\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = train_df if months_ym_map[current_month] in train_months else val_df\n",
    "#df1 = train_df\n",
    "df2 = train_df if months_ym_map[next_year_month] in train_months else val_df\n",
    "#df2 = train_df\n",
    "\n",
    "X, Y, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (X['ncodpers'].values == Y['ncodpers'].values).all(), \"WTF\"\n",
    "assert (X['ncodpers'].values == clients_last_choice['ncodpers'].values).all(), \"WTF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149983, 75)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>targets_diff</th>\n",
       "      <th>targets_logdiff</th>\n",
       "      <th>targets_logcount2_diff</th>\n",
       "      <th>targets_logcount2</th>\n",
       "      <th>targets_logcount1</th>\n",
       "      <th>targets_logDec</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_frq_prev</th>\n",
       "      <th>ind_plan_fin_ult1_frq_prev</th>\n",
       "      <th>ind_pres_fin_ult1_frq_prev</th>\n",
       "      <th>ind_reca_fin_ult1_frq_prev</th>\n",
       "      <th>ind_tjcr_fin_ult1_frq_prev</th>\n",
       "      <th>ind_valo_fin_ult1_frq_prev</th>\n",
       "      <th>ind_viv_fin_ult1_frq_prev</th>\n",
       "      <th>ind_nomina_ult1_frq_prev</th>\n",
       "      <th>ind_nom_pens_ult1_frq_prev</th>\n",
       "      <th>ind_recibo_ult1_frq_prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15889</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>-0.00042</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>14.571618</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.931887</td>\n",
       "      <td>0.94339</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.839655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15893</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.931887</td>\n",
       "      <td>0.94339</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.839655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15895</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>14.781716</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.160345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15897</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.098612</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>14.805207</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.072473</td>\n",
       "      <td>0.160345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15900</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>13.287691</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.94339</td>\n",
       "      <td>0.966357</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.160345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>15916</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>13.234752</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.160345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>15919</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>14.588785</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.94339</td>\n",
       "      <td>0.966357</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.160345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15920</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>14.803952</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.068113</td>\n",
       "      <td>0.94339</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.160345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>15921</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>13.296737</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.931887</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.995141</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.160345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>15922</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>14.586867</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992618</td>\n",
       "      <td>0.987667</td>\n",
       "      <td>0.996794</td>\n",
       "      <td>0.931887</td>\n",
       "      <td>0.94339</td>\n",
       "      <td>0.966357</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.932789</td>\n",
       "      <td>0.927527</td>\n",
       "      <td>0.160345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ncodpers  fecha_dato  targets_diff  targets_logdiff  \\\n",
       "1      15889  2015-05-28          32.0         3.496508   \n",
       "7      15893  2015-05-28           0.0         0.000000   \n",
       "13     15895  2015-05-28           0.0         0.000000   \n",
       "19     15897  2015-05-28          -2.0        -1.098612   \n",
       "25     15900  2015-05-28           0.0         0.000000   \n",
       "31     15916  2015-05-28           0.0         0.000000   \n",
       "37     15919  2015-05-28           0.0         0.000000   \n",
       "43     15920  2015-05-28           0.0         0.000000   \n",
       "49     15921  2015-05-28           0.0         0.000000   \n",
       "55     15922  2015-05-28           0.0         0.000000   \n",
       "\n",
       "    targets_logcount2_diff  targets_logcount2  targets_logcount1  \\\n",
       "1                 -0.00042           0.000047           0.000053   \n",
       "7                  0.00000           0.001413           0.001453   \n",
       "13                 0.00000           0.000002           0.000007   \n",
       "19                 0.00000           0.000002           0.000007   \n",
       "25                 0.00000           0.000261           0.000307   \n",
       "31                 0.00000           0.000007           0.000007   \n",
       "37                 0.00000           0.000058           0.000047   \n",
       "43                 0.00000           0.000052           0.000100   \n",
       "49                 0.00000           0.000002           0.000007   \n",
       "55                 0.00000           0.000064           0.000067   \n",
       "\n",
       "    targets_logDec  ind_empleado  pais_residencia            ...             \\\n",
       "1        14.571618             0                0            ...              \n",
       "7         2.833213             1                0            ...              \n",
       "13       14.781716             2                0            ...              \n",
       "19       14.805207             2                0            ...              \n",
       "25       13.287691             3                0            ...              \n",
       "31       13.234752             3                0            ...              \n",
       "37       14.588785             3                0            ...              \n",
       "43       14.803952             0                0            ...              \n",
       "49       13.296737             0                0            ...              \n",
       "55       14.586867             3                0            ...              \n",
       "\n",
       "    ind_hip_fin_ult1_frq_prev  ind_plan_fin_ult1_frq_prev  \\\n",
       "1                    0.992618                    0.987667   \n",
       "7                    0.992618                    0.987667   \n",
       "13                   0.992618                    0.012333   \n",
       "19                   0.992618                    0.012333   \n",
       "25                   0.992618                    0.987667   \n",
       "31                   0.992618                    0.012333   \n",
       "37                   0.992618                    0.987667   \n",
       "43                   0.992618                    0.987667   \n",
       "49                   0.992618                    0.012333   \n",
       "55                   0.992618                    0.987667   \n",
       "\n",
       "    ind_pres_fin_ult1_frq_prev  ind_reca_fin_ult1_frq_prev  \\\n",
       "1                     0.996794                    0.931887   \n",
       "7                     0.996794                    0.931887   \n",
       "13                    0.996794                    0.068113   \n",
       "19                    0.996794                    0.068113   \n",
       "25                    0.996794                    0.068113   \n",
       "31                    0.996794                    0.068113   \n",
       "37                    0.996794                    0.068113   \n",
       "43                    0.996794                    0.068113   \n",
       "49                    0.996794                    0.931887   \n",
       "55                    0.996794                    0.931887   \n",
       "\n",
       "    ind_tjcr_fin_ult1_frq_prev  ind_valo_fin_ult1_frq_prev  \\\n",
       "1                      0.94339                    0.033643   \n",
       "7                      0.94339                    0.033643   \n",
       "13                     0.05661                    0.033643   \n",
       "19                     0.05661                    0.033643   \n",
       "25                     0.94339                    0.966357   \n",
       "31                     0.05661                    0.033643   \n",
       "37                     0.94339                    0.966357   \n",
       "43                     0.94339                    0.033643   \n",
       "49                     0.05661                    0.033643   \n",
       "55                     0.94339                    0.966357   \n",
       "\n",
       "    ind_viv_fin_ult1_frq_prev  ind_nomina_ult1_frq_prev  \\\n",
       "1                    0.995141                  0.932789   \n",
       "7                    0.995141                  0.932789   \n",
       "13                   0.995141                  0.932789   \n",
       "19                   0.995141                  0.932789   \n",
       "25                   0.995141                  0.932789   \n",
       "31                   0.995141                  0.932789   \n",
       "37                   0.995141                  0.932789   \n",
       "43                   0.995141                  0.932789   \n",
       "49                   0.995141                  0.932789   \n",
       "55                   0.004859                  0.932789   \n",
       "\n",
       "    ind_nom_pens_ult1_frq_prev  ind_recibo_ult1_frq_prev  \n",
       "1                     0.927527                  0.839655  \n",
       "7                     0.927527                  0.839655  \n",
       "13                    0.927527                  0.160345  \n",
       "19                    0.072473                  0.160345  \n",
       "25                    0.927527                  0.160345  \n",
       "31                    0.927527                  0.160345  \n",
       "37                    0.927527                  0.160345  \n",
       "43                    0.927527                  0.160345  \n",
       "49                    0.927527                  0.160345  \n",
       "55                    0.927527                  0.160345  \n",
       "\n",
       "[10 rows x 75 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print X.shape\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149983, 53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>targets_str</th>\n",
       "      <th>lc_targets_str</th>\n",
       "      <th>ind_ahor_fin_ult1_diff</th>\n",
       "      <th>ind_aval_fin_ult1_diff</th>\n",
       "      <th>ind_cco_fin_ult1_diff</th>\n",
       "      <th>ind_cder_fin_ult1_diff</th>\n",
       "      <th>ind_cno_fin_ult1_diff</th>\n",
       "      <th>ind_ctju_fin_ult1_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_diff</th>\n",
       "      <th>ind_plan_fin_ult1_diff</th>\n",
       "      <th>ind_pres_fin_ult1_diff</th>\n",
       "      <th>ind_reca_fin_ult1_diff</th>\n",
       "      <th>ind_tjcr_fin_ult1_diff</th>\n",
       "      <th>ind_valo_fin_ult1_diff</th>\n",
       "      <th>ind_viv_fin_ult1_diff</th>\n",
       "      <th>ind_nomina_ult1_diff</th>\n",
       "      <th>ind_nom_pens_ult1_diff</th>\n",
       "      <th>ind_recibo_ult1_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>001000001000000000110000</td>\n",
       "      <td>001000001000000000010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15988</td>\n",
       "      <td>001000000000000000100000</td>\n",
       "      <td>001000000000000000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16525</td>\n",
       "      <td>001010001000110100100111</td>\n",
       "      <td>001010000000110100100111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16680</td>\n",
       "      <td>000010000000000011000111</td>\n",
       "      <td>000000000000000011000111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16826</td>\n",
       "      <td>001000000000000000001001</td>\n",
       "      <td>001000000000000000001000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17151</td>\n",
       "      <td>000010010000001001110111</td>\n",
       "      <td>000010010000001001010111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17231</td>\n",
       "      <td>001000000000000001110001</td>\n",
       "      <td>001000000000000001010001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17458</td>\n",
       "      <td>001000000000000000010000</td>\n",
       "      <td>000000000000000000010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17525</td>\n",
       "      <td>000010011001010000100110</td>\n",
       "      <td>000010011001010000100001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17528</td>\n",
       "      <td>000010000001100001110111</td>\n",
       "      <td>000010000001100001110011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fecha_dato  ncodpers               targets_str  \\\n",
       "1     2016-05-28     15889  001000001000000000110000   \n",
       "139   2016-05-28     15988  001000000000000000100000   \n",
       "433   2016-05-28     16525  001010001000110100100111   \n",
       "589   2016-05-28     16680  000010000000000011000111   \n",
       "655   2016-05-28     16826  001000000000000000001001   \n",
       "811   2016-05-28     17151  000010010000001001110111   \n",
       "877   2016-05-28     17231  001000000000000001110001   \n",
       "1039  2016-05-28     17458  001000000000000000010000   \n",
       "1111  2016-05-28     17525  000010011001010000100110   \n",
       "1123  2016-05-28     17528  000010000001100001110111   \n",
       "\n",
       "                lc_targets_str  ind_ahor_fin_ult1_diff  \\\n",
       "1     001000001000000000010000                     0.0   \n",
       "139   001000000000000000000000                     0.0   \n",
       "433   001010000000110100100111                     0.0   \n",
       "589   000000000000000011000111                     0.0   \n",
       "655   001000000000000000001000                     0.0   \n",
       "811   000010010000001001010111                     0.0   \n",
       "877   001000000000000001010001                     0.0   \n",
       "1039  000000000000000000010000                     0.0   \n",
       "1111  000010011001010000100001                     0.0   \n",
       "1123  000010000001100001110011                     0.0   \n",
       "\n",
       "      ind_aval_fin_ult1_diff  ind_cco_fin_ult1_diff  ind_cder_fin_ult1_diff  \\\n",
       "1                        0.0                    0.0                     0.0   \n",
       "139                      0.0                    0.0                     0.0   \n",
       "433                      0.0                    0.0                     0.0   \n",
       "589                      0.0                    0.0                     0.0   \n",
       "655                      0.0                    0.0                     0.0   \n",
       "811                      0.0                    0.0                     0.0   \n",
       "877                      0.0                    0.0                     0.0   \n",
       "1039                     0.0                    1.0                     0.0   \n",
       "1111                     0.0                    0.0                     0.0   \n",
       "1123                     0.0                    0.0                     0.0   \n",
       "\n",
       "      ind_cno_fin_ult1_diff  ind_ctju_fin_ult1_diff          ...           \\\n",
       "1                       0.0                     0.0          ...            \n",
       "139                     0.0                     0.0          ...            \n",
       "433                     0.0                     0.0          ...            \n",
       "589                     1.0                     0.0          ...            \n",
       "655                     0.0                     0.0          ...            \n",
       "811                     0.0                     0.0          ...            \n",
       "877                     0.0                     0.0          ...            \n",
       "1039                    0.0                     0.0          ...            \n",
       "1111                    0.0                     0.0          ...            \n",
       "1123                    0.0                     0.0          ...            \n",
       "\n",
       "      ind_hip_fin_ult1_diff  ind_plan_fin_ult1_diff  ind_pres_fin_ult1_diff  \\\n",
       "1                       0.0                     0.0                     0.0   \n",
       "139                     0.0                     0.0                     0.0   \n",
       "433                     0.0                     0.0                     0.0   \n",
       "589                     0.0                     0.0                     0.0   \n",
       "655                     0.0                     0.0                     0.0   \n",
       "811                     0.0                     0.0                     0.0   \n",
       "877                     0.0                     0.0                     0.0   \n",
       "1039                    0.0                     0.0                     0.0   \n",
       "1111                    0.0                     0.0                     0.0   \n",
       "1123                    0.0                     0.0                     0.0   \n",
       "\n",
       "      ind_reca_fin_ult1_diff  ind_tjcr_fin_ult1_diff  ind_valo_fin_ult1_diff  \\\n",
       "1                        0.0                     1.0                     0.0   \n",
       "139                      0.0                     1.0                     0.0   \n",
       "433                      0.0                     0.0                     0.0   \n",
       "589                      0.0                     0.0                     0.0   \n",
       "655                      0.0                     0.0                     0.0   \n",
       "811                      0.0                     1.0                     0.0   \n",
       "877                      0.0                     1.0                     0.0   \n",
       "1039                     0.0                     0.0                     0.0   \n",
       "1111                     0.0                     0.0                     0.0   \n",
       "1123                     0.0                     0.0                     0.0   \n",
       "\n",
       "      ind_viv_fin_ult1_diff  ind_nomina_ult1_diff  ind_nom_pens_ult1_diff  \\\n",
       "1                       0.0                   0.0                     0.0   \n",
       "139                     0.0                   0.0                     0.0   \n",
       "433                     0.0                   0.0                     0.0   \n",
       "589                     0.0                   0.0                     0.0   \n",
       "655                     0.0                   0.0                     0.0   \n",
       "811                     0.0                   0.0                     0.0   \n",
       "877                     0.0                   0.0                     0.0   \n",
       "1039                    0.0                   0.0                     0.0   \n",
       "1111                    0.0                   1.0                     1.0   \n",
       "1123                    0.0                   1.0                     0.0   \n",
       "\n",
       "      ind_recibo_ult1_diff  \n",
       "1                      0.0  \n",
       "139                    0.0  \n",
       "433                    0.0  \n",
       "589                    0.0  \n",
       "655                    1.0  \n",
       "811                    0.0  \n",
       "877                    0.0  \n",
       "1039                   0.0  \n",
       "1111                   0.0  \n",
       "1123                   0.0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print Y.shape\n",
    "Y[Y['targets_diff'] > 0][['fecha_dato', 'ncodpers', 'targets_str', 'lc_targets_str'] + TARGET_LABELS_DIFF.tolist() ].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149983, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>targets_str</th>\n",
       "      <th>lc_ind_ahor_fin_ult1</th>\n",
       "      <th>lc_ind_aval_fin_ult1</th>\n",
       "      <th>lc_ind_cco_fin_ult1</th>\n",
       "      <th>lc_ind_cder_fin_ult1</th>\n",
       "      <th>lc_ind_cno_fin_ult1</th>\n",
       "      <th>lc_ind_ctju_fin_ult1</th>\n",
       "      <th>lc_ind_ctma_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>lc_ind_hip_fin_ult1</th>\n",
       "      <th>lc_ind_plan_fin_ult1</th>\n",
       "      <th>lc_ind_pres_fin_ult1</th>\n",
       "      <th>lc_ind_reca_fin_ult1</th>\n",
       "      <th>lc_ind_tjcr_fin_ult1</th>\n",
       "      <th>lc_ind_valo_fin_ult1</th>\n",
       "      <th>lc_ind_viv_fin_ult1</th>\n",
       "      <th>lc_ind_nomina_ult1</th>\n",
       "      <th>lc_ind_nom_pens_ult1</th>\n",
       "      <th>lc_ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15889</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000001000000000110000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15893</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>000000000000000000010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15895</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000000000100101110001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15897</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>000010010000110101110011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15900</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>000000010000000001000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>15916</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>000010001000100101110001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>15919</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000000001000001000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>15920</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000010000000001010001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>15921</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000010001010100110001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>15922</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000010000000000001001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ncodpers  fecha_dato               targets_str  lc_ind_ahor_fin_ult1  \\\n",
       "5      15889  2016-05-28  001000001000000000110000                   0.0   \n",
       "11     15893  2016-05-28  000000000000000000010000                   0.0   \n",
       "17     15895  2016-05-28  001000000000100101110001                   0.0   \n",
       "23     15897  2016-05-28  000010010000110101110011                   0.0   \n",
       "29     15900  2016-05-28  000000010000000001000001                   0.0   \n",
       "35     15916  2016-05-28  000010001000100101110001                   0.0   \n",
       "41     15919  2016-05-28  001000000001000001000001                   0.0   \n",
       "47     15920  2016-05-28  001000010000000001010001                   0.0   \n",
       "53     15921  2016-05-28  001000010001010100110001                   0.0   \n",
       "59     15922  2016-05-28  001000010000000000001001                   0.0   \n",
       "\n",
       "    lc_ind_aval_fin_ult1  lc_ind_cco_fin_ult1  lc_ind_cder_fin_ult1  \\\n",
       "5                    0.0                  1.0                   0.0   \n",
       "11                   0.0                  0.0                   0.0   \n",
       "17                   0.0                  1.0                   0.0   \n",
       "23                   0.0                  0.0                   0.0   \n",
       "29                   0.0                  0.0                   0.0   \n",
       "35                   0.0                  0.0                   0.0   \n",
       "41                   0.0                  1.0                   0.0   \n",
       "47                   0.0                  1.0                   0.0   \n",
       "53                   0.0                  1.0                   0.0   \n",
       "59                   0.0                  1.0                   0.0   \n",
       "\n",
       "    lc_ind_cno_fin_ult1  lc_ind_ctju_fin_ult1  lc_ind_ctma_fin_ult1  \\\n",
       "5                   0.0                   0.0                   0.0   \n",
       "11                  0.0                   0.0                   0.0   \n",
       "17                  0.0                   0.0                   0.0   \n",
       "23                  1.0                   0.0                   0.0   \n",
       "29                  1.0                   0.0                   0.0   \n",
       "35                  1.0                   0.0                   0.0   \n",
       "41                  0.0                   0.0                   0.0   \n",
       "47                  0.0                   0.0                   0.0   \n",
       "53                  0.0                   0.0                   0.0   \n",
       "59                  0.0                   0.0                   0.0   \n",
       "\n",
       "           ...          lc_ind_hip_fin_ult1  lc_ind_plan_fin_ult1  \\\n",
       "5          ...                          0.0                   0.0   \n",
       "11         ...                          0.0                   0.0   \n",
       "17         ...                          0.0                   1.0   \n",
       "23         ...                          0.0                   1.0   \n",
       "29         ...                          0.0                   0.0   \n",
       "35         ...                          0.0                   1.0   \n",
       "41         ...                          0.0                   0.0   \n",
       "47         ...                          0.0                   0.0   \n",
       "53         ...                          0.0                   1.0   \n",
       "59         ...                          0.0                   0.0   \n",
       "\n",
       "    lc_ind_pres_fin_ult1  lc_ind_reca_fin_ult1  lc_ind_tjcr_fin_ult1  \\\n",
       "5                    0.0                   0.0                   0.0   \n",
       "11                   0.0                   0.0                   0.0   \n",
       "17                   0.0                   1.0                   1.0   \n",
       "23                   0.0                   1.0                   1.0   \n",
       "29                   0.0                   1.0                   0.0   \n",
       "35                   0.0                   1.0                   1.0   \n",
       "41                   0.0                   1.0                   0.0   \n",
       "47                   0.0                   1.0                   0.0   \n",
       "53                   0.0                   0.0                   1.0   \n",
       "59                   0.0                   0.0                   0.0   \n",
       "\n",
       "    lc_ind_valo_fin_ult1  lc_ind_viv_fin_ult1  lc_ind_nomina_ult1  \\\n",
       "5                    1.0                  0.0                 0.0   \n",
       "11                   1.0                  0.0                 0.0   \n",
       "17                   1.0                  0.0                 0.0   \n",
       "23                   1.0                  0.0                 0.0   \n",
       "29                   0.0                  0.0                 0.0   \n",
       "35                   1.0                  0.0                 0.0   \n",
       "41                   0.0                  0.0                 0.0   \n",
       "47                   1.0                  0.0                 0.0   \n",
       "53                   1.0                  0.0                 0.0   \n",
       "59                   0.0                  1.0                 0.0   \n",
       "\n",
       "    lc_ind_nom_pens_ult1  lc_ind_recibo_ult1  \n",
       "5                    0.0                 0.0  \n",
       "11                   0.0                 0.0  \n",
       "17                   0.0                 1.0  \n",
       "23                   1.0                 1.0  \n",
       "29                   0.0                 1.0  \n",
       "35                   0.0                 1.0  \n",
       "41                   0.0                 1.0  \n",
       "47                   0.0                 1.0  \n",
       "53                   0.0                 1.0  \n",
       "59                   0.0                 1.0  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print clients_last_choice.shape\n",
    "clients_last_choice.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another train/predict + CV implementation\n",
    "\n",
    "### Input\n",
    "\n",
    "- `X` : `[nb_samples, nb_features]` shaped pd.DataFrame\n",
    "    - `features_masks_list` : `{fm1_name: features_mask_1, fm2_name: features_mask_2, ...]` with `features_mask_i` is a list of feature column names. They can oversect.\n",
    "    \n",
    "- `Y` : `[nb_samples, nb_labels]` shaped pd.DataFrame\n",
    "    - `labels_masks_list` : `{lm1_name: labels_mask_1, lm2_name: labels_mask_2, ...}` with `labels_mask_i` is a list of labels column names. They can oversect.\n",
    "\n",
    "- `samples_masks_list` : `[samples_mask_1, samples_mask_2, ...]` with samples_mask_i is a function to produce a boolean pd.DataFrame . Used only for training. \n",
    "\n",
    "\n",
    "- Set of models `models` : list of functions to create a model, e.g. `[create_RF, create_NN, create_GBT]`\n",
    "\n",
    "\n",
    "### Training phase\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Merge\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11659bed0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEDCAYAAAA7jc+ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHPRmZplz0JuZZe7/AWe5/pXQ6xv1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11659cdd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = Y['targets_diff'] > 0\n",
    "targets_index_counts = np.zeros((len(TARGET_LABELS)))\n",
    "for i, c in enumerate(TARGET_LABELS):\n",
    "    s = (Y[mask][c] > 0).sum()\n",
    "    targets_index_counts[i] = s\n",
    "\n",
    "targets_index_counts = pd.Series(targets_index_counts)\n",
    "targets_index_counts.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_masks_list = [\n",
    "#     lambda x, y:  ~(x['targets_diff'].isin([0])) | ~(y['targets_diff'].isin([0])), \n",
    "#     lambda x, y:  (x['targets_diff'] > 0) | (y['targets_diff'] > 0), \n",
    "    lambda x, y:  (y['targets_diff'] > 0), \n",
    "]\n",
    "\n",
    "features_masks_dict = {\n",
    "#     'fm_all': features + target_features + TARGET_LABELS_FRQ.tolist() + TARGET_LABELS_FRQ_PREV,\n",
    "    'fm0': features + TARGET_LABELS_FRQ.tolist(),\n",
    "#     'fm1': ['pais_residencia', 'sexo', 'age', 'ind_nuevo', 'segmento', 'ind_empleado', 'ind_actividad_cliente', 'indresi'],\n",
    "    'fm1': TARGET_LABELS_FRQ_PREV,\n",
    "    'fm2': target_features,\n",
    "    'fm3': ['pais_residencia', 'sexo', 'age', 'segmento', 'renta'],\n",
    "#     'fm4': ['pais_residencia', 'sexo', 'age', 'renta', 'targets_logdiff', 'targets_logcount2_diff','targets_logcount2','targets_logcount1'],\n",
    "    'fm5': ['nomprov', 'ind_nuevo', 'renta', 'ind_actividad_cliente', 'canal_entrada'],\n",
    "#     'fm6': TARGET_LABELS_FRQ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "def create_RF(input_shape, output_shape):        \n",
    "    # https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "    return RandomForestClassifier(n_estimators=100, \n",
    "#                                   min_samples_split=100,\n",
    "#                                   min_samples_leaf=25,\n",
    "#                                   max_depth=10\n",
    "                                  max_features=1.0, \n",
    "#                                   oob_score=True,\n",
    "#                                   bootstrap=True,\n",
    "                                  n_jobs=-1\n",
    "                                 )\n",
    "\n",
    "def create_ET(input_shape, output_shape):\n",
    "    return ExtraTreesClassifier(n_estimators=100,\n",
    "#                                   min_samples_leaf=25,\n",
    "#                                   max_depth=10\n",
    "                                  max_features=1.0, \n",
    "                                  oob_score=True,\n",
    "                                  bootstrap=True,\n",
    "                                  n_jobs=-1\n",
    "\n",
    "                               )\n",
    "\n",
    "def create_GB(input_shape, output_shape):\n",
    "    return GradientBoostingClassifier(n_estimators=75)\n",
    "\n",
    "\n",
    "def create_NN0(input_shape, output_shape):\n",
    "        \n",
    "    assert len(input_shape) == 2, \"Input shape should be 2D\"\n",
    "    assert len(output_shape) == 2, \"Input shape should be 2D\"\n",
    "    n_features = input_shape[1]\n",
    "    output_dim = output_shape[1]\n",
    "    \n",
    "    def create_model(input_dim=n_features, output_dim=output_dim):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, init='uniform', input_shape=(input_dim,), activation='relu'))\n",
    "        model.add(Dropout(0.15))\n",
    "#         model.add(Dense(output_dim, activation='sigmoid'))\n",
    "        model.add(Dense(output_dim, activation='softmax'))\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "        model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    return KerasClassifier(build_fn=create_model, nb_epoch=200, batch_size=2000, verbose=0)\n",
    "    \n",
    "\n",
    "models_dict = {\n",
    "    'rf': create_RF,\n",
    "    'et': create_ET,\n",
    "    'gb': create_GB,\n",
    "#     'nn0': create_NN0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46667\n",
      "Int64Index([2, 4, 7, 8, 11, 12, 13, 15, 17, 18, 19, 21, 22, 23], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print mask.sum()\n",
    "print targets_index_counts[targets_index_counts > 100].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_0  <=>  [2, 4, 7, 8, 11, 12, 13, 15, 17, 18, 19, 21, 22, 23] <==> ['Current Accounts' 'Payroll Account' 'particular Account'\n",
      " 'particular Plus Account' 'Long-term deposits' 'e-account' 'Funds'\n",
      " 'Pensions (plan fin)' 'Taxes' 'Credit Card' 'Securities' 'Payroll'\n",
      " 'Pensions' 'Direct Debit']\n",
      "lm_1  <=>  [2] <==> ['Current Accounts']\n",
      "lm_2  <=>  [4] <==> ['Payroll Account']\n",
      "lm_3  <=>  [7] <==> ['particular Account']\n",
      "lm_4  <=>  [8] <==> ['particular Plus Account']\n",
      "lm_5  <=>  [11] <==> ['Long-term deposits']\n",
      "lm_6  <=>  [12] <==> ['e-account']\n",
      "lm_7  <=>  [13] <==> ['Funds']\n",
      "lm_8  <=>  [15] <==> ['Pensions (plan fin)']\n",
      "lm_9  <=>  [17] <==> ['Taxes']\n",
      "lm_10  <=>  [18] <==> ['Credit Card']\n",
      "lm_11  <=>  [19] <==> ['Securities']\n",
      "lm_12  <=>  [21] <==> ['Payroll']\n",
      "lm_13  <=>  [22] <==> ['Pensions']\n",
      "lm_14  <=>  [23] <==> ['Direct Debit']\n",
      "{'lm_12': array(['ind_nomina_ult1'], \n",
      "      dtype='|S17'), 'lm_13': array(['ind_nom_pens_ult1'], \n",
      "      dtype='|S17'), 'lm_10': array(['ind_tjcr_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_11': array(['ind_valo_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_14': array(['ind_recibo_ult1'], \n",
      "      dtype='|S17'), 'lm_8': array(['ind_plan_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_9': array(['ind_reca_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_0': array(['ind_cco_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctop_fin_ult1',\n",
      "       'ind_ctpp_fin_ult1', 'ind_dela_fin_ult1', 'ind_ecue_fin_ult1',\n",
      "       'ind_fond_fin_ult1', 'ind_plan_fin_ult1', 'ind_reca_fin_ult1',\n",
      "       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_nomina_ult1',\n",
      "       'ind_nom_pens_ult1', 'ind_recibo_ult1'], \n",
      "      dtype='|S17'), 'lm_1': array(['ind_cco_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_2': array(['ind_cno_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_3': array(['ind_ctop_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_4': array(['ind_ctpp_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_5': array(['ind_dela_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_6': array(['ind_ecue_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_7': array(['ind_fond_fin_ult1'], \n",
      "      dtype='|S17')}\n"
     ]
    }
   ],
   "source": [
    "NP_TARGET_LABELS = np.array(TARGET_LABELS)\n",
    "target_labels = NP_TARGET_LABELS\n",
    "\n",
    "common_groups = [\n",
    "    [2, 4, 7, 8, 11, 12, 13, 15, 17, 18, 19, 21, 22, 23],\n",
    "#     [2, 4],    \n",
    "#     [2, 6, 7, 8],\n",
    "#     [2, 18, 23, 12], \n",
    "#     [21, 22],\n",
    "#     [2, 12, 18],\n",
    "#     [2, 12, 23],\n",
    "#     [2, 18, 23],\n",
    "#     [18, 23, 21, 22],\n",
    "#     [21, 23, 22, 4],\n",
    "#     [3, 4], \n",
    "#     [22, 7, 8, 23],\n",
    "#     [0, 1, 14, 15, 17]\n",
    "]\n",
    "\n",
    "common_groups += [[i] for i in [2, 4, 7, 8, 11, 12, 13, 15, 17, 18, 19, 21, 22, 23]]\n",
    "\n",
    "\n",
    "\n",
    "def flatten(array):\n",
    "    out = []\n",
    "    for item in array:\n",
    "        out += item\n",
    "    return out\n",
    "\n",
    "others = list(set(range(24)) - set(flatten(common_groups)))\n",
    "\n",
    "# for i, a in enumerate(zip(TARGET_LABELS2, TARGET_LABELS)):\n",
    "#     print i, a\n",
    "    \n",
    "s = set({})\n",
    "labels_masks_dict = {}\n",
    "for i, g in enumerate(common_groups):\n",
    "    print 'lm_%i' % i, \" <=> \", g, \"<==>\", TARGET_LABELS2[g]\n",
    "    labels_masks_dict['lm_%i' % i] = target_labels[g]\n",
    "    s |= set(g)\n",
    "    \n",
    "# print 'lm_others', \"<=>\", others, \"<==>\", TARGET_LABELS2[others]\n",
    "# labels_masks_dict['lm_others'] = target_labels[others]\n",
    "# s |= set(others)\n",
    "\n",
    "# assert len(s) == len(target_labels), \"Sum is not equal 24, s=%i\" % s\n",
    "print labels_masks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'et': [(None, None, 'lm_0')],\n",
       " 'gb': [(None, None, 'lm_12'),\n",
       "  (None, None, 'lm_13'),\n",
       "  (None, None, 'lm_10'),\n",
       "  (None, None, 'lm_11'),\n",
       "  (None, None, 'lm_14'),\n",
       "  (None, None, 'lm_8'),\n",
       "  (None, None, 'lm_9'),\n",
       "  (None, None, 'lm_1'),\n",
       "  (None, None, 'lm_2'),\n",
       "  (None, None, 'lm_3'),\n",
       "  (None, None, 'lm_4'),\n",
       "  (None, None, 'lm_5'),\n",
       "  (None, None, 'lm_6'),\n",
       "  (None, None, 'lm_7')],\n",
       " 'rf': [(None, None, 'lm_0')]}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {model_name: [(samples_mask_code, features_mask_name, labels_mask_name), ...]}\n",
    "models_pipelines = {\n",
    "    'gb' : [(None, None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) == 1],    \n",
    "    'rf' : [(None, None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) > 1],\n",
    "    'et' : [(None, None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) > 1],\n",
    "}\n",
    "models_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from trainval import train_all, predict_all, probas_to_indices, score_estimators\n",
    "from utils import map7_score0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103316, 75) (103316, 53) (103316, 27)\n",
      "(46667, 75) (46667, 53) (46667, 27)\n"
     ]
    }
   ],
   "source": [
    "ll = 140000\n",
    "# ll = 1100\n",
    "\n",
    "mask = (X.index.isin((X.index[:ll//3]))) | (X.index.isin((X.index[2*ll//3:])))\n",
    "\n",
    "X1 = X[mask]\n",
    "Y1 = Y[mask]\n",
    "clc1 = clients_last_choice[mask]\n",
    "print X1.shape, Y1.shape, clc1.shape\n",
    "\n",
    "mask = X.index.isin(X.index[ll//3:2*ll//3])\n",
    "X2 = X[mask]\n",
    "Y2 = Y[mask]\n",
    "clc2 = clients_last_choice[mask]\n",
    "print X2.shape, Y2.shape, clc2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# res = Y1[labels_masks_dict['lm_0']].apply(dummies_to_decimal, axis=1)\n",
    "# res = pd.get_dummies(res)\n",
    "# res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import dummies_to_decimal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def prepare_to_fit(X_train, Y_train):    \n",
    "    x_train = X_train.values\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    y_train = Y_train.apply(dummies_to_decimal, axis=1)\n",
    "    y_train = pd.get_dummies(y_train)\n",
    "    y_train = y_train.values    \n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def prepare_to_test(X_val, Y_val=None):\n",
    "    x_val = X_val.values\n",
    "    x_val = StandardScaler().fit_transform(x_val)\n",
    "    if Y_val is not None:\n",
    "        y_val = Y_val.apply(dummies_to_decimal, axis=1)\n",
    "        y_val = pd.get_dummies(y_val)\n",
    "        y_val = y_val.values \n",
    "    else:\n",
    "        y_val = None\n",
    "    return x_val, y_val\n",
    "\n",
    "\n",
    "def probas_to_labels_probas(y_probas, class_indices, labels):\n",
    "    l = len(labels)\n",
    "    out = np.zeros((len(y_probas), l))\n",
    "    i = 0\n",
    "    for probas in y_probas:\n",
    "        if np.sum(probas) > 0:\n",
    "            pr = np.zeros((l,))\n",
    "            for index, p in zip(class_indices, probas):\n",
    "                dummies_str = decimal_to_dummies(index, l)\n",
    "                pr += p * np.array([float(v) for v in dummies_str])\n",
    "            out[i, :] = pr    \n",
    "        i += 1\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_kwargs = {\n",
    "    'samples_masks_list': samples_masks_list, \n",
    "    'features_masks_dict': features_masks_dict, \n",
    "    'labels_masks_dict': labels_masks_dict, \n",
    "    'models_dict': models_dict,\n",
    "    'labels': target_labels,\n",
    "    'transform_proba_func': probas_to_indices,\n",
    "#     'prepare_to_fit_func': prepare_to_fit,\n",
    "#     'prepare_to_test_func': prepare_to_test,   \n",
    "#     'probas_to_labels_probas_func': probas_to_labels_probas,\n",
    "    'threshold': 0.15,\n",
    "    'n_highest': 7,\n",
    "    'mode': 'sum',\n",
    "    'verbose': False,\n",
    "    'models_pipelines': models_pipelines,\n",
    "    'return_probas': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.644397\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.621925\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.684179\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.903735\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.778621\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.966596\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.781962\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.348618\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.344063\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.620103\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.600364\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.804130\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.890677\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.915578\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.705436\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.942606\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.830853\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.829031\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.827513\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.933799\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.813544\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.979046\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.911023\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.646827\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.649560\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.875797\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.886122\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.981172\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.985727\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.946857\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.903128\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.966292\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.652596\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.636502\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.684179\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.904343\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.767385\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.966596\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.781354\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.215002\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.208928\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.624962\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.609475\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.795020\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.887944\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.921956\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.738840\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.943213\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.840875\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.836623\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.854540\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.978439\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.829942\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997874\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.955664\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.998482\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.998785\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.873671\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.881871\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.981172\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.989979\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.962041\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.932584\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990282\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.901609\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.905861\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.868509\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992712\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.836927\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.999089\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990586\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.714850\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.718797\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.951716\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.975402\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990890\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.994534\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.993319\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.952930\n",
      "INFO:root:-- Process : sample_mask=3293/103316, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997267\n"
     ]
    }
   ],
   "source": [
    "estimators = train_all(X1, Y1, **_kwargs)\n",
    "\n",
    "#print estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'et': 0.58475554205891278,\n",
       " 'gb': 0.86716411435512564,\n",
       " 'rf': 0.58402672335256611}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = defaultdict(list)\n",
    "for e in estimators:\n",
    "    accuracies[e[0][2]].append(e[2])\n",
    "\n",
    "mean_accuracy = {}\n",
    "for key in accuracies:\n",
    "    accuracy_list = accuracies[key]\n",
    "    mean_accuracy[key] = sum(accuracy_list)/len(accuracy_list)\n",
    "    \n",
    "mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_12 -> 0.936379025864\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_13 -> 0.931279062292\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_10 -> 0.825615531318\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_11 -> 0.973964471682\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_14 -> 0.142948978936\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_8 -> 0.989435789744\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_9 -> 0.930164784537\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_0 -> 0.0398140013286\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_0 -> 0.0282426554096\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_1 -> 0.755351747488\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_2 -> 0.845243962543\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_3 -> 0.888165084535\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_4 -> 0.90740780423\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_5 -> 0.94707180663\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_6 -> 0.883450832494\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_7 -> 0.982707266377\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_12 -> 0.580795851458\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_13 -> 0.59613859901\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_10 -> 0.737609017078\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_11 -> 0.886493667902\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_14 -> 0.141277562303\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_8 -> 0.993978614438\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_9 -> 0.928750508925\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_0 -> 0.0092356483168\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_0 -> 0.00254998178584\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_1 -> 0.253005335676\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_2 -> 0.163005978529\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_3 -> 0.871579488718\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_4 -> 0.876636595453\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_5 -> 0.90320783423\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_6 -> 0.89414361326\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_7 -> 0.982107270662\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_12 -> 0.809308504939\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_13 -> 0.784130113356\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_10 -> 0.900657852444\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_11 -> 0.972785908672\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_14 -> 0.170634495468\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_8 -> 0.992371483061\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_9 -> 0.928579081578\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_0 -> 0.0246426811237\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_0 -> 0.0287140806137\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_1 -> 0.666702380697\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_2 -> 0.745751816058\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_3 -> 0.912100627853\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_4 -> 0.910064928108\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_5 -> 0.949321790559\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_6 -> 0.879272290912\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_7 -> 0.981550131785\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_12 -> 0.959821715559\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_13 -> 0.954828894079\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_10 -> 0.967514517753\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_11 -> 0.995071463775\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_14 -> 0.349176077314\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_8 -> 0.987014378469\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_9 -> 0.991042921122\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_0 -> 0.0709923500546\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_0 -> 0.277198020014\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_1 -> 0.831144063257\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_2 -> 0.967450232498\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_3 -> 0.993935757602\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_4 -> 0.987357233163\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_5 -> 0.962500267855\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_6 -> 0.982000128571\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_7 -> 0.994342897551\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_12 -> 0.968071656631\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_13 -> 0.971885915101\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_10 -> 0.937321876272\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_11 -> 0.999614288469\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_14 -> 0.298690723638\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_8 -> 0.999957143163\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_9 -> 0.999828572653\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_0 -> 0.165770244498\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_0 -> 0.0784708680652\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_1 -> 0.849165363105\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_2 -> 0.99605717102\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_3 -> 0.999421432704\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_4 -> 0.998992864337\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_5 -> 0.998328583367\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_6 -> 0.996485739388\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_7 -> 0.999571431633\n"
     ]
    }
   ],
   "source": [
    "_ = score_estimators(estimators, X2, Y2, **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Predict all --\n"
     ]
    }
   ],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X2, **_kwargs)\n",
    "#print y_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23, 2, 7, 4, 21, 18, 22] [23, 2, 7, 11, 4, 21, 18]\n",
      " [23, 2, 18, 8, 7, 17, 21] [23, 2, 4, 11, 18, 17] [23, 2, 11, 4, 18, 7, 17]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind_ahor_fin_ult1</th>\n",
       "      <th>ind_aval_fin_ult1</th>\n",
       "      <th>ind_cco_fin_ult1</th>\n",
       "      <th>ind_cder_fin_ult1</th>\n",
       "      <th>ind_cno_fin_ult1</th>\n",
       "      <th>ind_ctju_fin_ult1</th>\n",
       "      <th>ind_ctma_fin_ult1</th>\n",
       "      <th>ind_ctop_fin_ult1</th>\n",
       "      <th>ind_ctpp_fin_ult1</th>\n",
       "      <th>ind_deco_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.617978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.182508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.187367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.477073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.477680</td>\n",
       "      <td>1.475858</td>\n",
       "      <td>9.870331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.409049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.391436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.747039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.477073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.477680</td>\n",
       "      <td>1.475858</td>\n",
       "      <td>7.599757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280009</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.617978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.498026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.402976</td>\n",
       "      <td>5.415427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.377164</td>\n",
       "      <td>5.991497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.653811</td>\n",
       "      <td>4.651989</td>\n",
       "      <td>9.526268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280015</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.644701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.535682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.692681</td>\n",
       "      <td>0.993623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.870331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280021</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.572123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.228363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.423930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344063</td>\n",
       "      <td>1.342241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.436684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ind_ahor_fin_ult1  ind_aval_fin_ult1  ind_cco_fin_ult1  \\\n",
       "279997                0.0                0.0          7.617978   \n",
       "280003                0.0                0.0          7.409049   \n",
       "280009                0.0                0.0          7.617978   \n",
       "280015                0.0                0.0          7.644701   \n",
       "280021                0.0                0.0          7.572123   \n",
       "\n",
       "        ind_cder_fin_ult1  ind_cno_fin_ult1  ind_ctju_fin_ult1  \\\n",
       "279997                0.0          2.182508                0.0   \n",
       "280003                0.0          2.391436                0.0   \n",
       "280009                0.0          4.498026                0.0   \n",
       "280015                0.0          1.535682                0.0   \n",
       "280021                0.0          2.228363                0.0   \n",
       "\n",
       "        ind_ctma_fin_ult1  ind_ctop_fin_ult1  ind_ctpp_fin_ult1  \\\n",
       "279997                0.0           3.187367           0.000000   \n",
       "280003                0.0           5.747039           0.000000   \n",
       "280009                0.0           5.402976           5.415427   \n",
       "280015                0.0           0.000000           0.000000   \n",
       "280021                0.0           0.423930           0.000000   \n",
       "\n",
       "        ind_deco_fin_ult1       ...         ind_hip_fin_ult1  \\\n",
       "279997                0.0       ...                      0.0   \n",
       "280003                0.0       ...                      0.0   \n",
       "280009                0.0       ...                      0.0   \n",
       "280015                0.0       ...                      0.0   \n",
       "280021                0.0       ...                      0.0   \n",
       "\n",
       "        ind_plan_fin_ult1  ind_pres_fin_ult1  ind_reca_fin_ult1  \\\n",
       "279997                0.0                0.0           0.000000   \n",
       "280003                0.0                0.0           0.000000   \n",
       "280009                0.0                0.0           5.377164   \n",
       "280015                0.0                0.0           0.692681   \n",
       "280021                0.0                0.0           0.344063   \n",
       "\n",
       "        ind_tjcr_fin_ult1  ind_valo_fin_ult1  ind_viv_fin_ult1  \\\n",
       "279997           1.477073                0.0               0.0   \n",
       "280003           1.477073                0.0               0.0   \n",
       "280009           5.991497                0.0               0.0   \n",
       "280015           0.993623                0.0               0.0   \n",
       "280021           1.342241                0.0               0.0   \n",
       "\n",
       "        ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "279997         1.477680           1.475858         9.870331  \n",
       "280003         1.477680           1.475858         7.599757  \n",
       "280009         4.653811           4.651989         9.526268  \n",
       "280015         0.000000           0.000000         9.870331  \n",
       "280021         0.000000           0.000000         8.436684  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print y_preds[:5]\n",
    "Y_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labels_masks_dict['lm_0'], common_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_val = targets_str_to_indices(Y2[target_labels].values)\n",
    "# y_val = targets_str_to_indices(Y2[labels_masks_dict['lm_0']].values, index_map=common_groups[0])\n",
    "#print y_val[:100]\n",
    "#print y_preds[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.unique(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Compute max map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0308569224506\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0229675995512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.022967599551159693"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"- Compute max map7 score\")\n",
    "map7_score(y_val, y_val, clc2[LC_TARGET_LABELS].values)\n",
    "# map7_score0(y_val, y_val)\n",
    "logging.info(\"- Compute map7 score\")\n",
    "map7_score(y_val, y_preds, clc2[LC_TARGET_LABELS].values)\n",
    "# map7_score0(y_val, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On columns lm_0=['ind_cco_fin_ult1', 'ind_cder_fin_ult1', 'ind_cno_fin_ult1','ind_ctju_fin_ult1']\n",
    "\n",
    "- Feature mask: all : 'fm6', 'fm4', 'fm5', 'fm2', 'fm3', 'fm0', 'fm1'\n",
    "- threshold = 0.0\n",
    "\n",
    "Model | Map@7 | Max Map@7 | Labels mask | Samples mask\n",
    "--- | --- | --- | ---\n",
    "et | 0.007074370518592629 | 0.0075787893947 | lm_0 | all \n",
    "\n",
    "\n",
    "\n",
    "- Feature mask: fm0, fm1, fm3, fm4, fm5\n",
    "- threshold = 0.0\n",
    "\n",
    "Model | Map@7 | Max Map@7 | Labels mask | Samples mask\n",
    "--- | --- | --- | ---\n",
    "rf + et + gb | 0.006920126730031681 | 0.0075787893947 | lm_0 | all\n",
    "rf | 0.0068805235951309 | 0.0075787893947 | lm_0 | all\n",
    "et | 0.006936801734200433 | 0.0075787893947 | lm_0 | all \n",
    "gb | 0.0068805235951309 | 0.0075787893947 | lm_0 | all\n",
    "\n",
    "\n",
    "- Feature mask: fm0, fm1\n",
    "\n",
    "Model | Map@7 | Max Map@7 | Labels mask | Samples mask\n",
    "--- | --- | --- | ---\n",
    "rf + et + gb | 0.004627313656828414 | 0.0075787893947 | lm_0 | all\n",
    "rf | 0.004664832416208104 | 0.0075787893947 | lm_0 | all\n",
    "et | 0.004952476238119059 | 0.0075787893947 | lm_0 | all \n",
    "gb | 0.004489744872436218 | 0.0075787893947 | lm_0 | all\n",
    "\n",
    "- Features mask: fm0\n",
    "\n",
    "Model | Map@7 | Max Map@7 | Labels mask | Samples mask\n",
    "--- | --- | --- | --- | ---\n",
    "rf + et + gb | 0.0021010505252626313 | 0.0075787893947 | lm_0 | all\n",
    "rf + et | 0.001950975487743872 | 0.0075787893947 | lm_0 | all\n",
    "rf | 0.001550775387693847 | 0.0075787893947 | lm_0 | all\n",
    "gb | 0.0013006503251625813 | 0.0075787893947 | lm_0 | all\n",
    "et | 0.0017008504252126063 | 0.0075787893947 | lm_0 | all\n",
    "et |  0.0014007003501750874 | 0.0075787893947 | lm_0 | x>0 or y>0\n",
    "rf |  0.0008254127063531766 | 0.0075787893947 | lm_0 | .\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "0.021295269099703414 (GB on 'all')\n",
    "\n",
    "0.021271936353906683 (RF tunning)\n",
    "\n",
    "0.021668245671284416 (RF tunning)\n",
    "\n",
    "0.02136609107928888\n",
    "\n",
    "0.0211362663776694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print labels_masks_dict[estimators[0][0][1]]\n",
    "# print estimators[0][1].classes_\n",
    "# print estimators[0][1].n_classes_\n",
    "# print estimators[0][1].n_features_\n",
    "# print estimators[0][1].n_outputs_\n",
    "# print estimators[0][1].estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import targets_to_labels, targets_indices_to_labels, remove_last_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Count =  1\n",
      "['e-account']\n",
      "['Direct Debit', 'Payroll Account']\n",
      "--- Count =  2\n",
      "['Securities']\n",
      "[]\n",
      "--- Count =  3\n",
      "['Payroll', 'Pensions']\n",
      "['Credit Card', 'Payroll Account']\n",
      "--- Count =  4\n",
      "['Taxes']\n",
      "['Payroll Account', 'Payroll']\n",
      "--- Count =  5\n",
      "['particular Plus Account']\n",
      "['Direct Debit', 'Credit Card']\n",
      "--- Count =  6\n",
      "['e-account']\n",
      "['Direct Debit', 'Payroll Account', 'Payroll', 'Pensions']\n",
      "--- Count =  7\n",
      "['e-account']\n",
      "['Payroll', 'Pensions', 'Current Accounts']\n",
      "--- Count =  8\n",
      "['Credit Card']\n",
      "['Direct Debit', 'Current Accounts', 'Payroll Account', 'Payroll', 'Pensions', 'Long-term deposits']\n",
      "--- Count =  9\n",
      "['Credit Card']\n",
      "['Current Accounts', 'Pensions', 'Payroll']\n",
      "--- Count =  10\n",
      "['Payroll Account']\n",
      "['Payroll']\n",
      "--- Count =  11\n",
      "['e-account']\n",
      "['Direct Debit', 'Payroll Account', 'Credit Card', 'Long-term deposits', 'Payroll']\n",
      "--- Count =  12\n",
      "['particular Account']\n",
      "['Payroll Account']\n",
      "--- Count =  13\n",
      "['Credit Card']\n",
      "['Direct Debit', 'Payroll', 'Current Accounts']\n",
      "--- Count =  14\n",
      "['Credit Card']\n",
      "[]\n",
      "--- Count =  15\n",
      "['e-account']\n",
      "['Direct Debit', 'Payroll Account', 'Payroll', 'Credit Card', 'Pensions']\n",
      "--- Count =  16\n",
      "['Credit Card']\n",
      "['Payroll', 'Current Accounts', 'particular Plus Account', 'Long-term deposits']\n",
      "--- Count =  17\n",
      "['Long-term deposits', 'Funds']\n",
      "['Payroll Account', 'Pensions', 'Credit Card', 'Payroll']\n",
      "--- Count =  18\n",
      "['e-account']\n",
      "['Direct Debit', 'Payroll Account', 'Credit Card', 'Payroll']\n",
      "--- Count =  19\n",
      "['Securities']\n",
      "['Direct Debit', 'Credit Card', 'Payroll Account', 'Pensions', 'Payroll']\n",
      "--- Count =  20\n",
      "['Credit Card']\n",
      "[]\n",
      "--- Count =  21\n",
      "['Credit Card']\n",
      "['Payroll', 'Pensions']\n",
      "--- Count =  22\n",
      "['Pensions']\n",
      "['Credit Card', 'Payroll Account', 'Payroll', 'Taxes']\n",
      "--- Count =  23\n",
      "['Current Accounts']\n",
      "['Credit Card', 'Payroll', 'Pensions']\n",
      "--- Count =  24\n",
      "['e-account']\n",
      "['Direct Debit', 'Credit Card', 'Payroll Account', 'Payroll', 'Pensions']\n"
     ]
    }
   ],
   "source": [
    "limit = 25\n",
    "count = 0\n",
    "\n",
    "not_predicted_predicted = defaultdict(int)\n",
    "for last_choice, targets, products, proba in zip(clc2[LC_TARGET_LABELS].values, y_val, y_preds, Y_probas.values):\n",
    "    added_products = remove_last_choice(targets, last_choice)\n",
    "    predictions = remove_last_choice(products, last_choice)\n",
    "#     print \"---\", count, last_choice\n",
    "#     print targets, '->', added_products\n",
    "#     print products, '->', predictions\n",
    "#     if count == 3:\n",
    "#         break\n",
    "    \n",
    "    if len(added_products) == 0:\n",
    "        continue\n",
    "        \n",
    "    if len(set(added_products) & set(predictions)) > 0:\n",
    "#         print \"Predicted : \", added_products, predictions\n",
    "#         print set(added_products) & set(predictions)\n",
    "        continue\n",
    "\n",
    "    count += 1\n",
    "    if count < limit:\n",
    "        print \"--- Count = \", count\n",
    "        print targets_indices_to_labels(added_products, TARGET_LABELS2)#, targets_indices_to_labels(targets, TARGET_LABELS2)\n",
    "        print targets_indices_to_labels(predictions, TARGET_LABELS2)#, targets_indices_to_labels(products, TARGET_LABELS2)#, proba\n",
    "    \n",
    "    for p in added_products:\n",
    "        not_predicted_predicted[TARGET_LABELS2[p]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'Long-term deposits': 1, 'particular Account': 4, 'particular Plus Account': 9, 'e-account': 50, 'Payroll': 5, 'Pensions': 6, 'Taxes': 7, 'Payroll Account': 4, 'Securities': 10, 'Mas particular Account': 6, 'Funds': 2, 'Credit Card': 33, 'Current Accounts': 4, 'Junior Account': 2, 'Pensions (plan fin)': 1}) 46667\n"
     ]
    }
   ],
   "source": [
    "print not_predicted_predicted, y_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print y_probas[:10, target_groups[0]]\n",
    "#print Y[np.array(TARGET_LABELS)[target_groups[0]]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFold Cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from trainval import cross_val_score0, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Cross validation : \n",
      "INFO:root:\n",
      "\n",
      "\t\t-- Fold : 1 / 5\n",
      "\n",
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.658559\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.640924\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.709334\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.943752\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.748556\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.986926\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.823959\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.395865\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.392825\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.624810\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.605047\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.903618\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.902098\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.925509\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.716023\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.965035\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.817878\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.811797\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.844026\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.959562\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.807236\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.994223\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.921253\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.610216\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.610520\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.861964\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.874430\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.986622\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.987534\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.958346\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.904834\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.982670\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.657039\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.644877\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.711158\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.942840\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.750076\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.986926\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.822134\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.235330\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.232898\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.613864\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.612040\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.898449\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.901186\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.931590\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.730009\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.965339\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.827607\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.826391\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.855883\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.984798\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.821526\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.998480\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.957130\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.998784\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.998784\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.856491\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.872606\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988750\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988446\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.970812\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.929766\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.994831\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.889936\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.891760\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.865005\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.994527\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.826391\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.999392\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.991791\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.691700\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.692004\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.941928\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.973244\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.994527\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992399\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.993919\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.950137\n",
      "INFO:root:-- Process : sample_mask=3289/119986, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.999392\n",
      "INFO:root:-- Predict all --\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0370758372134\n",
      "INFO:root:\n",
      "\n",
      "\t\t-- Fold : 2 / 5\n",
      "\n",
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.652856\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.629619\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.687570\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.909295\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.772396\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.969205\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.800392\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.347144\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.349384\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.618421\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.604983\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.871781\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.892217\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.917973\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.702408\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.948208\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.823348\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.818869\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.837346\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.941769\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.802072\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.978163\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.908455\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.629899\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.636058\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.870101\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.874020\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988242\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.987962\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.953247\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.903975\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.966685\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.655655\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.633539\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.693449\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.909015\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.768197\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.969205\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.800672\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.220605\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.221165\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.624300\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.612262\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.868141\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.888858\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.923852\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.729843\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.948488\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.831187\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.828108\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.856103\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.980403\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.828947\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997480\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.957727\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.998040\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.998880\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.863942\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.868981\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988802\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990202\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.964726\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.929171\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988802\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.896137\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.900056\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.872060\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992161\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.834826\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.999160\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990761\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.720325\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.715845\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.947928\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.972284\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.994961\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.994121\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.994961\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.952128\n",
      "INFO:root:-- Process : sample_mask=3572/119986, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997480\n",
      "INFO:root:-- Predict all --\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0276813263337\n",
      "INFO:root:\n",
      "\n",
      "\t\t-- Fold : 3 / 5\n",
      "\n",
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.641937\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.618753\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.668470\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.905204\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.765842\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.968315\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.781556\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.327409\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.323287\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.612056\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.598145\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.810665\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.886656\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.912159\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.688563\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.943586\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.831530\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.829212\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.821741\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.929675\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.807316\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.978362\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.901597\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.640134\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.638331\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.877383\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.886399\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.978619\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.985059\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.943843\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.901855\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.964967\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.647347\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.629057\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.661515\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.905976\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.757084\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.968315\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.780010\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.193199\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.195260\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.617723\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.594281\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.802937\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.884596\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.917568\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.718187\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.944101\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.839773\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.835394\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.844410\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.978877\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.836167\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.996909\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.958269\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.998970\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999227\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.871973\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.880989\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.979392\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988923\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.960072\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.929418\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.989181\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.899536\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.904946\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.861154\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992787\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.830500\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.998970\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.991242\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.704791\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.705307\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.948995\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.972952\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.991242\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.993818\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.993302\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.951829\n",
      "INFO:root:-- Process : sample_mask=3882/119986, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997166\n",
      "INFO:root:-- Predict all --\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0201026517996\n",
      "INFO:root:\n",
      "\n",
      "\t\t-- Fold : 4 / 5\n",
      "\n",
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.648318\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.624618\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.639908\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.896534\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.767584\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.967635\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.770133\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.311927\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.308104\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.614424\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.597095\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.804281\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.851427\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.899083\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.691896\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.939348\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.830020\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.827982\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.801223\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.922783\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.814985\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.980887\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.894750\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.644495\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.644241\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.879205\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.889399\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.977574\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.982416\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.940367\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.903160\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.961264\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.653925\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.637870\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.642457\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.896024\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.768094\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.967890\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.768603\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.177625\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.179409\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.619266\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.602446\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.800204\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.850917\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.905963\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.720693\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.939857\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.841488\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.837411\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.835627\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.976300\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.833333\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.996432\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.958970\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.998981\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999235\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.874618\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.884302\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.978593\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.984709\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.954128\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.927625\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988277\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.896279\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.901121\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.851682\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990571\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.833078\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.999235\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992864\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.695464\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.705148\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.948267\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.972222\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.991081\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990826\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992864\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.952854\n",
      "INFO:root:-- Process : sample_mask=3924/119987, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997452\n",
      "INFO:root:-- Predict all --\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0175969256218\n",
      "INFO:root:\n",
      "\n",
      "\t\t-- Fold : 5 / 5\n",
      "\n",
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.652116\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.625233\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.630290\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.893798\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.764706\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.966196\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.758052\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.282672\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.284003\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.614054\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.595156\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.798243\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.845888\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.899920\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.656375\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.936918\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.837370\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.831781\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.789194\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.919617\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.808890\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.977642\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.900985\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.653447\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.650253\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.887410\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.896992\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.977908\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.981634\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.939313\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.892467\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.960607\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.649454\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.629225\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.628959\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.892201\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.762843\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.966463\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.757253\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.162364\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.161565\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.614320\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.590631\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.790791\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.844291\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.906042\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.691243\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.937184\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.844291\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.842161\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.826191\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.977908\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.836572\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.996007\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.964333\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.998403\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999734\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.880224\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.892467\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.977642\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.983764\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.954751\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.931328\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988022\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.895395\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.898057\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.845888\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992015\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.832579\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.998935\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992813\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.709076\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.705882\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.947565\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.973117\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990684\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.990418\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.993878\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.952356\n",
      "INFO:root:-- Process : sample_mask=3757/119987, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997072\n",
      "INFO:root:-- Predict all --\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0236035201519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation \n",
      " 5 | 0.017597 | 0.025212 | 0.023604 | 0.037076 | 0.00683 \n"
     ]
    }
   ],
   "source": [
    "# Unitary run\n",
    "nb_folds = 5\n",
    "results = cross_val_score((X, Y, clients_last_choice[LC_TARGET_LABELS].values), nb_folds=nb_folds, **_kwargs)\n",
    "\n",
    "print \"Cross-Validation \\n %i | %f | %f | %f | %f | %.5f \" % (nb_folds, results.min(), results.mean(), np.median(results), results.max(), results.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "# CV on various combinations :\n",
    "\n",
    "_samples_masks_list = [\n",
    "    'all',\n",
    "#     lambda x, y:  ~(x['targets_diff'].isin([0])), \n",
    "#     lambda x, y:  x['targets_diff'] > 0, \n",
    "#     lambda x, y:  x['targets_diff'] < 0, \n",
    "    lambda x, y:  ~(x['targets_diff'].isin([0])) | ~(y['targets_diff'].isin([0])), \n",
    "    lambda x, y:  (x['targets_diff'] > 0) | (y['targets_diff'] > 0), \n",
    "#     lambda x, y:  (x['targets_diff'] < 0) | (y['targets_diff'] < 0), \n",
    "#     lambda x, y:  (y['targets_diff'] > 0), \n",
    "#     lambda x, y:  y['targets_diff'] < 0, \n",
    "]\n",
    "\n",
    "_features_masks_dict = {\n",
    "    'fm0': features + target_features + TARGET_LABELS_FRQ.tolist() + TARGET_LABELS_FRQ_PREV,\n",
    "    'fm1': ['pais_residencia', 'sexo', 'age', 'ind_nuevo', 'segmento', 'ind_empleado', 'ind_actividad_cliente', 'indresi'],\n",
    "    'fm2': target_features,\n",
    "    'fm3': ['pais_residencia', 'sexo', 'age', 'segmento', 'renta'],\n",
    "    'fm4': ['pais_residencia', 'sexo', 'age', 'renta', 'targets_logdiff', 'targets_logcount2_diff','targets_logcount2','targets_logcount1'],\n",
    "    'fm5': ['nomprov', 'ind_nuevo', 'renta', 'ind_actividad_cliente', 'canal_entrada'],\n",
    "    'fm6': TARGET_LABELS_FRQ,\n",
    "}\n",
    "\n",
    "_models_dict = {\n",
    "    'rf': create_RF,\n",
    "    'et': create_ET,\n",
    "    'gb': create_GB,\n",
    "}\n",
    "\n",
    "_labels_masks_dict = {\n",
    "    'lm_0': labels_masks_dict['lm_0']\n",
    "}\n",
    "\n",
    "nb_folds = 5\n",
    "\n",
    "def BruteForceSearchCV():\n",
    "    \n",
    "    def get_models_combinations(items):\n",
    "        combins = list(combinations(items, 1))\n",
    "        combins += list(combinations(items, len(items)))\n",
    "        return combins\n",
    "    \n",
    "    def get_combinations(items):\n",
    "        combins = list(combinations(items, 1))\n",
    "        for i in range(2, len(items)+1):\n",
    "            combins += list(combinations(items, i))\n",
    "        return combins\n",
    "    \n",
    "    def get_items(items):\n",
    "        out = [[items[0],], ]\n",
    "        for i in items[1:]:\n",
    "            tmp = list(out[-1])\n",
    "            tmp.append(i)\n",
    "            out.append(tmp)        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    _labels_masks_combinations = get_items(sorted(_labels_masks_dict.keys()))\n",
    "    _features_masks_combinations = get_items(sorted(_features_masks_dict.keys()))\n",
    "    _models_combinations = get_models_combinations(_models_dict.keys())\n",
    "\n",
    "    # Very big loop:\n",
    "    for lm_keys in _labels_masks_combinations:\n",
    "        __labels_masks_dict = {}\n",
    "        for lm_key in lm_keys:\n",
    "            __labels_masks_dict[lm_key] = _labels_masks_dict[lm_key]\n",
    "\n",
    "        for i, sm in enumerate(_samples_masks_list):\n",
    "            __samples_masks_list = [sm]\n",
    "              \n",
    "            for fm_keys in _features_masks_combinations:\n",
    "                __features_masks_dict = {}\n",
    "                for fm_key in fm_keys:\n",
    "                    __features_masks_dict[fm_key] = _features_masks_dict[fm_key]\n",
    "                    \n",
    "                for m_keys in _models_combinations:\n",
    "                    __models_dict = {}\n",
    "                    for m_key in m_keys:\n",
    "                        __models_dict[m_key] = _models_dict[m_key]\n",
    "                    \n",
    "                    print \"\\n\\n---------------------------------------------------------------\" \n",
    "                    print \"--- PROCESS : \", __labels_masks_dict.keys(), i, __features_masks_dict.keys(), __models_dict.keys()\n",
    "                    print \"---------------------------------------------------------------\\n\" \n",
    "                    \n",
    "                    __kwargs = {\n",
    "                        'samples_masks_list': __samples_masks_list, \n",
    "                        'features_masks_dict': __features_masks_dict, \n",
    "                        'labels_masks_dict': __labels_masks_dict, \n",
    "                        'models_dict': __models_dict,\n",
    "                        'labels': target_labels,\n",
    "                        'transform_proba_func': probas_to_indices,\n",
    "                        'prepare_to_fit_func': prepare_to_fit,\n",
    "                        'prepare_to_test_func': prepare_to_test,   \n",
    "                        'probas_to_labels_probas_func': probas_to_labels_probas,\n",
    "                        'threshold': 0.0,\n",
    "                        'n_highest': 7,\n",
    "                        'mode': 'sum',\n",
    "                        'verbose': False,\n",
    "                        'return_probas': True\n",
    "                    }\n",
    "                    #  DEBUG : results = cross_val_score((X1, Y1, clc1[LC_TARGET_LABELS].values), nb_folds=nb_folds, **__kwargs)\n",
    "                    results = cross_val_score((X, Y, clients_last_choice[LC_TARGET_LABELS].values), nb_folds=nb_folds, **__kwargs)\n",
    "                    print \"=> CV : \", results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "BruteForceSearchCV()\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 201505 -> 201605 \n",
    "\n",
    "Cross-Validation \n",
    " 5 | 0.014585 | 0.018385 | 0.019147 | 0.022227 | 0.00294 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute cross-validation across several months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_folds = 3\n",
    "yms = [201504, 201505]\n",
    "#yms = [201505]\n",
    "\n",
    "for ym in yms:\n",
    "    logging.info(\"\\n-------------------------\")\n",
    "    logging.info(\"- Process month : %s\" % ym)\n",
    "    logging.info(\"-------------------------\\n\")\n",
    "    \n",
    "    ym1 = ym + 100    \n",
    "    df1 = train_df if months_ym_map[ym] in train_months else val_df\n",
    "    df2 = train_df if months_ym_map[ym1] in train_months else val_df\n",
    "    X, Y, clients_last_choice = get_XY(ym, df1, ym1, df2) \n",
    "    results = cross_val_score2((X, Y, clients_last_choice[LC_TARGET_LABELS].values), \n",
    "                                profiles=profiles,\n",
    "                                nb_folds=nb_folds)\n",
    "    print \"Cross-Validation \\n %i | %f | %f | %f | %.5f \" % (nb_folds, results.min(), results.mean(), results.max(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201505\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = train_df\n",
    "#df1 = val_df\n",
    "df2 = train_df #if months_ym_map[next_year_month] in train_months else val_df\n",
    "#df2 = val_df\n",
    "\n",
    "X, Y, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.647634\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.625706\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.657186\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.908597\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.762701\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.971342\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.785063\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.315675\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.311116\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.613113\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.599001\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.833695\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.874294\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.910117\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.687799\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.945940\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.826314\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.822840\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.811767\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.929440\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.807425\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.981546\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.904255\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.629179\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.629831\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.875163\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.884933\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.981329\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.985237\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.943986\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.903170\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.965914\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.651324\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.632870\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.660009\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.907729\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.759227\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.971342\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.783977\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.183891\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.183673\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.616370\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.596179\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.829353\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.872992\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.915545\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.716023\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.946157\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.833261\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.830439\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.842380\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.979158\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.826748\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.996743\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.958749\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.998480\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999132\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.870169\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.879288\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.981763\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.986539\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.956795\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.928571\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.989145\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_12\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.896005\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_13\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.899045\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.857794\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992401\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_14\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.831090\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.999132\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.991967\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.708641\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.706904\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.946157\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.972210\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992401\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.992184\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.993487\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.952019\n",
      "INFO:root:-- Process : sample_mask=4606/149983, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997612\n"
     ]
    }
   ],
   "source": [
    "estimators = train_all(X, Y, **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Predict all --\n"
     ]
    }
   ],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X, **_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check score on the data 2016/05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.230502438224\n",
      "INFO:root:- Compute max map7 score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.230502438224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Predicted map7 score: 0.603574260184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.603574260184\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"- Compute map7 score\")\n",
    "print map7_score(y_val, y_preds, clients_last_choice[LC_TARGET_LABELS].values)\n",
    "logging.info(\"- Compute max map7 score\")\n",
    "print map7_score(y_val, y_val, clients_last_choice[LC_TARGET_LABELS].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction for 2016/06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Load training data : \n",
      "INFO:root:- Load data : [201505, 201506]\n",
      "INFO:root:-- Select max clients\n",
      "INFO:root:- Number of lines with unknown data : 3716\n",
      "INFO:root:- Number of columns with nan : 10\n",
      "INFO:root:-- Process date : 201506\n",
      "INFO:root:-- Add logCount columns\n",
      "INFO:root:-- Process month : 2015-05-28\n",
      "INFO:root:-- Process month : 2015-06-28\n",
      "INFO:root:-- Add logDecimal columns\n",
      "INFO:root:-- Transform age/renta/logdiff\n",
      "INFO:root:-- Add target frequencies\n",
      "INFO:root:-- Add target diff\n",
      "INFO:root:- Load test data : \n",
      "INFO:root:- Load data : [201605]\n",
      "INFO:root:- Number of lines with unknown data : 0\n",
      "INFO:root:- Number of columns with nan : 10\n",
      "INFO:root:- Load data : []\n",
      "INFO:root:-- Read all data from the file : ../data/test_ver2.csv\n",
      "INFO:root:- Number of lines with unknown data : 0\n",
      "INFO:root:- Number of columns with nan : 10\n",
      "INFO:root:-- Process date : 201606\n",
      "INFO:root:-- Add target frequencies\n"
     ]
    }
   ],
   "source": [
    "full_train_df, test_df = load_train_test([201506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_diff</th>\n",
       "      <th>ind_plan_fin_ult1_diff</th>\n",
       "      <th>ind_pres_fin_ult1_diff</th>\n",
       "      <th>ind_reca_fin_ult1_diff</th>\n",
       "      <th>ind_tjcr_fin_ult1_diff</th>\n",
       "      <th>ind_valo_fin_ult1_diff</th>\n",
       "      <th>ind_viv_fin_ult1_diff</th>\n",
       "      <th>ind_nomina_ult1_diff</th>\n",
       "      <th>ind_nom_pens_ult1_diff</th>\n",
       "      <th>ind_recibo_ult1_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>631955</th>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053410</th>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631956</th>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>15890</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053333</th>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>15890</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421295</th>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>15892</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "      <td>-99999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fecha_dato  ncodpers  ind_empleado  pais_residencia  sexo  age  \\\n",
       "631955   2015-05-28     15889             3                0     1    9   \n",
       "1053410  2015-06-28     15889             3                0     1    9   \n",
       "631956   2015-05-28     15890             1                0     1   11   \n",
       "1053333  2015-06-28     15890             1                0     1   11   \n",
       "421295   2015-05-28     15892             3                0     0   11   \n",
       "\n",
       "         fecha_alta  ind_nuevo  antiguedad  indrel          ...           \\\n",
       "631955   1995-01-16          0         244     1.0          ...            \n",
       "1053410  1995-01-16          0         245     1.0          ...            \n",
       "631956   1995-01-16          0         244     1.0          ...            \n",
       "1053333  1995-01-16          0         245     1.0          ...            \n",
       "421295   1995-01-16          0         244     1.0          ...            \n",
       "\n",
       "         ind_hip_fin_ult1_diff  ind_plan_fin_ult1_diff  \\\n",
       "631955                -99999.0                -99999.0   \n",
       "1053410                    0.0                     0.0   \n",
       "631956                -99999.0                -99999.0   \n",
       "1053333                    0.0                     0.0   \n",
       "421295                -99999.0                -99999.0   \n",
       "\n",
       "         ind_pres_fin_ult1_diff  ind_reca_fin_ult1_diff  \\\n",
       "631955                 -99999.0                -99999.0   \n",
       "1053410                     0.0                     0.0   \n",
       "631956                 -99999.0                -99999.0   \n",
       "1053333                     0.0                     0.0   \n",
       "421295                 -99999.0                -99999.0   \n",
       "\n",
       "         ind_tjcr_fin_ult1_diff  ind_valo_fin_ult1_diff  \\\n",
       "631955                 -99999.0                -99999.0   \n",
       "1053410                     0.0                     0.0   \n",
       "631956                 -99999.0                -99999.0   \n",
       "1053333                     0.0                     0.0   \n",
       "421295                 -99999.0                -99999.0   \n",
       "\n",
       "         ind_viv_fin_ult1_diff  ind_nomina_ult1_diff  ind_nom_pens_ult1_diff  \\\n",
       "631955                -99999.0              -99999.0                -99999.0   \n",
       "1053410                    0.0                   0.0                     0.0   \n",
       "631956                -99999.0              -99999.0                -99999.0   \n",
       "1053333                    0.0                   0.0                     0.0   \n",
       "421295                -99999.0              -99999.0                -99999.0   \n",
       "\n",
       "         ind_recibo_ult1_diff  \n",
       "631955               -99999.0  \n",
       "1053410                   0.0  \n",
       "631956               -99999.0  \n",
       "1053333                   0.0  \n",
       "421295               -99999.0  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>canal_entrada</th>\n",
       "      <th>conyuemp</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ind_actividad_cliente</th>\n",
       "      <th>ind_ahor_fin_ult1</th>\n",
       "      <th>ind_aval_fin_ult1</th>\n",
       "      <th>ind_cco_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_frq</th>\n",
       "      <th>ind_plan_fin_ult1_frq</th>\n",
       "      <th>ind_pres_fin_ult1_frq</th>\n",
       "      <th>ind_reca_fin_ult1_frq</th>\n",
       "      <th>ind_tjcr_fin_ult1_frq</th>\n",
       "      <th>ind_valo_fin_ult1_frq</th>\n",
       "      <th>ind_viv_fin_ult1_frq</th>\n",
       "      <th>ind_nomina_ult1_frq</th>\n",
       "      <th>ind_nom_pens_ult1_frq</th>\n",
       "      <th>ind_recibo_ult1_frq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310094</th>\n",
       "      <td>56</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.951056</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.878428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929615</th>\n",
       "      <td>56</td>\n",
       "      <td>257</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310093</th>\n",
       "      <td>63</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.951056</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.976952</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.051961</td>\n",
       "      <td>0.056844</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547974</th>\n",
       "      <td>63</td>\n",
       "      <td>257</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310092</th>\n",
       "      <td>62</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.048944</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  antiguedad  canal_entrada  conyuemp  fecha_alta  fecha_dato  \\\n",
       "310094    56         256              3         1  1995-01-16  2016-05-28   \n",
       "929615    56         257              3         1  1995-01-16  2016-06-28   \n",
       "310093    63         256              3         1  1995-01-16  2016-05-28   \n",
       "1547974   63         257              3         1  1995-01-16  2016-06-28   \n",
       "310092    62         256              3         1  1995-01-16  2016-05-28   \n",
       "\n",
       "         ind_actividad_cliente  ind_ahor_fin_ult1  ind_aval_fin_ult1  \\\n",
       "310094                       1                0.0                0.0   \n",
       "929615                       1                NaN                NaN   \n",
       "310093                       1                0.0                0.0   \n",
       "1547974                      1                NaN                NaN   \n",
       "310092                       1                0.0                0.0   \n",
       "\n",
       "         ind_cco_fin_ult1         ...           ind_hip_fin_ult1_frq  \\\n",
       "310094                1.0         ...                       0.995129   \n",
       "929615                NaN         ...                            NaN   \n",
       "310093                0.0         ...                       0.995129   \n",
       "1547974               NaN         ...                            NaN   \n",
       "310092                1.0         ...                       0.995129   \n",
       "\n",
       "         ind_plan_fin_ult1_frq  ind_pres_fin_ult1_frq  ind_reca_fin_ult1_frq  \\\n",
       "310094                0.992084               0.997858               0.951056   \n",
       "929615                     NaN                    NaN                    NaN   \n",
       "310093                0.007916               0.997858               0.951056   \n",
       "1547974                    NaN                    NaN                    NaN   \n",
       "310092                0.992084               0.997858               0.048944   \n",
       "\n",
       "         ind_tjcr_fin_ult1_frq  ind_valo_fin_ult1_frq  ind_viv_fin_ult1_frq  \\\n",
       "310094                0.037459               0.023048              0.996817   \n",
       "929615                     NaN                    NaN                   NaN   \n",
       "310093                0.037459               0.976952              0.996817   \n",
       "1547974                    NaN                    NaN                   NaN   \n",
       "310092                0.037459               0.023048              0.996817   \n",
       "\n",
       "         ind_nomina_ult1_frq  ind_nom_pens_ult1_frq  ind_recibo_ult1_frq  \n",
       "310094              0.948039               0.943156             0.878428  \n",
       "929615                   NaN                    NaN                  NaN  \n",
       "310093              0.051961               0.056844             0.121572  \n",
       "1547974                  NaN                    NaN                  NaN  \n",
       "310092              0.948039               0.943156             0.121572  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months_ym_map = {}\n",
    "months = list(set(full_train_df['fecha_dato'].unique()) | set(test_df['fecha_dato'].unique()))\n",
    "for m in months:\n",
    "    months_ym_map[to_yearmonth(m)] = m\n",
    "    \n",
    "full_train_months = full_train_df['fecha_dato'].unique()\n",
    "test_months = test_df['fecha_dato'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201506\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = full_train_df\n",
    "df2 = test_df\n",
    "X, _, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(619034, 75) (1859230, 96)\n"
     ]
    }
   ],
   "source": [
    "print X.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>targets_diff</th>\n",
       "      <th>targets_logdiff</th>\n",
       "      <th>targets_logcount2_diff</th>\n",
       "      <th>targets_logcount2</th>\n",
       "      <th>targets_logcount1</th>\n",
       "      <th>targets_logDec</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_frq_prev</th>\n",
       "      <th>ind_plan_fin_ult1_frq_prev</th>\n",
       "      <th>ind_pres_fin_ult1_frq_prev</th>\n",
       "      <th>ind_reca_fin_ult1_frq_prev</th>\n",
       "      <th>ind_tjcr_fin_ult1_frq_prev</th>\n",
       "      <th>ind_valo_fin_ult1_frq_prev</th>\n",
       "      <th>ind_viv_fin_ult1_frq_prev</th>\n",
       "      <th>ind_nomina_ult1_frq_prev</th>\n",
       "      <th>ind_nom_pens_ult1_frq_prev</th>\n",
       "      <th>ind_recibo_ult1_frq_prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1053410</th>\n",
       "      <td>15889</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.031536e-05</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>14.571618</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.951056</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.878428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053333</th>\n",
       "      <td>15890</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.595545e-05</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>13.234620</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.951056</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.976952</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.051961</td>\n",
       "      <td>0.056844</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053311</th>\n",
       "      <td>15892</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>1572864.0</td>\n",
       "      <td>14.268409</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>1.276436e-05</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>14.559070</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.048944</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053310</th>\n",
       "      <td>15893</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.389720e-03</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.951056</td>\n",
       "      <td>0.962541</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.878428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053309</th>\n",
       "      <td>15894</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.179954e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>14.557124</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.048944</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.051961</td>\n",
       "      <td>0.056844</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053308</th>\n",
       "      <td>15895</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>-524288.0</td>\n",
       "      <td>-13.169798</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>7.977726e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>14.559192</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.048944</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053306</th>\n",
       "      <td>15897</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.977726e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>14.805208</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.048944</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.056844</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053305</th>\n",
       "      <td>15899</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.786636e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>14.602025</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.951056</td>\n",
       "      <td>0.962541</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053304</th>\n",
       "      <td>15900</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.385340e-04</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>13.287691</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.048944</td>\n",
       "      <td>0.962541</td>\n",
       "      <td>0.976952</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053303</th>\n",
       "      <td>15901</td>\n",
       "      <td>2015-06-28</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>-6.932448</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>2.712427e-05</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>14.572580</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995129</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.951056</td>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.996817</td>\n",
       "      <td>0.948039</td>\n",
       "      <td>0.943156</td>\n",
       "      <td>0.121572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ncodpers  fecha_dato  targets_diff  targets_logdiff  \\\n",
       "1053410     15889  2015-06-28           0.0         0.000000   \n",
       "1053333     15890  2015-06-28           0.0         0.000000   \n",
       "1053311     15892  2015-06-28     1572864.0        14.268409   \n",
       "1053310     15893  2015-06-28           0.0         0.000000   \n",
       "1053309     15894  2015-06-28           0.0         0.000000   \n",
       "1053308     15895  2015-06-28     -524288.0       -13.169798   \n",
       "1053306     15897  2015-06-28           2.0         1.098612   \n",
       "1053305     15899  2015-06-28           0.0         0.000000   \n",
       "1053304     15900  2015-06-28           0.0         0.000000   \n",
       "1053303     15901  2015-06-28       -1024.0        -6.932448   \n",
       "\n",
       "         targets_logcount2_diff  targets_logcount2  targets_logcount1  \\\n",
       "1053410                0.000000       3.031536e-05           0.000032   \n",
       "1053333                0.000000       1.595545e-05           0.000018   \n",
       "1053311               -0.000006       1.276436e-05           0.000014   \n",
       "1053310                0.000000       1.389720e-03           0.001417   \n",
       "1053309                0.000000       7.179954e-06           0.000008   \n",
       "1053308               -0.000002       7.977726e-07           0.000002   \n",
       "1053306                0.000000       7.977726e-07           0.000002   \n",
       "1053305                0.000000       4.786636e-06           0.000005   \n",
       "1053304                0.000000       2.385340e-04           0.000177   \n",
       "1053303                0.000022       2.712427e-05           0.000030   \n",
       "\n",
       "         targets_logDec  ind_empleado  pais_residencia  \\\n",
       "1053410       14.571618             3                0   \n",
       "1053333       13.234620             1                0   \n",
       "1053311       14.559070             3                0   \n",
       "1053310        2.833213             0                0   \n",
       "1053309       14.557124             1                0   \n",
       "1053308       14.559192             1                0   \n",
       "1053306       14.805208             1                0   \n",
       "1053305       14.602025             2                0   \n",
       "1053304       13.287691             2                0   \n",
       "1053303       14.572580             3                0   \n",
       "\n",
       "                   ...             ind_hip_fin_ult1_frq_prev  \\\n",
       "1053410            ...                              0.995129   \n",
       "1053333            ...                              0.995129   \n",
       "1053311            ...                              0.995129   \n",
       "1053310            ...                              0.995129   \n",
       "1053309            ...                              0.995129   \n",
       "1053308            ...                              0.995129   \n",
       "1053306            ...                              0.995129   \n",
       "1053305            ...                              0.995129   \n",
       "1053304            ...                              0.995129   \n",
       "1053303            ...                              0.995129   \n",
       "\n",
       "         ind_plan_fin_ult1_frq_prev  ind_pres_fin_ult1_frq_prev  \\\n",
       "1053410                    0.992084                    0.997858   \n",
       "1053333                    0.007916                    0.997858   \n",
       "1053311                    0.992084                    0.997858   \n",
       "1053310                    0.992084                    0.997858   \n",
       "1053309                    0.992084                    0.997858   \n",
       "1053308                    0.007916                    0.997858   \n",
       "1053306                    0.007916                    0.997858   \n",
       "1053305                    0.007916                    0.997858   \n",
       "1053304                    0.992084                    0.997858   \n",
       "1053303                    0.992084                    0.997858   \n",
       "\n",
       "         ind_reca_fin_ult1_frq_prev  ind_tjcr_fin_ult1_frq_prev  \\\n",
       "1053410                    0.951056                    0.037459   \n",
       "1053333                    0.951056                    0.037459   \n",
       "1053311                    0.048944                    0.037459   \n",
       "1053310                    0.951056                    0.962541   \n",
       "1053309                    0.048944                    0.037459   \n",
       "1053308                    0.048944                    0.037459   \n",
       "1053306                    0.048944                    0.037459   \n",
       "1053305                    0.951056                    0.962541   \n",
       "1053304                    0.048944                    0.962541   \n",
       "1053303                    0.951056                    0.037459   \n",
       "\n",
       "         ind_valo_fin_ult1_frq_prev  ind_viv_fin_ult1_frq_prev  \\\n",
       "1053410                    0.023048                   0.996817   \n",
       "1053333                    0.976952                   0.996817   \n",
       "1053311                    0.023048                   0.996817   \n",
       "1053310                    0.023048                   0.996817   \n",
       "1053309                    0.023048                   0.996817   \n",
       "1053308                    0.023048                   0.996817   \n",
       "1053306                    0.023048                   0.996817   \n",
       "1053305                    0.023048                   0.996817   \n",
       "1053304                    0.976952                   0.996817   \n",
       "1053303                    0.023048                   0.996817   \n",
       "\n",
       "         ind_nomina_ult1_frq_prev  ind_nom_pens_ult1_frq_prev  \\\n",
       "1053410                  0.948039                    0.943156   \n",
       "1053333                  0.051961                    0.056844   \n",
       "1053311                  0.948039                    0.943156   \n",
       "1053310                  0.948039                    0.943156   \n",
       "1053309                  0.051961                    0.056844   \n",
       "1053308                  0.948039                    0.943156   \n",
       "1053306                  0.948039                    0.056844   \n",
       "1053305                  0.948039                    0.943156   \n",
       "1053304                  0.948039                    0.943156   \n",
       "1053303                  0.948039                    0.943156   \n",
       "\n",
       "         ind_recibo_ult1_frq_prev  \n",
       "1053410                  0.878428  \n",
       "1053333                  0.121572  \n",
       "1053311                  0.121572  \n",
       "1053310                  0.878428  \n",
       "1053309                  0.121572  \n",
       "1053308                  0.121572  \n",
       "1053306                  0.121572  \n",
       "1053305                  0.121572  \n",
       "1053304                  0.121572  \n",
       "1053303                  0.121572  \n",
       "\n",
       "[10 rows x 75 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>targets_str</th>\n",
       "      <th>lc_ind_ahor_fin_ult1</th>\n",
       "      <th>lc_ind_aval_fin_ult1</th>\n",
       "      <th>lc_ind_cco_fin_ult1</th>\n",
       "      <th>lc_ind_cder_fin_ult1</th>\n",
       "      <th>lc_ind_cno_fin_ult1</th>\n",
       "      <th>lc_ind_ctju_fin_ult1</th>\n",
       "      <th>lc_ind_ctma_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>lc_ind_hip_fin_ult1</th>\n",
       "      <th>lc_ind_plan_fin_ult1</th>\n",
       "      <th>lc_ind_pres_fin_ult1</th>\n",
       "      <th>lc_ind_reca_fin_ult1</th>\n",
       "      <th>lc_ind_tjcr_fin_ult1</th>\n",
       "      <th>lc_ind_valo_fin_ult1</th>\n",
       "      <th>lc_ind_viv_fin_ult1</th>\n",
       "      <th>lc_ind_nomina_ult1</th>\n",
       "      <th>lc_ind_nom_pens_ult1</th>\n",
       "      <th>lc_ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>929615</th>\n",
       "      <td>15889</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547974</th>\n",
       "      <td>15890</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547975</th>\n",
       "      <td>15892</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547976</th>\n",
       "      <td>15893</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547977</th>\n",
       "      <td>15894</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547978</th>\n",
       "      <td>15895</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547980</th>\n",
       "      <td>15897</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547982</th>\n",
       "      <td>15899</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547983</th>\n",
       "      <td>15900</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547984</th>\n",
       "      <td>15901</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ncodpers  fecha_dato targets_str  lc_ind_ahor_fin_ult1  \\\n",
       "929615      15889  2016-06-28         NaN                   0.0   \n",
       "1547974     15890  2016-06-28         NaN                   0.0   \n",
       "1547975     15892  2016-06-28         NaN                   0.0   \n",
       "1547976     15893  2016-06-28         NaN                   0.0   \n",
       "1547977     15894  2016-06-28         NaN                   0.0   \n",
       "1547978     15895  2016-06-28         NaN                   0.0   \n",
       "1547980     15897  2016-06-28         NaN                   0.0   \n",
       "1547982     15899  2016-06-28         NaN                   0.0   \n",
       "1547983     15900  2016-06-28         NaN                   0.0   \n",
       "1547984     15901  2016-06-28         NaN                   0.0   \n",
       "\n",
       "         lc_ind_aval_fin_ult1  lc_ind_cco_fin_ult1  lc_ind_cder_fin_ult1  \\\n",
       "929615                    0.0                  1.0                   0.0   \n",
       "1547974                   0.0                  0.0                   0.0   \n",
       "1547975                   0.0                  1.0                   0.0   \n",
       "1547976                   0.0                  0.0                   0.0   \n",
       "1547977                   0.0                  1.0                   0.0   \n",
       "1547978                   0.0                  1.0                   0.0   \n",
       "1547980                   0.0                  0.0                   0.0   \n",
       "1547982                   0.0                  1.0                   0.0   \n",
       "1547983                   0.0                  0.0                   0.0   \n",
       "1547984                   0.0                  1.0                   0.0   \n",
       "\n",
       "         lc_ind_cno_fin_ult1  lc_ind_ctju_fin_ult1  lc_ind_ctma_fin_ult1  \\\n",
       "929615                   0.0                   0.0                   0.0   \n",
       "1547974                  1.0                   0.0                   0.0   \n",
       "1547975                  0.0                   0.0                   0.0   \n",
       "1547976                  0.0                   0.0                   0.0   \n",
       "1547977                  0.0                   0.0                   0.0   \n",
       "1547978                  0.0                   0.0                   0.0   \n",
       "1547980                  1.0                   0.0                   0.0   \n",
       "1547982                  0.0                   0.0                   0.0   \n",
       "1547983                  0.0                   0.0                   0.0   \n",
       "1547984                  0.0                   0.0                   0.0   \n",
       "\n",
       "                ...          lc_ind_hip_fin_ult1  lc_ind_plan_fin_ult1  \\\n",
       "929615          ...                          0.0                   0.0   \n",
       "1547974         ...                          0.0                   1.0   \n",
       "1547975         ...                          0.0                   0.0   \n",
       "1547976         ...                          0.0                   0.0   \n",
       "1547977         ...                          0.0                   0.0   \n",
       "1547978         ...                          0.0                   1.0   \n",
       "1547980         ...                          0.0                   1.0   \n",
       "1547982         ...                          0.0                   1.0   \n",
       "1547983         ...                          0.0                   0.0   \n",
       "1547984         ...                          0.0                   0.0   \n",
       "\n",
       "         lc_ind_pres_fin_ult1  lc_ind_reca_fin_ult1  lc_ind_tjcr_fin_ult1  \\\n",
       "929615                    0.0                   0.0                   1.0   \n",
       "1547974                   0.0                   0.0                   1.0   \n",
       "1547975                   0.0                   1.0                   1.0   \n",
       "1547976                   0.0                   0.0                   0.0   \n",
       "1547977                   0.0                   1.0                   1.0   \n",
       "1547978                   0.0                   1.0                   1.0   \n",
       "1547980                   0.0                   1.0                   1.0   \n",
       "1547982                   0.0                   0.0                   0.0   \n",
       "1547983                   0.0                   1.0                   0.0   \n",
       "1547984                   0.0                   0.0                   1.0   \n",
       "\n",
       "         lc_ind_valo_fin_ult1  lc_ind_viv_fin_ult1  lc_ind_nomina_ult1  \\\n",
       "929615                    1.0                  0.0                 0.0   \n",
       "1547974                   0.0                  0.0                 1.0   \n",
       "1547975                   1.0                  0.0                 0.0   \n",
       "1547976                   1.0                  0.0                 0.0   \n",
       "1547977                   1.0                  0.0                 1.0   \n",
       "1547978                   1.0                  0.0                 0.0   \n",
       "1547980                   1.0                  0.0                 0.0   \n",
       "1547982                   1.0                  0.0                 0.0   \n",
       "1547983                   0.0                  0.0                 0.0   \n",
       "1547984                   1.0                  0.0                 0.0   \n",
       "\n",
       "         lc_ind_nom_pens_ult1  lc_ind_recibo_ult1  \n",
       "929615                    0.0                 0.0  \n",
       "1547974                   1.0                 1.0  \n",
       "1547975                   0.0                 1.0  \n",
       "1547976                   0.0                 0.0  \n",
       "1547977                   1.0                 1.0  \n",
       "1547978                   0.0                 1.0  \n",
       "1547980                   1.0                 1.0  \n",
       "1547982                   0.0                 1.0  \n",
       "1547983                   0.0                 1.0  \n",
       "1547984                   0.0                 1.0  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_last_choice.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_submission(predicted_added_products, clients, clc, target_labels):\n",
    "    added_products_col = []\n",
    "    count = 0 \n",
    "    for products, last_choice in zip(predicted_added_products, clc):\n",
    "        predictions = remove_last_choice(products, last_choice)\n",
    "        added_products_col.append(' '.join([target_labels[i] for i in predictions]))\n",
    "        count+=1\n",
    "        if count % 100000 == 0:\n",
    "            logging.info(\"Elapsed : %i\", count)\n",
    "            \n",
    "    out = pd.DataFrame(data={'ncodpers': clients, 'added_products': added_products_col}, columns=['ncodpers', 'added_products'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Predict all --\n",
      "INFO:root:- Get submission dataframe:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-4deb8ff53023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"- Get submission dataframe:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ncodpers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients_last_choice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET_LABELS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X, **_kwargs)\n",
    "\n",
    "logging.info(\"- Get submission dataframe:\")\n",
    "clients = X['ncodpers'].values\n",
    "#submission = get_submission(y_preds, clients, clients_last_choice[LC_TARGET_LABELS].values, TARGET_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Elapsed : 100000\n",
      "INFO:root:Elapsed : 200000\n",
      "INFO:root:Elapsed : 300000\n",
      "INFO:root:Elapsed : 400000\n",
      "INFO:root:Elapsed : 500000\n",
      "INFO:root:Elapsed : 600000\n"
     ]
    }
   ],
   "source": [
    "submission = get_submission(y_preds, clients, clients_last_choice[LC_TARGET_LABELS].values, TARGET_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print submission.shape\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_estimators = []\n",
    "for e in estimators:\n",
    "    # estimators = [([features_mask_name, labels_mask_name, model_name], estimator_object, accuracy), ...]\n",
    "    features_mask_name, labels_mask_name, model_name = e[0]\n",
    "#     print features_mask_name, labels_mask_name, model_name\n",
    "    if set(features_masks_dict[features_mask_name]).issubset(test_df.columns):\n",
    "#         print \"Append the estimator\"\n",
    "        selected_estimators.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Predict all --\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-735016855426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_clients_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msubmission2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_clients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLC_TARGET_LABELS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/SPR/trees/trainval.py\u001b[0m in \u001b[0;36mpredict_all\u001b[0;34m(estimators, X_val, features_masks_dict, labels_masks_dict, labels, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         logging.debug(\"-- Process : model={}, features_mask={}, labels_mask={}\".format(model_name, features_mask_name,\n\u001b[1;32m    287\u001b[0m                                                                                       labels_mask_name))\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_to_test_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Test data shapes : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/SPR/trees/trainval.py\u001b[0m in \u001b[0;36mprepare_to_test\u001b[0;34m(X_val, Y_val)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_to_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    581\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    582\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "submission_clients = set(submission['ncodpers'].unique())\n",
    "test_clients = set(test_df['ncodpers'].unique())\n",
    "if submission_clients != test_clients:\n",
    "    missing_clients = list(test_clients - submission_clients)\n",
    "    missing_clients_mask = test_df['ncodpers'].isin(missing_clients)\n",
    "    \n",
    "    X1 = test_df[missing_clients_mask]\n",
    "        \n",
    "    y_preds, Y_probas = predict_all(selected_estimators, X1, **_kwargs)    \n",
    "    submission2 = get_submission(y_preds, missing_clients, X1[LC_TARGET_LABELS].values, TARGET_LABELS)\n",
    "    \n",
    "#     submission = pd.concat([submission, submission2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission2 = get_submission(y_preds, missing_clients, X1[LC_TARGET_LABELS].values, TARGET_LABELS)\n",
    "#submission = pd.concat([submission, submission2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print submission2.shape\n",
    "submission2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get submission DataFrame and write csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print submission.shape\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "logging.info('- Generate submission')\n",
    "submission_file = '../results/submission_' + \\\n",
    "                  str(datetime.now().strftime(\"%Y-%m-%d-%H-%M\")) + \\\n",
    "                  '.csv'\n",
    "\n",
    "submission.to_csv(submission_file, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../results/submission_2016-11-17-16-37.csv', 'r') as r:\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
