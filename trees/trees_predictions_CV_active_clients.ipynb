{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decision trees tryouts on SPR data, inspired by Kaggle Forum \"When less is more\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and validation data as \n",
    "    month : [ Features | Targets| Difference | Last Choice Targets  ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.getLogger().handlers = []\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../common\")\n",
    "\n",
    "from dataset import load_trainval, LC_TARGET_LABELS, TARGET_LABELS_FRQ, TARGET_LABELS_DIFF\n",
    "from utils import to_yearmonth, TARGET_LABELS, TARGET_LABELS2\n",
    "from utils import target_str_to_labels, decimal_to_dummies, targets_str_to_indices, targets_dec_to_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    u'ind_empleado', u'pais_residencia',\n",
    "    u'sexo', u'age', u'ind_nuevo', u'antiguedad', u'indrel',\n",
    "    u'ult_fec_cli_1t', u'indrel_1mes', u'tiprel_1mes', u'indresi',\n",
    "    u'indext', u'conyuemp', u'canal_entrada', u'indfall', u'nomprov',\n",
    "    u'ind_actividad_cliente', u'renta', u'segmento'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Load training data : \n",
      "INFO:root:- Load data : [201504, 201505, 201601, 201602, 201604, 201605]\n",
      "INFO:root:-- Select 150000 clients\n",
      "INFO:root:- Number of lines with unknown data : 38\n",
      "INFO:root:- Number of columns with nan : 9\n",
      "INFO:root:-- Process date : 201505\n",
      "INFO:root:-- Process date : 201602\n",
      "INFO:root:-- Process date : 201605\n",
      "INFO:root:-- Add logCount columns\n",
      "INFO:root:-- Process month : 2015-04-28\n",
      "INFO:root:-- Process month : 2015-05-28\n",
      "INFO:root:-- Process month : 2016-01-28\n",
      "INFO:root:-- Process month : 2016-02-28\n",
      "INFO:root:-- Process month : 2016-04-28\n",
      "INFO:root:-- Process month : 2016-05-28\n",
      "INFO:root:-- Add logDecimal columns\n",
      "INFO:root:-- Transform age/renta/logdiff\n",
      "INFO:root:-- Add target values frequencies\n",
      "INFO:root:-- Add target diff\n"
     ]
    }
   ],
   "source": [
    "# train_yearmonths_list = [201504, 201505, 201604]\n",
    "train_yearmonths_list = [201505, 201602, 201605]\n",
    "# train_yearmonths_list = [201505]\n",
    "#val_yearmonth = [201605]\n",
    "train_nb_clients = 150000\n",
    "# train_nb_clients = 1500\n",
    "#train_df, val_df = load_trainval(train_yearmonths_list, val_yearmonth, train_nb_clients, val_nb_clients=1500)\n",
    "train_df = load_trainval(train_yearmonths_list, train_nb_clients=train_nb_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_ahor_fin_ult1_frq</th>\n",
       "      <th>ind_aval_fin_ult1_frq</th>\n",
       "      <th>ind_cco_fin_ult1_frq</th>\n",
       "      <th>ind_cder_fin_ult1_frq</th>\n",
       "      <th>ind_cno_fin_ult1_frq</th>\n",
       "      <th>ind_ctju_fin_ult1_frq</th>\n",
       "      <th>ind_ctma_fin_ult1_frq</th>\n",
       "      <th>ind_ctop_fin_ult1_frq</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_frq</th>\n",
       "      <th>ind_plan_fin_ult1_frq</th>\n",
       "      <th>ind_pres_fin_ult1_frq</th>\n",
       "      <th>ind_reca_fin_ult1_frq</th>\n",
       "      <th>ind_tjcr_fin_ult1_frq</th>\n",
       "      <th>ind_valo_fin_ult1_frq</th>\n",
       "      <th>ind_viv_fin_ult1_frq</th>\n",
       "      <th>ind_nomina_ult1_frq</th>\n",
       "      <th>ind_nom_pens_ult1_frq</th>\n",
       "      <th>ind_recibo_ult1_frq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210121</th>\n",
       "      <td>2015-04-28</td>\n",
       "      <td>15893</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.896008</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.839723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051663</th>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>15893</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.896008</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.839723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638558</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>15893</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.896008</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.839723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663080</th>\n",
       "      <td>2016-02-28</td>\n",
       "      <td>15893</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.896008</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.839723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532959</th>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>15893</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.896008</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.839723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338255</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15893</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.896008</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.839723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210119</th>\n",
       "      <td>2015-04-28</td>\n",
       "      <td>15895</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.771475</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.103992</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051665</th>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>15895</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.771475</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.103992</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638556</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>15895</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.771475</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.896008</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663078</th>\n",
       "      <td>2016-02-28</td>\n",
       "      <td>15895</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.771475</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.896008</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.834539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fecha_dato  ncodpers  ind_ahor_fin_ult1_frq  ind_aval_fin_ult1_frq  \\\n",
       "210121   2015-04-28     15893               0.999866               0.999993   \n",
       "1051663  2015-05-28     15893               0.999866               0.999993   \n",
       "1638558  2016-01-28     15893               0.999866               0.999993   \n",
       "2663080  2016-02-28     15893               0.999866               0.999993   \n",
       "3532959  2016-04-28     15893               0.999866               0.999993   \n",
       "4338255  2016-05-28     15893               0.999866               0.999993   \n",
       "210119   2015-04-28     15895               0.999866               0.999993   \n",
       "1051665  2015-05-28     15895               0.999866               0.999993   \n",
       "1638556  2016-01-28     15895               0.999866               0.999993   \n",
       "2663078  2016-02-28     15895               0.999866               0.999993   \n",
       "\n",
       "         ind_cco_fin_ult1_frq  ind_cder_fin_ult1_frq  ind_cno_fin_ult1_frq  \\\n",
       "210121               0.228525               0.999582              0.896008   \n",
       "1051663              0.228525               0.999582              0.896008   \n",
       "1638558              0.228525               0.999582              0.896008   \n",
       "2663080              0.228525               0.999582              0.896008   \n",
       "3532959              0.228525               0.999582              0.896008   \n",
       "4338255              0.228525               0.999582              0.896008   \n",
       "210119               0.771475               0.999582              0.103992   \n",
       "1051665              0.771475               0.999582              0.103992   \n",
       "1638556              0.771475               0.999582              0.896008   \n",
       "2663078              0.771475               0.999582              0.896008   \n",
       "\n",
       "         ind_ctju_fin_ult1_frq  ind_ctma_fin_ult1_frq  ind_ctop_fin_ult1_frq  \\\n",
       "210121                0.988999               0.989548               0.834539   \n",
       "1051663               0.988999               0.989548               0.834539   \n",
       "1638558               0.988999               0.989548               0.834539   \n",
       "2663080               0.988999               0.989548               0.834539   \n",
       "3532959               0.988999               0.989548               0.834539   \n",
       "4338255               0.988999               0.989548               0.834539   \n",
       "210119                0.988999               0.989548               0.834539   \n",
       "1051665               0.988999               0.989548               0.834539   \n",
       "1638556               0.988999               0.989548               0.834539   \n",
       "2663078               0.988999               0.989548               0.834539   \n",
       "\n",
       "                ...           ind_hip_fin_ult1_frq  ind_plan_fin_ult1_frq  \\\n",
       "210121          ...                       0.992401               0.988205   \n",
       "1051663         ...                       0.992401               0.988205   \n",
       "1638558         ...                       0.992401               0.988205   \n",
       "2663080         ...                       0.992401               0.988205   \n",
       "3532959         ...                       0.992401               0.988205   \n",
       "4338255         ...                       0.992401               0.988205   \n",
       "210119          ...                       0.992401               0.011795   \n",
       "1051665         ...                       0.992401               0.011795   \n",
       "1638556         ...                       0.992401               0.011795   \n",
       "2663078         ...                       0.992401               0.011795   \n",
       "\n",
       "         ind_pres_fin_ult1_frq  ind_reca_fin_ult1_frq  ind_tjcr_fin_ult1_frq  \\\n",
       "210121                0.996991               0.932021               0.943835   \n",
       "1051663               0.996991               0.932021               0.943835   \n",
       "1638558               0.996991               0.932021               0.943835   \n",
       "2663080               0.996991               0.932021               0.943835   \n",
       "3532959               0.996991               0.932021               0.943835   \n",
       "4338255               0.996991               0.932021               0.943835   \n",
       "210119                0.996991               0.067979               0.056165   \n",
       "1051665               0.996991               0.067979               0.056165   \n",
       "1638556               0.996991               0.067979               0.056165   \n",
       "2663078               0.996991               0.067979               0.056165   \n",
       "\n",
       "         ind_valo_fin_ult1_frq  ind_viv_fin_ult1_frq  ind_nomina_ult1_frq  \\\n",
       "210121                0.033589              0.995103             0.931998   \n",
       "1051663               0.033589              0.995103             0.931998   \n",
       "1638558               0.033589              0.995103             0.931998   \n",
       "2663080               0.033589              0.995103             0.931998   \n",
       "3532959               0.033589              0.995103             0.931998   \n",
       "4338255               0.033589              0.995103             0.931998   \n",
       "210119                0.033589              0.995103             0.931998   \n",
       "1051665               0.033589              0.995103             0.931998   \n",
       "1638556               0.033589              0.995103             0.931998   \n",
       "2663078               0.033589              0.995103             0.931998   \n",
       "\n",
       "         ind_nom_pens_ult1_frq  ind_recibo_ult1_frq  \n",
       "210121                0.926746             0.839723  \n",
       "1051663               0.926746             0.839723  \n",
       "1638558               0.926746             0.839723  \n",
       "2663080               0.926746             0.839723  \n",
       "3532959               0.926746             0.839723  \n",
       "4338255               0.926746             0.839723  \n",
       "210119                0.926746             0.160277  \n",
       "1051665               0.926746             0.160277  \n",
       "1638556               0.926746             0.160277  \n",
       "2663078               0.926746             0.160277  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['fecha_dato', 'ncodpers'] + TARGET_LABELS_FRQ.tolist()].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_clients(df1, mask1, mask2, df2=None):\n",
    "    active_clients1 = df1[mask1]['ncodpers'].unique()\n",
    "    if df2 is not None:\n",
    "        active_clients2 = df2[mask2]['ncodpers'].unique()\n",
    "    else:\n",
    "        active_clients2 = df1[mask2]['ncodpers'].unique()\n",
    "    active_clients = list(set(active_clients1) & set(active_clients2)) \n",
    "    \n",
    "    if df2 is not None:\n",
    "        return df1['ncodpers'].isin(active_clients), df2['ncodpers'].isin(active_clients)\n",
    "    return df1['ncodpers'].isin(active_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months_ym_map = {}\n",
    "# months = list(set(train_df['fecha_dato'].unique()) | set(val_df['fecha_dato'].unique()))\n",
    "months = train_df['fecha_dato'].unique()\n",
    "for m in months:\n",
    "    months_ym_map[to_yearmonth(m)] = m\n",
    "\n",
    "        \n",
    "train_months = train_df['fecha_dato'].unique()\n",
    "# val_months = val_df['fecha_dato'].unique()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import get_added_products, remove_last_choice, apk, map7_score\n",
    "from visualization import visualize_train_test, visualize_folds, compare_two_datasets, compare_folds, compare_folds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_features = ['targets_diff', 'targets_logdiff', 'targets_logcount2_diff', 'targets_logcount2', 'targets_logcount1', 'targets_logDec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_XY(current_month, df1, next_year_month, df2, months_ym_map):\n",
    "    month_mask = df1['fecha_dato'] == months_ym_map[current_month]\n",
    "    next_year_month_mask = df2['fecha_dato'] == months_ym_map[next_year_month]\n",
    "    next_year_prev_month_mask = df2['fecha_dato'] == months_ym_map[next_year_month - 1]\n",
    "    \n",
    "    # get common clients from df1 at this month and df2 at next year month\n",
    "    common_clients_mask1, common_clients_mask2 = get_common_clients(df1, month_mask, next_year_month_mask, df2)\n",
    "    common_clients_mask2, common_clients_mask3 = get_common_clients(df2, common_clients_mask2 & next_year_month_mask, next_year_prev_month_mask, df2)\n",
    "        \n",
    "    c1 = df1[common_clients_mask1 & month_mask]['ncodpers'].values\n",
    "    c2 = df2[common_clients_mask2 & next_year_month_mask]['ncodpers'].values\n",
    "    c3 = df2[common_clients_mask3 & next_year_prev_month_mask]['ncodpers'].values\n",
    "    assert (c1 == c2).all() and (c2 == c3).all(), \"Problem with common clients\" \n",
    "    \n",
    "    X = df1[common_clients_mask1 & month_mask][['ncodpers', 'fecha_dato'] + target_features + features + TARGET_LABELS_FRQ.tolist()]            \n",
    "   \n",
    "    if TARGET_LABELS[0] in df2.columns and TARGET_LABELS_DIFF[0] in df2.columns and not df2[next_year_month_mask][TARGET_LABELS].isnull().all().all():\n",
    "        Y = df2[common_clients_mask2 & next_year_month_mask][['ncodpers', 'fecha_dato', 'targets_str', 'lc_targets_str', 'targets_diff'] + TARGET_LABELS + TARGET_LABELS_DIFF.tolist()]    \n",
    "        assert (X['ncodpers'].values == Y['ncodpers'].values).all(), \"There is a problem in alignment\"\n",
    "        Y.index = X.index                \n",
    "    else:\n",
    "        Y = None\n",
    "        \n",
    "    if TARGET_LABELS_FRQ[0] in df2.columns and not df2[next_year_prev_month_mask][TARGET_LABELS].isnull().all().all():\n",
    "        # Add TARGET_LABELS_FRQ from previous month to X:\n",
    "        target_labels_frq = df2[common_clients_mask3 & next_year_prev_month_mask][['ncodpers'] + TARGET_LABELS_FRQ.tolist()]\n",
    "        assert (X['ncodpers'].values == target_labels_frq['ncodpers'].values).all(), \"There is a problem in alignment\"\n",
    "        target_labels_frq = target_labels_frq[TARGET_LABELS_FRQ]\n",
    "        target_labels_frq.columns = [c + '_prev' for c in TARGET_LABELS_FRQ]\n",
    "        target_labels_frq.index = X.index\n",
    "        X = pd.concat([X, target_labels_frq], axis=1)        \n",
    "\n",
    "    \n",
    "    if LC_TARGET_LABELS[0] in df2.columns:\n",
    "        clients_last_choice = df2[common_clients_mask2 & next_year_month_mask][['ncodpers', 'fecha_dato', 'targets_str'] + LC_TARGET_LABELS.tolist()]\n",
    "    else:\n",
    "        clients_last_choice = None\n",
    "        \n",
    "    return X, Y, clients_last_choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201505\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = train_df if months_ym_map[current_month] in train_months else val_df\n",
    "#df1 = train_df\n",
    "df2 = train_df if months_ym_map[next_year_month] in train_months else val_df\n",
    "#df2 = train_df\n",
    "\n",
    "X, Y, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (X['ncodpers'].values == Y['ncodpers'].values).all(), \"WTF\"\n",
    "assert (X['ncodpers'].values == clients_last_choice['ncodpers'].values).all(), \"WTF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149981, 75)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>targets_diff</th>\n",
       "      <th>targets_logdiff</th>\n",
       "      <th>targets_logcount2_diff</th>\n",
       "      <th>targets_logcount2</th>\n",
       "      <th>targets_logcount1</th>\n",
       "      <th>targets_logDec</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_frq_prev</th>\n",
       "      <th>ind_plan_fin_ult1_frq_prev</th>\n",
       "      <th>ind_pres_fin_ult1_frq_prev</th>\n",
       "      <th>ind_reca_fin_ult1_frq_prev</th>\n",
       "      <th>ind_tjcr_fin_ult1_frq_prev</th>\n",
       "      <th>ind_valo_fin_ult1_frq_prev</th>\n",
       "      <th>ind_viv_fin_ult1_frq_prev</th>\n",
       "      <th>ind_nomina_ult1_frq_prev</th>\n",
       "      <th>ind_nom_pens_ult1_frq_prev</th>\n",
       "      <th>ind_recibo_ult1_frq_prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1051663</th>\n",
       "      <td>15893</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.839723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051665</th>\n",
       "      <td>15895</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>14.781716</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051667</th>\n",
       "      <td>15899</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>14.602025</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051668</th>\n",
       "      <td>15900</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>13.287691</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.966411</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051673</th>\n",
       "      <td>15907</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>14.587507</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051660</th>\n",
       "      <td>15917</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>14.586863</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.966411</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051651</th>\n",
       "      <td>15927</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>10.424244</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.966411</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051654</th>\n",
       "      <td>15930</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>14.556709</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.932021</td>\n",
       "      <td>0.943835</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.839723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051704</th>\n",
       "      <td>15944</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>14.602877</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992401</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.996991</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.966411</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.931998</td>\n",
       "      <td>0.926746</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051715</th>\n",
       "      <td>15959</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>13.342841</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>0.988205</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>0.966411</td>\n",
       "      <td>0.995103</td>\n",
       "      <td>0.068002</td>\n",
       "      <td>0.073254</td>\n",
       "      <td>0.160277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ncodpers  fecha_dato  targets_diff  targets_logdiff  \\\n",
       "1051663     15893  2015-05-28           0.0              0.0   \n",
       "1051665     15895  2015-05-28           0.0              0.0   \n",
       "1051667     15899  2015-05-28           0.0              0.0   \n",
       "1051668     15900  2015-05-28           0.0              0.0   \n",
       "1051673     15907  2015-05-28           0.0              0.0   \n",
       "1051660     15917  2015-05-28           0.0              0.0   \n",
       "1051651     15927  2015-05-28           0.0              0.0   \n",
       "1051654     15930  2015-05-28           0.0              0.0   \n",
       "1051704     15944  2015-05-28           0.0              0.0   \n",
       "1051715     15959  2015-05-28           0.0              0.0   \n",
       "\n",
       "         targets_logcount2_diff  targets_logcount2  targets_logcount1  \\\n",
       "1051663                     0.0           0.001256           0.001273   \n",
       "1051665                     0.0           0.000004           0.000013   \n",
       "1051667                     0.0           0.000007           0.000007   \n",
       "1051668                     0.0           0.000239           0.000293   \n",
       "1051673                     0.0           0.000011           0.000007   \n",
       "1051660                     0.0           0.004920           0.004841   \n",
       "1051651                     0.0           0.000002           0.000007   \n",
       "1051654                     0.0           0.000034           0.000033   \n",
       "1051704                     0.0           0.000010           0.000013   \n",
       "1051715                     0.0           0.000007           0.000007   \n",
       "\n",
       "         targets_logDec  ind_empleado  pais_residencia  \\\n",
       "1051663        2.833213             0                0   \n",
       "1051665       14.781716             2                0   \n",
       "1051667       14.602025             4                0   \n",
       "1051668       13.287691             4                0   \n",
       "1051673       14.587507             1                0   \n",
       "1051660       14.586863             1                0   \n",
       "1051651       10.424244             4                0   \n",
       "1051654       14.556709             0                0   \n",
       "1051704       14.602877             4                0   \n",
       "1051715       13.342841             4                0   \n",
       "\n",
       "                   ...             ind_hip_fin_ult1_frq_prev  \\\n",
       "1051663            ...                              0.992401   \n",
       "1051665            ...                              0.992401   \n",
       "1051667            ...                              0.992401   \n",
       "1051668            ...                              0.992401   \n",
       "1051673            ...                              0.992401   \n",
       "1051660            ...                              0.992401   \n",
       "1051651            ...                              0.992401   \n",
       "1051654            ...                              0.992401   \n",
       "1051704            ...                              0.992401   \n",
       "1051715            ...                              0.007599   \n",
       "\n",
       "         ind_plan_fin_ult1_frq_prev  ind_pres_fin_ult1_frq_prev  \\\n",
       "1051663                    0.988205                    0.996991   \n",
       "1051665                    0.011795                    0.996991   \n",
       "1051667                    0.011795                    0.996991   \n",
       "1051668                    0.988205                    0.996991   \n",
       "1051673                    0.011795                    0.996991   \n",
       "1051660                    0.988205                    0.996991   \n",
       "1051651                    0.011795                    0.996991   \n",
       "1051654                    0.011795                    0.996991   \n",
       "1051704                    0.988205                    0.996991   \n",
       "1051715                    0.988205                    0.003009   \n",
       "\n",
       "         ind_reca_fin_ult1_frq_prev  ind_tjcr_fin_ult1_frq_prev  \\\n",
       "1051663                    0.932021                    0.943835   \n",
       "1051665                    0.067979                    0.056165   \n",
       "1051667                    0.932021                    0.943835   \n",
       "1051668                    0.067979                    0.943835   \n",
       "1051673                    0.067979                    0.056165   \n",
       "1051660                    0.932021                    0.943835   \n",
       "1051651                    0.932021                    0.943835   \n",
       "1051654                    0.932021                    0.943835   \n",
       "1051704                    0.067979                    0.056165   \n",
       "1051715                    0.067979                    0.056165   \n",
       "\n",
       "         ind_valo_fin_ult1_frq_prev  ind_viv_fin_ult1_frq_prev  \\\n",
       "1051663                    0.033589                   0.995103   \n",
       "1051665                    0.033589                   0.995103   \n",
       "1051667                    0.033589                   0.995103   \n",
       "1051668                    0.966411                   0.995103   \n",
       "1051673                    0.033589                   0.995103   \n",
       "1051660                    0.966411                   0.995103   \n",
       "1051651                    0.966411                   0.995103   \n",
       "1051654                    0.033589                   0.995103   \n",
       "1051704                    0.966411                   0.995103   \n",
       "1051715                    0.966411                   0.995103   \n",
       "\n",
       "         ind_nomina_ult1_frq_prev  ind_nom_pens_ult1_frq_prev  \\\n",
       "1051663                  0.931998                    0.926746   \n",
       "1051665                  0.931998                    0.926746   \n",
       "1051667                  0.931998                    0.926746   \n",
       "1051668                  0.931998                    0.926746   \n",
       "1051673                  0.931998                    0.926746   \n",
       "1051660                  0.931998                    0.926746   \n",
       "1051651                  0.931998                    0.926746   \n",
       "1051654                  0.931998                    0.926746   \n",
       "1051704                  0.931998                    0.926746   \n",
       "1051715                  0.068002                    0.073254   \n",
       "\n",
       "         ind_recibo_ult1_frq_prev  \n",
       "1051663                  0.839723  \n",
       "1051665                  0.160277  \n",
       "1051667                  0.160277  \n",
       "1051668                  0.160277  \n",
       "1051673                  0.160277  \n",
       "1051660                  0.160277  \n",
       "1051651                  0.160277  \n",
       "1051654                  0.839723  \n",
       "1051704                  0.160277  \n",
       "1051715                  0.160277  \n",
       "\n",
       "[10 rows x 75 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print X.shape\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149981, 53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>targets_str</th>\n",
       "      <th>lc_targets_str</th>\n",
       "      <th>ind_ahor_fin_ult1_diff</th>\n",
       "      <th>ind_aval_fin_ult1_diff</th>\n",
       "      <th>ind_cco_fin_ult1_diff</th>\n",
       "      <th>ind_cder_fin_ult1_diff</th>\n",
       "      <th>ind_cno_fin_ult1_diff</th>\n",
       "      <th>ind_ctju_fin_ult1_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_diff</th>\n",
       "      <th>ind_plan_fin_ult1_diff</th>\n",
       "      <th>ind_pres_fin_ult1_diff</th>\n",
       "      <th>ind_reca_fin_ult1_diff</th>\n",
       "      <th>ind_tjcr_fin_ult1_diff</th>\n",
       "      <th>ind_valo_fin_ult1_diff</th>\n",
       "      <th>ind_viv_fin_ult1_diff</th>\n",
       "      <th>ind_nomina_ult1_diff</th>\n",
       "      <th>ind_nom_pens_ult1_diff</th>\n",
       "      <th>ind_recibo_ult1_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1051585</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15993</td>\n",
       "      <td>000010000001100001100001</td>\n",
       "      <td>000010000001100001000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051627</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16056</td>\n",
       "      <td>001010001000000000000110</td>\n",
       "      <td>001010001000000000000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051739</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16294</td>\n",
       "      <td>000010000000100000000111</td>\n",
       "      <td>000010000000000000000111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051354</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16576</td>\n",
       "      <td>000010010001100000110000</td>\n",
       "      <td>000000010001100000110000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051334</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16731</td>\n",
       "      <td>001000000001100000000001</td>\n",
       "      <td>001000000001100000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051325</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16857</td>\n",
       "      <td>001000011000000001010000</td>\n",
       "      <td>000000011000000001010001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051523</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16988</td>\n",
       "      <td>000010001000000001100011</td>\n",
       "      <td>000010001000000001000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051438</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17151</td>\n",
       "      <td>000010010000001001110111</td>\n",
       "      <td>000010010000001001010111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051478</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17236</td>\n",
       "      <td>000010000000100101110111</td>\n",
       "      <td>000010000000100101110011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052264</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17364</td>\n",
       "      <td>000010000000100000100111</td>\n",
       "      <td>000010000000100000000111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fecha_dato  ncodpers               targets_str  \\\n",
       "1051585  2016-05-28     15993  000010000001100001100001   \n",
       "1051627  2016-05-28     16056  001010001000000000000110   \n",
       "1051739  2016-05-28     16294  000010000000100000000111   \n",
       "1051354  2016-05-28     16576  000010010001100000110000   \n",
       "1051334  2016-05-28     16731  001000000001100000000001   \n",
       "1051325  2016-05-28     16857  001000011000000001010000   \n",
       "1051523  2016-05-28     16988  000010001000000001100011   \n",
       "1051438  2016-05-28     17151  000010010000001001110111   \n",
       "1051478  2016-05-28     17236  000010000000100101110111   \n",
       "1052264  2016-05-28     17364  000010000000100000100111   \n",
       "\n",
       "                   lc_targets_str  ind_ahor_fin_ult1_diff  \\\n",
       "1051585  000010000001100001000001                     0.0   \n",
       "1051627  001010001000000000000001                     0.0   \n",
       "1051739  000010000000000000000111                     0.0   \n",
       "1051354  000000010001100000110000                     0.0   \n",
       "1051334  001000000001100000000000                     0.0   \n",
       "1051325  000000011000000001010001                     0.0   \n",
       "1051523  000010001000000001000011                     0.0   \n",
       "1051438  000010010000001001010111                     0.0   \n",
       "1051478  000010000000100101110011                     0.0   \n",
       "1052264  000010000000100000000111                     0.0   \n",
       "\n",
       "         ind_aval_fin_ult1_diff  ind_cco_fin_ult1_diff  \\\n",
       "1051585                     0.0                    0.0   \n",
       "1051627                     0.0                    0.0   \n",
       "1051739                     0.0                    0.0   \n",
       "1051354                     0.0                    0.0   \n",
       "1051334                     0.0                    0.0   \n",
       "1051325                     0.0                    1.0   \n",
       "1051523                     0.0                    0.0   \n",
       "1051438                     0.0                    0.0   \n",
       "1051478                     0.0                    0.0   \n",
       "1052264                     0.0                    0.0   \n",
       "\n",
       "         ind_cder_fin_ult1_diff  ind_cno_fin_ult1_diff  \\\n",
       "1051585                     0.0                    0.0   \n",
       "1051627                     0.0                    0.0   \n",
       "1051739                     0.0                    0.0   \n",
       "1051354                     0.0                    1.0   \n",
       "1051334                     0.0                    0.0   \n",
       "1051325                     0.0                    0.0   \n",
       "1051523                     0.0                    0.0   \n",
       "1051438                     0.0                    0.0   \n",
       "1051478                     0.0                    0.0   \n",
       "1052264                     0.0                    0.0   \n",
       "\n",
       "         ind_ctju_fin_ult1_diff          ...           ind_hip_fin_ult1_diff  \\\n",
       "1051585                     0.0          ...                             0.0   \n",
       "1051627                     0.0          ...                             0.0   \n",
       "1051739                     0.0          ...                             0.0   \n",
       "1051354                     0.0          ...                             0.0   \n",
       "1051334                     0.0          ...                             0.0   \n",
       "1051325                     0.0          ...                             0.0   \n",
       "1051523                     0.0          ...                             0.0   \n",
       "1051438                     0.0          ...                             0.0   \n",
       "1051478                     0.0          ...                             0.0   \n",
       "1052264                     0.0          ...                             0.0   \n",
       "\n",
       "         ind_plan_fin_ult1_diff  ind_pres_fin_ult1_diff  \\\n",
       "1051585                     0.0                     0.0   \n",
       "1051627                     0.0                     0.0   \n",
       "1051739                     0.0                     0.0   \n",
       "1051354                     0.0                     0.0   \n",
       "1051334                     0.0                     0.0   \n",
       "1051325                     0.0                     0.0   \n",
       "1051523                     0.0                     0.0   \n",
       "1051438                     0.0                     0.0   \n",
       "1051478                     0.0                     0.0   \n",
       "1052264                     0.0                     0.0   \n",
       "\n",
       "         ind_reca_fin_ult1_diff  ind_tjcr_fin_ult1_diff  \\\n",
       "1051585                     0.0                     1.0   \n",
       "1051627                     0.0                     0.0   \n",
       "1051739                     0.0                     0.0   \n",
       "1051354                     0.0                     0.0   \n",
       "1051334                     0.0                     0.0   \n",
       "1051325                     0.0                     0.0   \n",
       "1051523                     0.0                     1.0   \n",
       "1051438                     0.0                     1.0   \n",
       "1051478                     0.0                     0.0   \n",
       "1052264                     0.0                     1.0   \n",
       "\n",
       "         ind_valo_fin_ult1_diff  ind_viv_fin_ult1_diff  ind_nomina_ult1_diff  \\\n",
       "1051585                     0.0                    0.0                   0.0   \n",
       "1051627                     0.0                    0.0                   1.0   \n",
       "1051739                     0.0                    0.0                   0.0   \n",
       "1051354                     0.0                    0.0                   0.0   \n",
       "1051334                     0.0                    0.0                   0.0   \n",
       "1051325                     0.0                    0.0                   0.0   \n",
       "1051523                     0.0                    0.0                   0.0   \n",
       "1051438                     0.0                    0.0                   0.0   \n",
       "1051478                     0.0                    0.0                   1.0   \n",
       "1052264                     0.0                    0.0                   0.0   \n",
       "\n",
       "         ind_nom_pens_ult1_diff  ind_recibo_ult1_diff  \n",
       "1051585                     0.0                   0.0  \n",
       "1051627                     1.0                   0.0  \n",
       "1051739                     0.0                   0.0  \n",
       "1051354                     0.0                   0.0  \n",
       "1051334                     0.0                   1.0  \n",
       "1051325                     0.0                   0.0  \n",
       "1051523                     0.0                   0.0  \n",
       "1051438                     0.0                   0.0  \n",
       "1051478                     0.0                   0.0  \n",
       "1052264                     0.0                   0.0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print Y.shape\n",
    "Y[Y['targets_diff'] > 0][['fecha_dato', 'ncodpers', 'targets_str', 'lc_targets_str'] + TARGET_LABELS_DIFF.tolist() ].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149981, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>lc_ind_ahor_fin_ult1</th>\n",
       "      <th>lc_ind_aval_fin_ult1</th>\n",
       "      <th>lc_ind_cco_fin_ult1</th>\n",
       "      <th>lc_ind_cder_fin_ult1</th>\n",
       "      <th>lc_ind_cno_fin_ult1</th>\n",
       "      <th>lc_ind_ctju_fin_ult1</th>\n",
       "      <th>lc_ind_ctma_fin_ult1</th>\n",
       "      <th>lc_ind_ctop_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>lc_ind_hip_fin_ult1</th>\n",
       "      <th>lc_ind_plan_fin_ult1</th>\n",
       "      <th>lc_ind_pres_fin_ult1</th>\n",
       "      <th>lc_ind_reca_fin_ult1</th>\n",
       "      <th>lc_ind_tjcr_fin_ult1</th>\n",
       "      <th>lc_ind_valo_fin_ult1</th>\n",
       "      <th>lc_ind_viv_fin_ult1</th>\n",
       "      <th>lc_ind_nomina_ult1</th>\n",
       "      <th>lc_ind_nom_pens_ult1</th>\n",
       "      <th>lc_ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4338255</th>\n",
       "      <td>15893</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338253</th>\n",
       "      <td>15895</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338249</th>\n",
       "      <td>15899</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338248</th>\n",
       "      <td>15900</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338243</th>\n",
       "      <td>15907</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338235</th>\n",
       "      <td>15917</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338282</th>\n",
       "      <td>15927</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338279</th>\n",
       "      <td>15930</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338266</th>\n",
       "      <td>15944</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338197</th>\n",
       "      <td>15959</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ncodpers  fecha_dato  lc_ind_ahor_fin_ult1  lc_ind_aval_fin_ult1  \\\n",
       "4338255     15893  2016-05-28                   0.0                   0.0   \n",
       "4338253     15895  2016-05-28                   0.0                   0.0   \n",
       "4338249     15899  2016-05-28                   0.0                   0.0   \n",
       "4338248     15900  2016-05-28                   0.0                   0.0   \n",
       "4338243     15907  2016-05-28                   0.0                   0.0   \n",
       "4338235     15917  2016-05-28                   0.0                   0.0   \n",
       "4338282     15927  2016-05-28                   0.0                   0.0   \n",
       "4338279     15930  2016-05-28                   0.0                   0.0   \n",
       "4338266     15944  2016-05-28                   0.0                   0.0   \n",
       "4338197     15959  2016-05-28                   0.0                   0.0   \n",
       "\n",
       "         lc_ind_cco_fin_ult1  lc_ind_cder_fin_ult1  lc_ind_cno_fin_ult1  \\\n",
       "4338255                  0.0                   0.0                  0.0   \n",
       "4338253                  1.0                   0.0                  0.0   \n",
       "4338249                  1.0                   0.0                  0.0   \n",
       "4338248                  0.0                   0.0                  1.0   \n",
       "4338243                  1.0                   0.0                  0.0   \n",
       "4338235                  1.0                   0.0                  0.0   \n",
       "4338282                  1.0                   0.0                  0.0   \n",
       "4338279                  1.0                   0.0                  0.0   \n",
       "4338266                  1.0                   0.0                  0.0   \n",
       "4338197                  0.0                   0.0                  1.0   \n",
       "\n",
       "         lc_ind_ctju_fin_ult1  lc_ind_ctma_fin_ult1  lc_ind_ctop_fin_ult1  \\\n",
       "4338255                   0.0                   0.0                   0.0   \n",
       "4338253                   0.0                   0.0                   0.0   \n",
       "4338249                   0.0                   0.0                   1.0   \n",
       "4338248                   0.0                   0.0                   1.0   \n",
       "4338243                   0.0                   0.0                   1.0   \n",
       "4338235                   0.0                   0.0                   1.0   \n",
       "4338282                   0.0                   0.0                   0.0   \n",
       "4338279                   0.0                   0.0                   0.0   \n",
       "4338266                   0.0                   0.0                   1.0   \n",
       "4338197                   0.0                   0.0                   1.0   \n",
       "\n",
       "                ...          lc_ind_hip_fin_ult1  lc_ind_plan_fin_ult1  \\\n",
       "4338255         ...                          0.0                   0.0   \n",
       "4338253         ...                          0.0                   1.0   \n",
       "4338249         ...                          0.0                   1.0   \n",
       "4338248         ...                          0.0                   0.0   \n",
       "4338243         ...                          0.0                   1.0   \n",
       "4338235         ...                          0.0                   0.0   \n",
       "4338282         ...                          0.0                   1.0   \n",
       "4338279         ...                          0.0                   1.0   \n",
       "4338266         ...                          0.0                   0.0   \n",
       "4338197         ...                          1.0                   0.0   \n",
       "\n",
       "         lc_ind_pres_fin_ult1  lc_ind_reca_fin_ult1  lc_ind_tjcr_fin_ult1  \\\n",
       "4338255                   0.0                   0.0                   0.0   \n",
       "4338253                   0.0                   1.0                   1.0   \n",
       "4338249                   0.0                   0.0                   0.0   \n",
       "4338248                   0.0                   1.0                   0.0   \n",
       "4338243                   0.0                   1.0                   1.0   \n",
       "4338235                   0.0                   0.0                   0.0   \n",
       "4338282                   0.0                   0.0                   0.0   \n",
       "4338279                   0.0                   0.0                   0.0   \n",
       "4338266                   0.0                   1.0                   1.0   \n",
       "4338197                   1.0                   1.0                   1.0   \n",
       "\n",
       "         lc_ind_valo_fin_ult1  lc_ind_viv_fin_ult1  lc_ind_nomina_ult1  \\\n",
       "4338255                   1.0                  0.0                 0.0   \n",
       "4338253                   1.0                  0.0                 0.0   \n",
       "4338249                   1.0                  0.0                 0.0   \n",
       "4338248                   0.0                  0.0                 0.0   \n",
       "4338243                   1.0                  0.0                 0.0   \n",
       "4338235                   0.0                  0.0                 0.0   \n",
       "4338282                   0.0                  0.0                 0.0   \n",
       "4338279                   1.0                  0.0                 0.0   \n",
       "4338266                   0.0                  0.0                 0.0   \n",
       "4338197                   0.0                  0.0                 1.0   \n",
       "\n",
       "         lc_ind_nom_pens_ult1  lc_ind_recibo_ult1  \n",
       "4338255                   0.0                 0.0  \n",
       "4338253                   0.0                 1.0  \n",
       "4338249                   0.0                 1.0  \n",
       "4338248                   0.0                 1.0  \n",
       "4338243                   0.0                 1.0  \n",
       "4338235                   0.0                 1.0  \n",
       "4338282                   0.0                 1.0  \n",
       "4338279                   0.0                 0.0  \n",
       "4338266                   0.0                 1.0  \n",
       "4338197                   1.0                 1.0  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print clients_last_choice.shape\n",
    "clients_last_choice.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another train/predict + CV implementation\n",
    "\n",
    "### Input\n",
    "\n",
    "- `X` : `[nb_samples, nb_features]` shaped pd.DataFrame\n",
    "    - `features_masks_list` : `{fm1_name: features_mask_1, fm2_name: features_mask_2, ...]` with `features_mask_i` is a list of feature column names. They can oversect.\n",
    "    \n",
    "- `Y` : `[nb_samples, nb_labels]` shaped pd.DataFrame\n",
    "    - `labels_masks_list` : `{lm1_name: labels_mask_1, lm2_name: labels_mask_2, ...}` with `labels_mask_i` is a list of labels column names. They can oversect.\n",
    "\n",
    "- `samples_masks_list` : `[samples_mask_1, samples_mask_2, ...]` with samples_mask_i is a function to produce a boolean pd.DataFrame . Used only for training. \n",
    "\n",
    "\n",
    "- Set of models `models` : list of functions to create a model, e.g. `[create_RF, create_NN, create_GBT]`\n",
    "\n",
    "\n",
    "### Training phase\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_masks_list = [\n",
    "#    lambda x:  ~(x['targets_diff'].isin([0])), \n",
    "#     lambda x, y:  x['targets_diff'] > 0, \n",
    "#     lambda x, y:  x['targets_diff'] < 0, \n",
    "    lambda x, y:  (x['targets_diff'] > 0) | (y['targets_diff'] > 0), \n",
    "#     lambda x, y:  y['targets_diff'] < 0, \n",
    "]\n",
    "\n",
    "TARGET_LABELS_FRQ_PREV = [c + '_prev' for c in TARGET_LABELS_FRQ]\n",
    "\n",
    "features_masks_dict = {\n",
    "#     'fm_all': None,\n",
    "    'fm0': features + target_features + TARGET_LABELS_FRQ.tolist() + TARGET_LABELS_FRQ_PREV,\n",
    "    'fm1': ['pais_residencia', 'sexo', 'age', 'ind_nuevo', 'segmento', 'ind_empleado', 'ind_actividad_cliente', 'indresi'],\n",
    "    'fm2': target_features,\n",
    "    'fm3': ['pais_residencia', 'sexo', 'age', 'segmento', 'renta'],\n",
    "#     'fm4': ['pais_residencia', 'sexo', 'age', 'renta', 'targets_logdiff', 'targets_logcount2_diff','targets_logcount2','targets_logcount1'],\n",
    "    'fm5': ['nomprov', 'ind_nuevo', 'renta', 'ind_actividad_cliente', 'canal_entrada'],\n",
    "#     'fm6': TARGET_LABELS_FRQ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "def create_RF(input_shape, output_shape):        \n",
    "    # https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "    return RandomForestClassifier(n_estimators=100, \n",
    "#                                   min_samples_split=100,\n",
    "#                                   min_samples_leaf=25,\n",
    "#                                   max_depth=10\n",
    "                                  max_features=1.0, \n",
    "                                  oob_score=True,\n",
    "                                  bootstrap=True,\n",
    "                                  n_jobs=-1\n",
    "                                 )\n",
    "\n",
    "def create_ET(input_shape, output_shape):\n",
    "    return ExtraTreesClassifier(n_estimators=100,\n",
    "#                                   min_samples_leaf=25,\n",
    "#                                   max_depth=10\n",
    "                                  max_features=1.0, \n",
    "                                  oob_score=True,\n",
    "                                  bootstrap=True,\n",
    "                                  n_jobs=-1\n",
    "\n",
    "                               )\n",
    "\n",
    "def create_GB(input_shape, output_shape):\n",
    "    return GradientBoostingClassifier(n_estimators=75)\n",
    "\n",
    "models_dict = {\n",
    "    'rf': create_RF,\n",
    "    'et': create_ET,\n",
    "    'gb': create_GB,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_0  <=>  [2, 3, 4, 5]\n",
      "lm_1  <=>  [2, 6, 7, 8, 12]\n",
      "lm_2  <=>  [2, 18, 23]\n",
      "lm_3  <=>  [21, 22]\n",
      "lm_4  <=>  [2, 12, 18]\n",
      "lm_5  <=>  [2, 12, 23]\n",
      "lm_6  <=>  [2, 18, 23]\n",
      "lm_7  <=>  [18, 23, 21, 22]\n",
      "lm_8  <=>  [21, 23, 22, 4]\n",
      "lm_9  <=>  [22, 7, 8]\n",
      "{'lm_8': array(['ind_nomina_ult1_diff', 'ind_recibo_ult1_diff',\n",
      "       'ind_nom_pens_ult1_diff', 'ind_cno_fin_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_9': array(['ind_nom_pens_ult1_diff', 'ind_ctop_fin_ult1_diff',\n",
      "       'ind_ctpp_fin_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_others': array(['ind_ahor_fin_ult1_diff', 'ind_aval_fin_ult1_diff',\n",
      "       'ind_deco_fin_ult1_diff', 'ind_deme_fin_ult1_diff',\n",
      "       'ind_dela_fin_ult1_diff', 'ind_fond_fin_ult1_diff',\n",
      "       'ind_hip_fin_ult1_diff', 'ind_plan_fin_ult1_diff',\n",
      "       'ind_pres_fin_ult1_diff', 'ind_reca_fin_ult1_diff',\n",
      "       'ind_valo_fin_ult1_diff', 'ind_viv_fin_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_0': array(['ind_cco_fin_ult1_diff', 'ind_cder_fin_ult1_diff',\n",
      "       'ind_cno_fin_ult1_diff', 'ind_ctju_fin_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_1': array(['ind_cco_fin_ult1_diff', 'ind_ctma_fin_ult1_diff',\n",
      "       'ind_ctop_fin_ult1_diff', 'ind_ctpp_fin_ult1_diff',\n",
      "       'ind_ecue_fin_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_2': array(['ind_cco_fin_ult1_diff', 'ind_tjcr_fin_ult1_diff',\n",
      "       'ind_recibo_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_3': array(['ind_nomina_ult1_diff', 'ind_nom_pens_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_4': array(['ind_cco_fin_ult1_diff', 'ind_ecue_fin_ult1_diff',\n",
      "       'ind_tjcr_fin_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_5': array(['ind_cco_fin_ult1_diff', 'ind_ecue_fin_ult1_diff',\n",
      "       'ind_recibo_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_6': array(['ind_cco_fin_ult1_diff', 'ind_tjcr_fin_ult1_diff',\n",
      "       'ind_recibo_ult1_diff'], \n",
      "      dtype='|S22'), 'lm_7': array(['ind_tjcr_fin_ult1_diff', 'ind_recibo_ult1_diff',\n",
      "       'ind_nomina_ult1_diff', 'ind_nom_pens_ult1_diff'], \n",
      "      dtype='|S22')}\n"
     ]
    }
   ],
   "source": [
    "NP_TARGET_LABELS = np.array(TARGET_LABELS)\n",
    "target_labels = TARGET_LABELS_DIFF\n",
    "\n",
    "common_groups = [\n",
    "#     [2, ],\n",
    "    [2, 3, 4, 5],\n",
    "    [2, 6, 7, 8, 12],\n",
    "    [2, 18, 23], \n",
    "    [21, 22],\n",
    "    [2, 12, 18],\n",
    "    [2, 12, 23],\n",
    "    [2, 18, 23],\n",
    "    [18, 23, 21, 22],\n",
    "    [21, 23, 22, 4],\n",
    "#     [18, ],\n",
    "#     [12, ],\n",
    "#     [21, ],\n",
    "#     [22, ],\n",
    "#     [23, ],\n",
    "#     [3, 4], \n",
    "    [22, 7, 8],\n",
    "#     [17, ],\n",
    "#     [i] for i in range(24)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def flatten(array):\n",
    "    out = []\n",
    "    for item in array:\n",
    "        out += item\n",
    "    return out\n",
    "\n",
    "others = list(set(range(24)) - set(flatten(common_groups)))\n",
    "\n",
    "\n",
    "# for i, a in enumerate(zip(TARGET_LABELS2, TARGET_LABELS)):\n",
    "#     print i, a\n",
    "    \n",
    "s = set({})\n",
    "labels_masks_dict = {}\n",
    "for i, g in enumerate(common_groups):\n",
    "    print 'lm_%i' % i, \" <=> \", g\n",
    "    labels_masks_dict['lm_%i' % i] = target_labels[g]\n",
    "    s |= set(g)\n",
    "labels_masks_dict['lm_others'] = target_labels[others]\n",
    "s |= set(others)\n",
    "\n",
    "assert len(s) == len(target_labels), \"Sum is not equal 24, s=%i\" % s\n",
    "print labels_masks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'et': [(None, None, 'lm_8'),\n",
       "  (None, None, 'lm_9'),\n",
       "  (None, None, 'lm_others'),\n",
       "  (None, None, 'lm_0'),\n",
       "  (None, None, 'lm_1'),\n",
       "  (None, None, 'lm_2'),\n",
       "  (None, None, 'lm_3'),\n",
       "  (None, None, 'lm_4'),\n",
       "  (None, None, 'lm_5'),\n",
       "  (None, None, 'lm_6'),\n",
       "  (None, None, 'lm_7')],\n",
       " 'gb': [],\n",
       " 'rf': [(None, None, 'lm_8'),\n",
       "  (None, None, 'lm_9'),\n",
       "  (None, None, 'lm_others'),\n",
       "  (None, None, 'lm_0'),\n",
       "  (None, None, 'lm_1'),\n",
       "  (None, None, 'lm_2'),\n",
       "  (None, None, 'lm_3'),\n",
       "  (None, None, 'lm_4'),\n",
       "  (None, None, 'lm_5'),\n",
       "  (None, None, 'lm_6'),\n",
       "  (None, None, 'lm_7')]}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {model_name: [(samples_mask_code, features_mask_name, labels_mask_name), ...]}\n",
    "models_pipelines = {\n",
    "    'gb' : [('all', None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) == 1],\n",
    "    'rf' : [(None, None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) > 1],\n",
    "    'et' : [(None, None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) > 1],\n",
    "}\n",
    "models_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from trainval import train_all, predict_all, probas_to_indices, score_estimators\n",
    "from utils import map7_score0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 75) (110000, 53) (110000, 27)\n",
      "(39981, 75) (39981, 53) (39981, 27)\n"
     ]
    }
   ],
   "source": [
    "ll = 110000\n",
    "\n",
    "mask = X.index.isin(X.index[:ll])\n",
    "\n",
    "X1 = X[mask]\n",
    "Y1 = Y[mask]\n",
    "clc = clients_last_choice[mask]\n",
    "print X1.shape, Y1.shape, clc.shape\n",
    "\n",
    "mask = X.index.isin(X.index[ll:ll+ll//2])\n",
    "X2 = X[mask]\n",
    "Y2 = Y[mask]\n",
    "clc2 = clients_last_choice[mask]\n",
    "print X2.shape, Y2.shape, clc2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_kwargs = {'samples_masks_list': samples_masks_list, \n",
    "            'features_masks_dict': features_masks_dict, \n",
    "            'labels_masks_dict': labels_masks_dict, \n",
    "            'models_dict': models_dict,\n",
    "            'labels': target_labels,\n",
    "            'transform_proba_func': probas_to_indices,\n",
    "            'threshold': 0.1,\n",
    "            'n_highest': 7,\n",
    "            'mode': 'sum',\n",
    "            'verbose': False,\n",
    "            'models_pipelines': models_pipelines,\n",
    "            'return_probas': True\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.720000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.721306\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.898286\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.898776\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.988571\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.988571\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.930612\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.930776\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.920653\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.920816\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.663510\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.664327\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.903510\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.903347\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.808327\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.808327\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.760327\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.760653\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.664163\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.663510\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.619429\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.620245\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.802449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.801143\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.927673\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.927673\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.990694\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.990694\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.948082\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.948082\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.935673\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.935837\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.778612\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.779592\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.932082\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.931265\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.866286\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.866449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.823510\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.823837\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.779102\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.778449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.759020\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.759020\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.671510\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.671020\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.887673\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.887673\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.986449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.986449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.921633\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.921633\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.909551\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.909551\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.602449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.602449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.893878\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.894041\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.781224\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.781061\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.712980\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.714122\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.602122\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.602612\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.557224\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.556898\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999837\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999837\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999837\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999837\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999837\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.658612\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.658449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.882449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.882449\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.986122\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.986122\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.917878\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.917878\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.905143\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.905143\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.588735\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.588735\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.889306\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.889306\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.773551\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.773551\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.703673\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.703673\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.589061\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.589061\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.539918\n",
      "INFO:root:-- Process : sample_mask=6125/110000, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.539918\n"
     ]
    }
   ],
   "source": [
    "estimators = train_all(X1, Y1, **_kwargs)\n",
    "\n",
    "#print estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'et': 0.84330092764378484, 'rf': 0.84334248608534312}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = defaultdict(list)\n",
    "for e in estimators:\n",
    "    accuracies[e[0][2]].append(e[2])\n",
    "\n",
    "mean_accuracy = {}\n",
    "for key in accuracies:\n",
    "    accuracy_list = accuracies[key]\n",
    "    mean_accuracy[key] = sum(accuracy_list)/len(accuracy_list)\n",
    "    \n",
    "mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_8 -> 0.937445286511\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_8 -> 0.852880118056\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_9 -> 0.986518596333\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_9 -> 0.96190690578\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_others -> 0.998549310923\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_others -> 0.997598859458\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_0 -> 0.99262149521\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_0 -> 0.979590305395\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_1 -> 0.99339686351\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_1 -> 0.986293489407\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_2 -> 0.908781671294\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_2 -> 0.817288211901\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_3 -> 0.986693679498\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_3 -> 0.965608664115\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_4 -> 0.967259448238\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_4 -> 0.917810960206\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_5 -> 0.937945524124\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_5 -> 0.83629724119\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_6 -> 0.922838348215\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_6 -> 0.844776268728\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_7 -> 0.924889322428\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_7 -> 0.828218403742\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_8 -> 0.974437857983\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_8 -> 0.977664390586\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_9 -> 0.992871614017\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_9 -> 0.992871614017\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_others -> 0.999349691103\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_others -> 0.999349691103\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_0 -> 0.988994772517\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_0 -> 0.992296340762\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_1 -> 0.990445461594\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_1 -> 0.991621019984\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_2 -> 0.979365198469\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_2 -> 0.981691303369\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_3 -> 0.988919736875\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_3 -> 0.992796578375\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_4 -> 0.988744653711\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_4 -> 0.991470948701\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_5 -> 0.977314224257\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_5 -> 0.979915459843\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_6 -> 0.977114129211\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_6 -> 0.98054075686\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_7 -> 0.978264675721\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_7 -> 0.974988119357\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_8 -> 0.965658687877\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_8 -> 0.88879717866\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_9 -> 0.990245366549\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_9 -> 0.952777569345\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_others -> 0.999349691103\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_others -> 0.999349691103\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_0 -> 0.991896150672\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_0 -> 0.970260873915\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_1 -> 0.992871614017\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_1 -> 0.986368525049\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_2 -> 0.951451939671\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_2 -> 0.9136839999\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_3 -> 0.991020734849\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_3 -> 0.955378804932\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_4 -> 0.984467622121\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_4 -> 0.965158450264\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_5 -> 0.974137715415\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_5 -> 0.914684475126\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_6 -> 0.961731822616\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_6 -> 0.909632075236\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_7 -> 0.961631775093\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_7 -> 0.848327955779\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_8 -> 0.977139141092\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_8 -> 0.972511943173\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_9 -> 0.992171281359\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_9 -> 0.991921162552\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_others -> 0.999299667342\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_others -> 0.999299667342\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_0 -> 0.992696530852\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_0 -> 0.977364248018\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_1 -> 0.992871614017\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_1 -> 0.977814461869\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_2 -> 0.980290638053\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_2 -> 0.97626372527\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_3 -> 0.991120782372\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_3 -> 0.991395913059\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_4 -> 0.991545984343\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_4 -> 0.988094344814\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_5 -> 0.980690828143\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_5 -> 0.971661539231\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_6 -> 0.979815412321\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_6 -> 0.973787549086\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_7 -> 0.975413321328\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_7 -> 0.974062679773\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_8 -> 0.97668892724\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_8 -> 0.949626072384\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_9 -> 0.991020734849\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_9 -> 0.963607713664\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_others -> 0.999349691103\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_others -> 0.999349691103\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_0 -> 0.992971661539\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_0 -> 0.992046221955\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_1 -> 0.994297291213\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_1 -> 0.994297291213\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_2 -> 0.973312323354\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_2 -> 0.902728796178\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_3 -> 0.990945699207\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_3 -> 0.910757609865\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_4 -> 0.992271328881\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_4 -> 0.983742277582\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_5 -> 0.981666291488\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_5 -> 0.981616267727\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_6 -> 0.98174132713\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_6 -> 0.973137240189\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_7 -> 0.976438808434\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_7 -> 0.9144593682\n"
     ]
    }
   ],
   "source": [
    "_ = score_estimators(estimators, X2, Y2, **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Predict all --\n"
     ]
    }
   ],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X2, **_kwargs)\n",
    "#print y_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind_ahor_fin_ult1_diff</th>\n",
       "      <th>ind_aval_fin_ult1_diff</th>\n",
       "      <th>ind_cco_fin_ult1_diff</th>\n",
       "      <th>ind_cder_fin_ult1_diff</th>\n",
       "      <th>ind_cno_fin_ult1_diff</th>\n",
       "      <th>ind_ctju_fin_ult1_diff</th>\n",
       "      <th>ind_ctma_fin_ult1_diff</th>\n",
       "      <th>ind_ctop_fin_ult1_diff</th>\n",
       "      <th>ind_ctpp_fin_ult1_diff</th>\n",
       "      <th>ind_deco_fin_ult1_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_diff</th>\n",
       "      <th>ind_plan_fin_ult1_diff</th>\n",
       "      <th>ind_pres_fin_ult1_diff</th>\n",
       "      <th>ind_reca_fin_ult1_diff</th>\n",
       "      <th>ind_tjcr_fin_ult1_diff</th>\n",
       "      <th>ind_valo_fin_ult1_diff</th>\n",
       "      <th>ind_viv_fin_ult1_diff</th>\n",
       "      <th>ind_nomina_ult1_diff</th>\n",
       "      <th>ind_nom_pens_ult1_diff</th>\n",
       "      <th>ind_recibo_ult1_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672292</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672312</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672314</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.846433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282018</td>\n",
       "      <td>0.365153</td>\n",
       "      <td>0.352829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ind_ahor_fin_ult1_diff  ind_aval_fin_ult1_diff  ind_cco_fin_ult1_diff  \\\n",
       "672292                     0.0                     0.0               0.629822   \n",
       "672312                     0.0                     0.0               0.000000   \n",
       "672314                     0.0                     0.0               0.153567   \n",
       "672320                     0.0                     0.0               0.000000   \n",
       "672322                     0.0                     0.0               0.000000   \n",
       "\n",
       "        ind_cder_fin_ult1_diff  ind_cno_fin_ult1_diff  ind_ctju_fin_ult1_diff  \\\n",
       "672292                     0.0                    0.0                     0.0   \n",
       "672312                     0.0                    0.0                     0.0   \n",
       "672314                     0.0                    0.0                     0.0   \n",
       "672320                     0.0                    0.0                     0.0   \n",
       "672322                     0.0                    0.0                     0.0   \n",
       "\n",
       "        ind_ctma_fin_ult1_diff  ind_ctop_fin_ult1_diff  \\\n",
       "672292                     0.0                     0.0   \n",
       "672312                     0.0                     0.0   \n",
       "672314                     0.0                     0.0   \n",
       "672320                     0.0                     0.0   \n",
       "672322                     0.0                     0.0   \n",
       "\n",
       "        ind_ctpp_fin_ult1_diff  ind_deco_fin_ult1_diff          ...           \\\n",
       "672292                     0.0                     0.0          ...            \n",
       "672312                     0.0                     0.0          ...            \n",
       "672314                     0.0                     0.0          ...            \n",
       "672320                     0.0                     0.0          ...            \n",
       "672322                     0.0                     0.0          ...            \n",
       "\n",
       "        ind_hip_fin_ult1_diff  ind_plan_fin_ult1_diff  ind_pres_fin_ult1_diff  \\\n",
       "672292                    0.0                     0.0                     0.0   \n",
       "672312                    0.0                     0.0                     0.0   \n",
       "672314                    0.0                     0.0                     0.0   \n",
       "672320                    0.0                     0.0                     0.0   \n",
       "672322                    0.0                     0.0                     0.0   \n",
       "\n",
       "        ind_reca_fin_ult1_diff  ind_tjcr_fin_ult1_diff  \\\n",
       "672292                     0.0                0.092221   \n",
       "672312                     0.0                0.000000   \n",
       "672314                     0.0                0.846433   \n",
       "672320                     0.0                0.000000   \n",
       "672322                     0.0                0.000000   \n",
       "\n",
       "        ind_valo_fin_ult1_diff  ind_viv_fin_ult1_diff  ind_nomina_ult1_diff  \\\n",
       "672292                     0.0                    0.0              0.000000   \n",
       "672312                     0.0                    0.0              0.000000   \n",
       "672314                     0.0                    0.0              0.000000   \n",
       "672320                     0.0                    0.0              0.282018   \n",
       "672322                     0.0                    0.0              0.000000   \n",
       "\n",
       "        ind_nom_pens_ult1_diff  ind_recibo_ult1_diff  \n",
       "672292                0.000000              0.277957  \n",
       "672312                0.000000              0.000000  \n",
       "672314                0.000000              0.000000  \n",
       "672320                0.365153              0.352829  \n",
       "672322                0.000000              0.000000  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [23] [] [] [] [] [] [19]\n",
      " [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] []\n",
      " [23] [] [] [] [] [] [] [] [] [] [18] [] [] [] [] [] [] [] [] [] [] [] []\n",
      " [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] [] []\n",
      " [] [] [] [] []]\n",
      "[[2, 23] [] [18, 2] [22, 23, 21] [] [] [23] [2] [21, 22] [22, 21] [23] [23]\n",
      " [] [23] [23] [23] [2] [] [] [23] [12] [18] [18] [] [22, 21, 18] [] [23] []\n",
      " [] [18] [21, 22] [22, 21, 18] [] [] [] [] [] [] [23] [18] [] [] [18] []\n",
      " [23] [18] [] [23] [7] [18] [] [] [23] [18, 23] [] [18] [] [23] [] [] []\n",
      " [23] [18] [23, 18] [21, 22, 23] [23] [18] [] [18] [] [21, 22, 18] [21, 22]\n",
      " [21, 22, 18] [] [21, 22, 18] [23] [] [] [21, 22] [18] [18] [18] [21, 22]\n",
      " [] [] [] [22, 21] [23] [21, 22] [23] [23] [] [23] [] [7, 18] [] [2, 18] []\n",
      " [] []]\n"
     ]
    }
   ],
   "source": [
    "print y_val[:100]\n",
    "print y_preds[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Compute max map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0291638528301\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.00774048228464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007740482284640759"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = targets_str_to_indices(Y2[target_labels].values)\n",
    "\n",
    "logging.info(\"- Compute max map7 score\")\n",
    "#map7_score(y_val, y_val, clc2[LC_TARGET_LABELS].values)\n",
    "map7_score0(y_val, y_val)\n",
    "logging.info(\"- Compute map7 score\")\n",
    "#map7_score(y_val, y_preds, clc2[LC_TARGET_LABELS].values)\n",
    "map7_score0(y_val, y_preds)\n",
    "#logging.info(\"- Compute AUC ROC : \")\n",
    "#print roc_auc_score(y_val, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.021295269099703414 (GB on 'all')\n",
    "\n",
    "0.021271936353906683 (RF tunning)\n",
    "\n",
    "0.021668245671284416 (RF tunning)\n",
    "\n",
    "0.02136609107928888\n",
    "\n",
    "0.0211362663776694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print labels_masks_dict[estimators[0][0][1]]\n",
    "# print estimators[0][1].classes_\n",
    "# print estimators[0][1].n_classes_\n",
    "# print estimators[0][1].n_features_\n",
    "# print estimators[0][1].n_outputs_\n",
    "# print estimators[0][1].estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import targets_to_labels, targets_indices_to_labels, remove_last_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Count =  1\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "[] []\n",
      "--- Count =  2\n",
      "['Securities'] ['Securities']\n",
      "[] []\n",
      "--- Count =  3\n",
      "['Credit Card'] ['Credit Card']\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "--- Count =  4\n",
      "['Payroll', 'Pensions'] ['Payroll', 'Pensions']\n",
      "[] []\n",
      "--- Count =  5\n",
      "['Payroll', 'Pensions'] ['Payroll', 'Pensions']\n",
      "[] []\n",
      "--- Count =  6\n",
      "['Credit Card'] ['Credit Card']\n",
      "[] ['Direct Debit']\n",
      "--- Count =  7\n",
      "['Taxes'] ['Taxes']\n",
      "[] []\n",
      "--- Count =  8\n",
      "['e-account'] ['e-account']\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "--- Count =  9\n",
      "['e-account'] ['e-account']\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "--- Count =  10\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "[] []\n",
      "--- Count =  11\n",
      "['e-account'] ['e-account']\n",
      "['Credit Card'] ['Credit Card']\n",
      "--- Count =  12\n",
      "['Payroll', 'Pensions'] ['Payroll', 'Pensions']\n",
      "[] []\n",
      "--- Count =  13\n",
      "['Payroll', 'Pensions'] ['Payroll', 'Pensions']\n",
      "[] []\n",
      "--- Count =  14\n",
      "['Payroll Account'] ['Payroll Account']\n",
      "[] []\n",
      "--- Count =  15\n",
      "['e-account'] ['e-account']\n",
      "['Pensions'] ['Pensions']\n",
      "--- Count =  16\n",
      "['Payroll Account'] ['Payroll Account']\n",
      "[] ['Direct Debit']\n",
      "--- Count =  17\n",
      "['Payroll Account'] ['Payroll Account']\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "--- Count =  18\n",
      "['Payroll Account'] ['Payroll Account']\n",
      "[] ['Credit Card']\n",
      "--- Count =  19\n",
      "['Payroll Account', 'Pensions'] ['Payroll Account', 'Pensions']\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "--- Count =  20\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "[] []\n",
      "--- Count =  21\n",
      "['e-account'] ['e-account']\n",
      "['Direct Debit'] ['Direct Debit']\n",
      "--- Count =  22\n",
      "['Payroll', 'Pensions'] ['Payroll', 'Pensions']\n",
      "[] []\n",
      "--- Count =  23\n",
      "['Payroll', 'Pensions'] ['Payroll', 'Pensions']\n",
      "[] []\n",
      "--- Count =  24\n",
      "['Current Accounts'] ['Current Accounts']\n",
      "['Credit Card'] ['Credit Card']\n"
     ]
    }
   ],
   "source": [
    "limit = 25\n",
    "count = 0\n",
    "\n",
    "not_predicted_predicted = defaultdict(int)\n",
    "for last_choice, targets, products, proba in zip(clc2[LC_TARGET_LABELS].values, y_val, y_preds, Y_probas.values):\n",
    "    added_products = remove_last_choice(targets, last_choice)\n",
    "    predictions = remove_last_choice(products, last_choice)\n",
    "    \n",
    "    if len(added_products) == 0:\n",
    "        continue\n",
    "        \n",
    "    if len(set(added_products) & set(predictions)) > 0:\n",
    "#         print \"Predicted : \", added_products, predictions\n",
    "#         print set(added_products) & set(predictions)\n",
    "        continue\n",
    "\n",
    "    count += 1\n",
    "    if count < limit:\n",
    "        print \"--- Count = \", count\n",
    "        print targets_indices_to_labels(added_products, TARGET_LABELS2), targets_indices_to_labels(targets, TARGET_LABELS2)\n",
    "        print targets_indices_to_labels(predictions, TARGET_LABELS2), targets_indices_to_labels(products, TARGET_LABELS2)#, proba\n",
    "    \n",
    "    for p in added_products:\n",
    "        not_predicted_predicted[TARGET_LABELS2[p]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'Long-term deposits': 1, 'Direct Debit': 69, 'e-account': 75, 'Payroll': 260, 'Pensions': 260, 'Taxes': 21, 'Payroll Account': 120, 'Securities': 3, 'Credit Card': 35, 'Current Accounts': 74, 'Junior Account': 2}) 39981\n"
     ]
    }
   ],
   "source": [
    "print not_predicted_predicted, y_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print y_probas[:10, target_groups[0]]\n",
    "#print Y[np.array(TARGET_LABELS)[target_groups[0]]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFold Cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from trainval import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_folds = 5\n",
    "results = cross_val_score((X, Y, clients_last_choice[LC_TARGET_LABELS].values), \n",
    "                            nb_folds=nb_folds, **_kwargs)\n",
    "\n",
    "print \"Cross-Validation \\n %i | %f | %f | %f | %f | %.5f \" % (nb_folds, results.min(), results.mean(), np.median(results), results.max(), results.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 201505 -> 201605 \n",
    "\n",
    "Cross-Validation \n",
    " 5 | 0.014585 | 0.018385 | 0.019147 | 0.022227 | 0.00294 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute cross-validation across several months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_folds = 3\n",
    "yms = [201504, 201505]\n",
    "#yms = [201505]\n",
    "\n",
    "for ym in yms:\n",
    "    logging.info(\"\\n-------------------------\")\n",
    "    logging.info(\"- Process month : %s\" % ym)\n",
    "    logging.info(\"-------------------------\\n\")\n",
    "    \n",
    "    ym1 = ym + 100    \n",
    "    df1 = train_df if months_ym_map[ym] in train_months else val_df\n",
    "    df2 = train_df if months_ym_map[ym1] in train_months else val_df\n",
    "    X, Y, clients_last_choice = get_XY(ym, df1, ym1, df2) \n",
    "    results = cross_val_score2((X, Y, clients_last_choice[LC_TARGET_LABELS].values), \n",
    "                                profiles=profiles,\n",
    "                                nb_folds=nb_folds)\n",
    "    print \"Cross-Validation \\n %i | %f | %f | %f | %.5f \" % (nb_folds, results.min(), results.mean(), results.max(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201505\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = train_df\n",
    "#df1 = val_df\n",
    "df2 = train_df #if months_ym_map[next_year_month] in train_months else val_df\n",
    "#df2 = val_df\n",
    "\n",
    "X, Y, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = train_all(X, Y, **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X, **_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check score on the data 2016/05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.info(\"- Compute map7 score\")\n",
    "print map7_score(y_val, y_preds, clients_last_choice[LC_TARGET_LABELS].values)\n",
    "logging.info(\"- Compute max map7 score\")\n",
    "print map7_score(y_val, y_val, clients_last_choice[LC_TARGET_LABELS].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction for 2016/06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_train_df, test_df = load_train_test([201506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months_ym_map = {}\n",
    "months = list(set(full_train_df['fecha_dato'].unique()) | set(test_df['fecha_dato'].unique()))\n",
    "for m in months:\n",
    "    months_ym_map[to_yearmonth(m)] = m\n",
    "    \n",
    "full_train_months = full_train_df['fecha_dato'].unique()\n",
    "test_months = test_df['fecha_dato'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201506\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = full_train_df\n",
    "df2 = test_df\n",
    "X, _, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clients_last_choice.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_submission(predicted_added_products, clients, clc, target_labels):\n",
    "    added_products_col = []\n",
    "    count = 0 \n",
    "    for products, last_choice in zip(predicted_added_products, clc):\n",
    "        predictions = remove_last_choice(products, last_choice)\n",
    "        added_products_col.append(' '.join([target_labels[i] for i in predictions]))\n",
    "        count+=1\n",
    "        if count % 100000 == 0:\n",
    "            logging.info(\"Elapsed : %i\", count)\n",
    "            \n",
    "    out = pd.DataFrame(data={'ncodpers': clients, 'added_products': added_products_col}, columns=['ncodpers', 'added_products'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X, **_kwargs)\n",
    "\n",
    "logging.info(\"- Get submission dataframe:\")\n",
    "clients = X['ncodpers'].values\n",
    "submission = get_submission(y_pred, clients, clients_last_choice[TARGET_LABELS].values, TARGET_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_clients = set(submission['ncodpers'].unique())\n",
    "test_clients = set(test_df['ncodpers'].unique())\n",
    "if submission_clients != test_clients:\n",
    "    missing_clients = list(test_clients - submission_clients)\n",
    "        \n",
    "#     selected_estimators = []\n",
    "#     for e in estimators:\n",
    "#         if e[0]\n",
    "        \n",
    "    \n",
    "    missing_added_products = np.zeros((len(missing_clients)))\n",
    "    submission = pd.concat([submission, \n",
    "                            pd.DataFrame(data={\n",
    "                                'ncodpers': missing_clients, \n",
    "                                'added_products': missing_added_products\n",
    "                            }, columns=['ncodpers', 'added_products'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get submission DataFrame and write csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print submission.shape\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "logging.info('- Generate submission')\n",
    "submission_file = '../results/submission_' + \\\n",
    "                  str(datetime.now().strftime(\"%Y-%m-%d-%H-%M\")) + \\\n",
    "                  '.csv'\n",
    "\n",
    "submission.to_csv(submission_file, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../results/submission_2016-11-17-16-37.csv', 'r') as r:\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
