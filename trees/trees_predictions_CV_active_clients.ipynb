{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decision trees tryouts on SPR data, inspired by Kaggle Forum \"When less is more\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and validation data as \n",
    "    month : [ Features | Targets| Difference | Last Choice Targets  ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.getLogger().handlers = []\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../common\")\n",
    "\n",
    "from dataset import load_trainval, LC_TARGET_LABELS, TARGET_LABELS_FRQ, TARGET_LABELS_DIFF\n",
    "from utils import to_yearmonth, TARGET_LABELS, TARGET_LABELS2\n",
    "from utils import target_str_to_labels, decimal_to_dummies, targets_str_to_indices, targets_dec_to_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    u'ind_empleado', u'pais_residencia',\n",
    "    u'sexo', u'age', u'ind_nuevo', u'antiguedad', u'indrel',\n",
    "    u'ult_fec_cli_1t', u'indrel_1mes', u'tiprel_1mes', u'indresi',\n",
    "    u'indext', u'conyuemp', u'canal_entrada', u'indfall', u'nomprov',\n",
    "    u'ind_actividad_cliente', u'renta', u'segmento'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Load training data : \n",
      "INFO:root:- Load data : [201504, 201505, 201601, 201602, 201604, 201605]\n",
      "INFO:root:-- Select 150000 clients\n",
      "INFO:root:- Number of lines with unknown data : 24\n",
      "INFO:root:- Number of columns with nan : 9\n",
      "INFO:root:-- Process date : 201505\n",
      "INFO:root:-- Process date : 201602\n",
      "INFO:root:-- Process date : 201605\n",
      "INFO:root:-- Add logCount columns\n",
      "INFO:root:-- Process month : 2015-04-28\n",
      "INFO:root:-- Process month : 2015-05-28\n",
      "INFO:root:-- Process month : 2016-01-28\n",
      "INFO:root:-- Process month : 2016-02-28\n",
      "INFO:root:-- Process month : 2016-04-28\n",
      "INFO:root:-- Process month : 2016-05-28\n",
      "INFO:root:-- Add logDecimal columns\n",
      "INFO:root:-- Transform age/renta/logdiff\n",
      "INFO:root:-- Add target values frequencies\n",
      "INFO:root:-- Add target diff\n"
     ]
    }
   ],
   "source": [
    "# train_yearmonths_list = [201504, 201505, 201604]\n",
    "train_yearmonths_list = [201505, 201602, 201605]\n",
    "# train_yearmonths_list = [201505]\n",
    "#val_yearmonth = [201605]\n",
    "train_nb_clients = 150000\n",
    "# train_nb_clients = 1500\n",
    "#train_df, val_df = load_trainval(train_yearmonths_list, val_yearmonth, train_nb_clients, val_nb_clients=1500)\n",
    "train_df = load_trainval(train_yearmonths_list, train_nb_clients=train_nb_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_ahor_fin_ult1_frq</th>\n",
       "      <th>ind_aval_fin_ult1_frq</th>\n",
       "      <th>ind_cco_fin_ult1_frq</th>\n",
       "      <th>ind_cder_fin_ult1_frq</th>\n",
       "      <th>ind_cno_fin_ult1_frq</th>\n",
       "      <th>ind_ctju_fin_ult1_frq</th>\n",
       "      <th>ind_ctma_fin_ult1_frq</th>\n",
       "      <th>ind_ctop_fin_ult1_frq</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_frq</th>\n",
       "      <th>ind_plan_fin_ult1_frq</th>\n",
       "      <th>ind_pres_fin_ult1_frq</th>\n",
       "      <th>ind_reca_fin_ult1_frq</th>\n",
       "      <th>ind_tjcr_fin_ult1_frq</th>\n",
       "      <th>ind_valo_fin_ult1_frq</th>\n",
       "      <th>ind_viv_fin_ult1_frq</th>\n",
       "      <th>ind_nomina_ult1_frq</th>\n",
       "      <th>ind_nom_pens_ult1_frq</th>\n",
       "      <th>ind_recibo_ult1_frq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210118</th>\n",
       "      <td>2015-04-28</td>\n",
       "      <td>15897</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.771917</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.05683</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.072066</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051666</th>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>15897</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.771917</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.05683</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638553</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>15897</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.771917</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.05683</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663084</th>\n",
       "      <td>2016-02-28</td>\n",
       "      <td>15897</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.771917</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.05683</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.072066</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532963</th>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>15897</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.228083</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.05683</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.072066</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338251</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15897</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.228083</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.05683</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.072066</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210141</th>\n",
       "      <td>2015-04-28</td>\n",
       "      <td>15920</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.771917</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051644</th>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>15920</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.771917</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638479</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>15920</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.771917</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.897138</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663097</th>\n",
       "      <td>2016-02-28</td>\n",
       "      <td>15920</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.771917</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.897138</td>\n",
       "      <td>0.988494</td>\n",
       "      <td>0.989508</td>\n",
       "      <td>0.165461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fecha_dato  ncodpers  ind_ahor_fin_ult1_frq  ind_aval_fin_ult1_frq  \\\n",
       "210118   2015-04-28     15897               0.999877               0.999974   \n",
       "1051666  2015-05-28     15897               0.999877               0.999974   \n",
       "1638553  2016-01-28     15897               0.999877               0.999974   \n",
       "2663084  2016-02-28     15897               0.999877               0.999974   \n",
       "3532963  2016-04-28     15897               0.999877               0.999974   \n",
       "4338251  2016-05-28     15897               0.999877               0.999974   \n",
       "210141   2015-04-28     15920               0.999877               0.999974   \n",
       "1051644  2015-05-28     15920               0.999877               0.999974   \n",
       "1638479  2016-01-28     15920               0.999877               0.999974   \n",
       "2663097  2016-02-28     15920               0.999877               0.999974   \n",
       "\n",
       "         ind_cco_fin_ult1_frq  ind_cder_fin_ult1_frq  ind_cno_fin_ult1_frq  \\\n",
       "210118               0.771917               0.999471              0.102862   \n",
       "1051666              0.771917               0.999471              0.102862   \n",
       "1638553              0.771917               0.999471              0.102862   \n",
       "2663084              0.771917               0.999471              0.102862   \n",
       "3532963              0.228083               0.999471              0.102862   \n",
       "4338251              0.228083               0.999471              0.102862   \n",
       "210141               0.771917               0.999471              0.102862   \n",
       "1051644              0.771917               0.999471              0.102862   \n",
       "1638479              0.771917               0.999471              0.897138   \n",
       "2663097              0.771917               0.999471              0.897138   \n",
       "\n",
       "         ind_ctju_fin_ult1_frq  ind_ctma_fin_ult1_frq  ind_ctop_fin_ult1_frq  \\\n",
       "210118                0.988494               0.989508               0.165461   \n",
       "1051666               0.988494               0.989508               0.165461   \n",
       "1638553               0.988494               0.989508               0.165461   \n",
       "2663084               0.988494               0.989508               0.165461   \n",
       "3532963               0.988494               0.989508               0.165461   \n",
       "4338251               0.988494               0.989508               0.165461   \n",
       "210141                0.988494               0.989508               0.165461   \n",
       "1051644               0.988494               0.989508               0.165461   \n",
       "1638479               0.988494               0.989508               0.165461   \n",
       "2663097               0.988494               0.989508               0.165461   \n",
       "\n",
       "                ...           ind_hip_fin_ult1_frq  ind_plan_fin_ult1_frq  \\\n",
       "210118          ...                       0.992379               0.012021   \n",
       "1051666         ...                       0.992379               0.012021   \n",
       "1638553         ...                       0.992379               0.012021   \n",
       "2663084         ...                       0.992379               0.012021   \n",
       "3532963         ...                       0.992379               0.012021   \n",
       "4338251         ...                       0.992379               0.012021   \n",
       "210141          ...                       0.992379               0.987979   \n",
       "1051644         ...                       0.992379               0.987979   \n",
       "1638479         ...                       0.992379               0.987979   \n",
       "2663097         ...                       0.992379               0.987979   \n",
       "\n",
       "         ind_pres_fin_ult1_frq  ind_reca_fin_ult1_frq  ind_tjcr_fin_ult1_frq  \\\n",
       "210118                0.996619               0.067668                0.05683   \n",
       "1051666               0.996619               0.067668                0.05683   \n",
       "1638553               0.996619               0.067668                0.05683   \n",
       "2663084               0.996619               0.067668                0.05683   \n",
       "3532963               0.996619               0.067668                0.05683   \n",
       "4338251               0.996619               0.067668                0.05683   \n",
       "210141                0.996619               0.067668                0.94317   \n",
       "1051644               0.996619               0.067668                0.94317   \n",
       "1638479               0.996619               0.067668                0.94317   \n",
       "2663097               0.996619               0.067668                0.94317   \n",
       "\n",
       "         ind_valo_fin_ult1_frq  ind_viv_fin_ult1_frq  ind_nomina_ult1_frq  \\\n",
       "210118                 0.03342              0.994801              0.93296   \n",
       "1051666                0.03342              0.994801              0.93296   \n",
       "1638553                0.03342              0.994801              0.93296   \n",
       "2663084                0.03342              0.994801              0.93296   \n",
       "3532963                0.03342              0.994801              0.93296   \n",
       "4338251                0.03342              0.994801              0.93296   \n",
       "210141                 0.96658              0.994801              0.93296   \n",
       "1051644                0.96658              0.994801              0.93296   \n",
       "1638479                0.03342              0.994801              0.93296   \n",
       "2663097                0.03342              0.994801              0.93296   \n",
       "\n",
       "         ind_nom_pens_ult1_frq  ind_recibo_ult1_frq  \n",
       "210118                0.072066             0.159387  \n",
       "1051666               0.927934             0.159387  \n",
       "1638553               0.927934             0.159387  \n",
       "2663084               0.072066             0.159387  \n",
       "3532963               0.072066             0.159387  \n",
       "4338251               0.072066             0.159387  \n",
       "210141                0.927934             0.159387  \n",
       "1051644               0.927934             0.159387  \n",
       "1638479               0.927934             0.159387  \n",
       "2663097               0.927934             0.159387  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['fecha_dato', 'ncodpers'] + TARGET_LABELS_FRQ.tolist()].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_common_clients(df1, mask1, mask2, df2=None):\n",
    "    active_clients1 = df1[mask1]['ncodpers'].unique()\n",
    "    if df2 is not None:\n",
    "        active_clients2 = df2[mask2]['ncodpers'].unique()\n",
    "    else:\n",
    "        active_clients2 = df1[mask2]['ncodpers'].unique()\n",
    "    active_clients = list(set(active_clients1) & set(active_clients2)) \n",
    "    \n",
    "    if df2 is not None:\n",
    "        return df1['ncodpers'].isin(active_clients), df2['ncodpers'].isin(active_clients)\n",
    "    return df1['ncodpers'].isin(active_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months_ym_map = {}\n",
    "# months = list(set(train_df['fecha_dato'].unique()) | set(val_df['fecha_dato'].unique()))\n",
    "months = train_df['fecha_dato'].unique()\n",
    "for m in months:\n",
    "    months_ym_map[to_yearmonth(m)] = m\n",
    "\n",
    "        \n",
    "train_months = train_df['fecha_dato'].unique()\n",
    "# val_months = val_df['fecha_dato'].unique()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import get_added_products, remove_last_choice, apk, map7_score\n",
    "from visualization import visualize_train_test, visualize_folds, compare_two_datasets, compare_folds, compare_folds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_features = ['targets_diff', 'targets_logdiff', 'targets_logcount2_diff', 'targets_logcount2', 'targets_logcount1', 'targets_logDec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_XY(current_month, df1, next_year_month, df2, months_ym_map):\n",
    "    month_mask = df1['fecha_dato'] == months_ym_map[current_month]\n",
    "    next_year_month_mask = df2['fecha_dato'] == months_ym_map[next_year_month]\n",
    "    next_year_prev_month_mask = df2['fecha_dato'] == months_ym_map[next_year_month - 1]\n",
    "    \n",
    "    # get common clients from df1 at this month and df2 at next year month\n",
    "    common_clients_mask1, common_clients_mask2 = get_common_clients(df1, month_mask, next_year_month_mask, df2)\n",
    "    common_clients_mask2, common_clients_mask3 = get_common_clients(df2, common_clients_mask2 & next_year_month_mask, next_year_prev_month_mask, df2)\n",
    "        \n",
    "    c1 = df1[common_clients_mask1 & month_mask]['ncodpers'].values\n",
    "    c2 = df2[common_clients_mask2 & next_year_month_mask]['ncodpers'].values\n",
    "    c3 = df2[common_clients_mask3 & next_year_prev_month_mask]['ncodpers'].values\n",
    "    assert (c1 == c2).all() and (c2 == c3).all(), \"Problem with common clients\" \n",
    "    \n",
    "    X = df1[common_clients_mask1 & month_mask][['ncodpers', 'fecha_dato'] + target_features + features + TARGET_LABELS_FRQ.tolist()]            \n",
    "   \n",
    "    if TARGET_LABELS[0] in df2.columns and TARGET_LABELS_DIFF[0] in df2.columns and not df2[next_year_month_mask][TARGET_LABELS].isnull().all().all():\n",
    "        Y = df2[common_clients_mask2 & next_year_month_mask][['ncodpers', 'fecha_dato', 'targets_str', 'lc_targets_str', 'targets_diff'] + TARGET_LABELS + TARGET_LABELS_DIFF.tolist()]    \n",
    "        assert (X['ncodpers'].values == Y['ncodpers'].values).all(), \"There is a problem in alignment\"\n",
    "        Y.index = X.index                \n",
    "    else:\n",
    "        Y = None\n",
    "        \n",
    "    if TARGET_LABELS_FRQ[0] in df2.columns and not df2[next_year_prev_month_mask][TARGET_LABELS].isnull().all().all():\n",
    "        # Add TARGET_LABELS_FRQ from previous month to X:\n",
    "        target_labels_frq = df2[common_clients_mask3 & next_year_prev_month_mask][['ncodpers'] + TARGET_LABELS_FRQ.tolist()]\n",
    "        assert (X['ncodpers'].values == target_labels_frq['ncodpers'].values).all(), \"There is a problem in alignment\"\n",
    "        target_labels_frq = target_labels_frq[TARGET_LABELS_FRQ]\n",
    "        target_labels_frq.columns = [c + '_prev' for c in TARGET_LABELS_FRQ]\n",
    "        target_labels_frq.index = X.index\n",
    "        X = pd.concat([X, target_labels_frq], axis=1)        \n",
    "\n",
    "    \n",
    "    if LC_TARGET_LABELS[0] in df2.columns:\n",
    "        clients_last_choice = df2[common_clients_mask2 & next_year_month_mask][['ncodpers', 'fecha_dato', 'targets_str'] + LC_TARGET_LABELS.tolist()]\n",
    "    else:\n",
    "        clients_last_choice = None\n",
    "        \n",
    "    return X, Y, clients_last_choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201505\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = train_df if months_ym_map[current_month] in train_months else val_df\n",
    "#df1 = train_df\n",
    "df2 = train_df if months_ym_map[next_year_month] in train_months else val_df\n",
    "#df2 = train_df\n",
    "\n",
    "X, Y, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert (X['ncodpers'].values == Y['ncodpers'].values).all(), \"WTF\"\n",
    "assert (X['ncodpers'].values == clients_last_choice['ncodpers'].values).all(), \"WTF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149988, 75)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>targets_diff</th>\n",
       "      <th>targets_logdiff</th>\n",
       "      <th>targets_logcount2_diff</th>\n",
       "      <th>targets_logcount2</th>\n",
       "      <th>targets_logcount1</th>\n",
       "      <th>targets_logDec</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_frq_prev</th>\n",
       "      <th>ind_plan_fin_ult1_frq_prev</th>\n",
       "      <th>ind_pres_fin_ult1_frq_prev</th>\n",
       "      <th>ind_reca_fin_ult1_frq_prev</th>\n",
       "      <th>ind_tjcr_fin_ult1_frq_prev</th>\n",
       "      <th>ind_valo_fin_ult1_frq_prev</th>\n",
       "      <th>ind_viv_fin_ult1_frq_prev</th>\n",
       "      <th>ind_nomina_ult1_frq_prev</th>\n",
       "      <th>ind_nom_pens_ult1_frq_prev</th>\n",
       "      <th>ind_recibo_ult1_frq_prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1051666</th>\n",
       "      <td>15897</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.098612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>14.805207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.05683</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.072066</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051644</th>\n",
       "      <td>15920</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>14.803952</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051649</th>\n",
       "      <td>15925</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>14.586878</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.932332</td>\n",
       "      <td>0.05683</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051651</th>\n",
       "      <td>15927</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>10.424244</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.932332</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051655</th>\n",
       "      <td>15932</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.932332</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.840613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051678</th>\n",
       "      <td>15937</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>14.792953</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051680</th>\n",
       "      <td>15939</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>14.557312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.932332</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051700</th>\n",
       "      <td>15940</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>14.589708</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.932332</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.03342</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051706</th>\n",
       "      <td>15948</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>5.549076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.932332</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.840613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051712</th>\n",
       "      <td>15956</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>14.601901</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992379</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.932332</td>\n",
       "      <td>0.94317</td>\n",
       "      <td>0.96658</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.93296</td>\n",
       "      <td>0.927934</td>\n",
       "      <td>0.159387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ncodpers  fecha_dato  targets_diff  targets_logdiff  \\\n",
       "1051666     15897  2015-05-28          -2.0        -1.098612   \n",
       "1051644     15920  2015-05-28           0.0         0.000000   \n",
       "1051649     15925  2015-05-28           0.0         0.000000   \n",
       "1051651     15927  2015-05-28           0.0         0.000000   \n",
       "1051655     15932  2015-05-28           0.0         0.000000   \n",
       "1051678     15937  2015-05-28           0.0         0.000000   \n",
       "1051680     15939  2015-05-28           0.0         0.000000   \n",
       "1051700     15940  2015-05-28           0.0         0.000000   \n",
       "1051706     15948  2015-05-28           0.0         0.000000   \n",
       "1051712     15956  2015-05-28           0.0         0.000000   \n",
       "\n",
       "         targets_logcount2_diff  targets_logcount2  targets_logcount1  \\\n",
       "1051666                     0.0           0.000002           0.000007   \n",
       "1051644                     0.0           0.000046           0.000067   \n",
       "1051649                     0.0           0.001333           0.001527   \n",
       "1051651                     0.0           0.000002           0.000007   \n",
       "1051655                     0.0           0.001372           0.001367   \n",
       "1051678                     0.0           0.000003           0.000007   \n",
       "1051680                     0.0           0.000064           0.000087   \n",
       "1051700                     0.0           0.000020           0.000027   \n",
       "1051706                     0.0           0.001520           0.001540   \n",
       "1051712                     0.0           0.000604           0.000560   \n",
       "\n",
       "         targets_logDec  ind_empleado  pais_residencia  \\\n",
       "1051666       14.805207             1                0   \n",
       "1051644       14.803952             3                0   \n",
       "1051649       14.586878             3                0   \n",
       "1051651       10.424244             2                0   \n",
       "1051655        2.833213             0                0   \n",
       "1051678       14.792953             1                0   \n",
       "1051680       14.557312             0                0   \n",
       "1051700       14.589708             2                0   \n",
       "1051706        5.549076             0                0   \n",
       "1051712       14.601901             2                0   \n",
       "\n",
       "                   ...             ind_hip_fin_ult1_frq_prev  \\\n",
       "1051666            ...                              0.992379   \n",
       "1051644            ...                              0.992379   \n",
       "1051649            ...                              0.992379   \n",
       "1051651            ...                              0.992379   \n",
       "1051655            ...                              0.992379   \n",
       "1051678            ...                              0.992379   \n",
       "1051680            ...                              0.007621   \n",
       "1051700            ...                              0.992379   \n",
       "1051706            ...                              0.992379   \n",
       "1051712            ...                              0.992379   \n",
       "\n",
       "         ind_plan_fin_ult1_frq_prev  ind_pres_fin_ult1_frq_prev  \\\n",
       "1051666                    0.012021                    0.996619   \n",
       "1051644                    0.987979                    0.996619   \n",
       "1051649                    0.987979                    0.996619   \n",
       "1051651                    0.012021                    0.996619   \n",
       "1051655                    0.987979                    0.996619   \n",
       "1051678                    0.012021                    0.996619   \n",
       "1051680                    0.987979                    0.996619   \n",
       "1051700                    0.987979                    0.996619   \n",
       "1051706                    0.012021                    0.996619   \n",
       "1051712                    0.987979                    0.996619   \n",
       "\n",
       "         ind_reca_fin_ult1_frq_prev  ind_tjcr_fin_ult1_frq_prev  \\\n",
       "1051666                    0.067668                     0.05683   \n",
       "1051644                    0.067668                     0.94317   \n",
       "1051649                    0.932332                     0.05683   \n",
       "1051651                    0.932332                     0.94317   \n",
       "1051655                    0.932332                     0.94317   \n",
       "1051678                    0.067668                     0.94317   \n",
       "1051680                    0.932332                     0.94317   \n",
       "1051700                    0.932332                     0.94317   \n",
       "1051706                    0.932332                     0.94317   \n",
       "1051712                    0.932332                     0.94317   \n",
       "\n",
       "         ind_valo_fin_ult1_frq_prev  ind_viv_fin_ult1_frq_prev  \\\n",
       "1051666                     0.03342                   0.994801   \n",
       "1051644                     0.03342                   0.994801   \n",
       "1051649                     0.96658                   0.994801   \n",
       "1051651                     0.96658                   0.994801   \n",
       "1051655                     0.03342                   0.994801   \n",
       "1051678                     0.03342                   0.994801   \n",
       "1051680                     0.96658                   0.994801   \n",
       "1051700                     0.03342                   0.994801   \n",
       "1051706                     0.96658                   0.994801   \n",
       "1051712                     0.96658                   0.994801   \n",
       "\n",
       "         ind_nomina_ult1_frq_prev  ind_nom_pens_ult1_frq_prev  \\\n",
       "1051666                   0.93296                    0.072066   \n",
       "1051644                   0.93296                    0.927934   \n",
       "1051649                   0.93296                    0.927934   \n",
       "1051651                   0.93296                    0.927934   \n",
       "1051655                   0.93296                    0.927934   \n",
       "1051678                   0.93296                    0.927934   \n",
       "1051680                   0.93296                    0.927934   \n",
       "1051700                   0.93296                    0.927934   \n",
       "1051706                   0.93296                    0.927934   \n",
       "1051712                   0.93296                    0.927934   \n",
       "\n",
       "         ind_recibo_ult1_frq_prev  \n",
       "1051666                  0.159387  \n",
       "1051644                  0.159387  \n",
       "1051649                  0.159387  \n",
       "1051651                  0.159387  \n",
       "1051655                  0.840613  \n",
       "1051678                  0.159387  \n",
       "1051680                  0.159387  \n",
       "1051700                  0.159387  \n",
       "1051706                  0.840613  \n",
       "1051712                  0.159387  \n",
       "\n",
       "[10 rows x 75 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print X.shape\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149988, 53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>targets_str</th>\n",
       "      <th>lc_targets_str</th>\n",
       "      <th>ind_ahor_fin_ult1_diff</th>\n",
       "      <th>ind_aval_fin_ult1_diff</th>\n",
       "      <th>ind_cco_fin_ult1_diff</th>\n",
       "      <th>ind_cder_fin_ult1_diff</th>\n",
       "      <th>ind_cno_fin_ult1_diff</th>\n",
       "      <th>ind_ctju_fin_ult1_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_diff</th>\n",
       "      <th>ind_plan_fin_ult1_diff</th>\n",
       "      <th>ind_pres_fin_ult1_diff</th>\n",
       "      <th>ind_reca_fin_ult1_diff</th>\n",
       "      <th>ind_tjcr_fin_ult1_diff</th>\n",
       "      <th>ind_valo_fin_ult1_diff</th>\n",
       "      <th>ind_viv_fin_ult1_diff</th>\n",
       "      <th>ind_nomina_ult1_diff</th>\n",
       "      <th>ind_nom_pens_ult1_diff</th>\n",
       "      <th>ind_recibo_ult1_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1051641</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15988</td>\n",
       "      <td>001000000000000000100000</td>\n",
       "      <td>001000000000000000000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051627</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16056</td>\n",
       "      <td>001010001000000000000110</td>\n",
       "      <td>001010001000000000000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051616</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16125</td>\n",
       "      <td>001000010000100000000001</td>\n",
       "      <td>001000000000100000000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051805</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16202</td>\n",
       "      <td>001010000000010001100111</td>\n",
       "      <td>001010000000010001000111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051739</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16294</td>\n",
       "      <td>000010000000100000000111</td>\n",
       "      <td>000010000000000000000111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051779</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16506</td>\n",
       "      <td>000010000000001001000111</td>\n",
       "      <td>000010000000001001000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051792</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16525</td>\n",
       "      <td>001010001000110100100111</td>\n",
       "      <td>001010000000110100100111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051258</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16787</td>\n",
       "      <td>001000000000100101110111</td>\n",
       "      <td>001000000000100101110001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051523</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>16988</td>\n",
       "      <td>000010001000000001100011</td>\n",
       "      <td>000010001000000001000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051478</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>17236</td>\n",
       "      <td>000010000000100101110111</td>\n",
       "      <td>000010000000100101110011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fecha_dato  ncodpers               targets_str  \\\n",
       "1051641  2016-05-28     15988  001000000000000000100000   \n",
       "1051627  2016-05-28     16056  001010001000000000000110   \n",
       "1051616  2016-05-28     16125  001000010000100000000001   \n",
       "1051805  2016-05-28     16202  001010000000010001100111   \n",
       "1051739  2016-05-28     16294  000010000000100000000111   \n",
       "1051779  2016-05-28     16506  000010000000001001000111   \n",
       "1051792  2016-05-28     16525  001010001000110100100111   \n",
       "1051258  2016-05-28     16787  001000000000100101110111   \n",
       "1051523  2016-05-28     16988  000010001000000001100011   \n",
       "1051478  2016-05-28     17236  000010000000100101110111   \n",
       "\n",
       "                   lc_targets_str  ind_ahor_fin_ult1_diff  \\\n",
       "1051641  001000000000000000000000                     0.0   \n",
       "1051627  001010001000000000000001                     0.0   \n",
       "1051616  001000000000100000000001                     0.0   \n",
       "1051805  001010000000010001000111                     0.0   \n",
       "1051739  000010000000000000000111                     0.0   \n",
       "1051779  000010000000001001000001                     0.0   \n",
       "1051792  001010000000110100100111                     0.0   \n",
       "1051258  001000000000100101110001                     0.0   \n",
       "1051523  000010001000000001000011                     0.0   \n",
       "1051478  000010000000100101110011                     0.0   \n",
       "\n",
       "         ind_aval_fin_ult1_diff  ind_cco_fin_ult1_diff  \\\n",
       "1051641                     0.0                    0.0   \n",
       "1051627                     0.0                    0.0   \n",
       "1051616                     0.0                    0.0   \n",
       "1051805                     0.0                    0.0   \n",
       "1051739                     0.0                    0.0   \n",
       "1051779                     0.0                    0.0   \n",
       "1051792                     0.0                    0.0   \n",
       "1051258                     0.0                    0.0   \n",
       "1051523                     0.0                    0.0   \n",
       "1051478                     0.0                    0.0   \n",
       "\n",
       "         ind_cder_fin_ult1_diff  ind_cno_fin_ult1_diff  \\\n",
       "1051641                     0.0                    0.0   \n",
       "1051627                     0.0                    0.0   \n",
       "1051616                     0.0                    0.0   \n",
       "1051805                     0.0                    0.0   \n",
       "1051739                     0.0                    0.0   \n",
       "1051779                     0.0                    0.0   \n",
       "1051792                     0.0                    0.0   \n",
       "1051258                     0.0                    0.0   \n",
       "1051523                     0.0                    0.0   \n",
       "1051478                     0.0                    0.0   \n",
       "\n",
       "         ind_ctju_fin_ult1_diff          ...           ind_hip_fin_ult1_diff  \\\n",
       "1051641                     0.0          ...                             0.0   \n",
       "1051627                     0.0          ...                             0.0   \n",
       "1051616                     0.0          ...                             0.0   \n",
       "1051805                     0.0          ...                             0.0   \n",
       "1051739                     0.0          ...                             0.0   \n",
       "1051779                     0.0          ...                             0.0   \n",
       "1051792                     0.0          ...                             0.0   \n",
       "1051258                     0.0          ...                             0.0   \n",
       "1051523                     0.0          ...                             0.0   \n",
       "1051478                     0.0          ...                             0.0   \n",
       "\n",
       "         ind_plan_fin_ult1_diff  ind_pres_fin_ult1_diff  \\\n",
       "1051641                     0.0                     0.0   \n",
       "1051627                     0.0                     0.0   \n",
       "1051616                     0.0                     0.0   \n",
       "1051805                     0.0                     0.0   \n",
       "1051739                     0.0                     0.0   \n",
       "1051779                     0.0                     0.0   \n",
       "1051792                     0.0                     0.0   \n",
       "1051258                     0.0                     0.0   \n",
       "1051523                     0.0                     0.0   \n",
       "1051478                     0.0                     0.0   \n",
       "\n",
       "         ind_reca_fin_ult1_diff  ind_tjcr_fin_ult1_diff  \\\n",
       "1051641                     0.0                     1.0   \n",
       "1051627                     0.0                     0.0   \n",
       "1051616                     0.0                     0.0   \n",
       "1051805                     0.0                     1.0   \n",
       "1051739                     0.0                     0.0   \n",
       "1051779                     0.0                     0.0   \n",
       "1051792                     0.0                     0.0   \n",
       "1051258                     0.0                     0.0   \n",
       "1051523                     0.0                     1.0   \n",
       "1051478                     0.0                     0.0   \n",
       "\n",
       "         ind_valo_fin_ult1_diff  ind_viv_fin_ult1_diff  ind_nomina_ult1_diff  \\\n",
       "1051641                     0.0                    0.0                   0.0   \n",
       "1051627                     0.0                    0.0                   1.0   \n",
       "1051616                     0.0                    0.0                   0.0   \n",
       "1051805                     0.0                    0.0                   0.0   \n",
       "1051739                     0.0                    0.0                   0.0   \n",
       "1051779                     0.0                    0.0                   1.0   \n",
       "1051792                     0.0                    0.0                   0.0   \n",
       "1051258                     0.0                    0.0                   1.0   \n",
       "1051523                     0.0                    0.0                   0.0   \n",
       "1051478                     0.0                    0.0                   1.0   \n",
       "\n",
       "         ind_nom_pens_ult1_diff  ind_recibo_ult1_diff  \n",
       "1051641                     0.0                   0.0  \n",
       "1051627                     1.0                   0.0  \n",
       "1051616                     0.0                   0.0  \n",
       "1051805                     0.0                   0.0  \n",
       "1051739                     0.0                   0.0  \n",
       "1051779                     1.0                   0.0  \n",
       "1051792                     0.0                   0.0  \n",
       "1051258                     1.0                   0.0  \n",
       "1051523                     0.0                   0.0  \n",
       "1051478                     0.0                   0.0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print Y.shape\n",
    "Y[Y['targets_diff'] > 0][['fecha_dato', 'ncodpers', 'targets_str', 'lc_targets_str'] + TARGET_LABELS_DIFF.tolist() ].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149988, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>targets_str</th>\n",
       "      <th>lc_ind_ahor_fin_ult1</th>\n",
       "      <th>lc_ind_aval_fin_ult1</th>\n",
       "      <th>lc_ind_cco_fin_ult1</th>\n",
       "      <th>lc_ind_cder_fin_ult1</th>\n",
       "      <th>lc_ind_cno_fin_ult1</th>\n",
       "      <th>lc_ind_ctju_fin_ult1</th>\n",
       "      <th>lc_ind_ctma_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>lc_ind_hip_fin_ult1</th>\n",
       "      <th>lc_ind_plan_fin_ult1</th>\n",
       "      <th>lc_ind_pres_fin_ult1</th>\n",
       "      <th>lc_ind_reca_fin_ult1</th>\n",
       "      <th>lc_ind_tjcr_fin_ult1</th>\n",
       "      <th>lc_ind_valo_fin_ult1</th>\n",
       "      <th>lc_ind_viv_fin_ult1</th>\n",
       "      <th>lc_ind_nomina_ult1</th>\n",
       "      <th>lc_ind_nom_pens_ult1</th>\n",
       "      <th>lc_ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4338251</th>\n",
       "      <td>15897</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>000010010000110101110011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338261</th>\n",
       "      <td>15920</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000010000000001010001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338284</th>\n",
       "      <td>15925</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000010000000000100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338282</th>\n",
       "      <td>15927</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>000000001000000100000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338278</th>\n",
       "      <td>15932</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>000000000000000000010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338273</th>\n",
       "      <td>15937</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001010001000110101010001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338271</th>\n",
       "      <td>15939</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000000000101000000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338270</th>\n",
       "      <td>15940</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000010001100000010001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338263</th>\n",
       "      <td>15948</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>000000000000000100000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338200</th>\n",
       "      <td>15956</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>001000011000000000000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ncodpers  fecha_dato               targets_str  lc_ind_ahor_fin_ult1  \\\n",
       "4338251     15897  2016-05-28  000010010000110101110011                   0.0   \n",
       "4338261     15920  2016-05-28  001000010000000001010001                   0.0   \n",
       "4338284     15925  2016-05-28  001000010000000000100000                   0.0   \n",
       "4338282     15927  2016-05-28  000000001000000100000001                   0.0   \n",
       "4338278     15932  2016-05-28  000000000000000000010000                   0.0   \n",
       "4338273     15937  2016-05-28  001010001000110101010001                   0.0   \n",
       "4338271     15939  2016-05-28  001000000000101000000001                   0.0   \n",
       "4338270     15940  2016-05-28  001000010001100000010001                   0.0   \n",
       "4338263     15948  2016-05-28  000000000000000100000000                   0.0   \n",
       "4338200     15956  2016-05-28  001000011000000000000001                   0.0   \n",
       "\n",
       "         lc_ind_aval_fin_ult1  lc_ind_cco_fin_ult1  lc_ind_cder_fin_ult1  \\\n",
       "4338251                   0.0                  0.0                   0.0   \n",
       "4338261                   0.0                  1.0                   0.0   \n",
       "4338284                   0.0                  1.0                   0.0   \n",
       "4338282                   0.0                  1.0                   0.0   \n",
       "4338278                   0.0                  0.0                   0.0   \n",
       "4338273                   0.0                  1.0                   0.0   \n",
       "4338271                   0.0                  1.0                   0.0   \n",
       "4338270                   0.0                  1.0                   0.0   \n",
       "4338263                   0.0                  0.0                   0.0   \n",
       "4338200                   0.0                  1.0                   0.0   \n",
       "\n",
       "         lc_ind_cno_fin_ult1  lc_ind_ctju_fin_ult1  lc_ind_ctma_fin_ult1  \\\n",
       "4338251                  1.0                   0.0                   0.0   \n",
       "4338261                  0.0                   0.0                   0.0   \n",
       "4338284                  0.0                   0.0                   0.0   \n",
       "4338282                  0.0                   0.0                   0.0   \n",
       "4338278                  0.0                   0.0                   0.0   \n",
       "4338273                  1.0                   0.0                   0.0   \n",
       "4338271                  0.0                   0.0                   0.0   \n",
       "4338270                  0.0                   0.0                   0.0   \n",
       "4338263                  0.0                   0.0                   0.0   \n",
       "4338200                  0.0                   0.0                   0.0   \n",
       "\n",
       "                ...          lc_ind_hip_fin_ult1  lc_ind_plan_fin_ult1  \\\n",
       "4338251         ...                          0.0                   1.0   \n",
       "4338261         ...                          0.0                   0.0   \n",
       "4338284         ...                          0.0                   0.0   \n",
       "4338282         ...                          0.0                   1.0   \n",
       "4338278         ...                          0.0                   0.0   \n",
       "4338273         ...                          0.0                   1.0   \n",
       "4338271         ...                          1.0                   0.0   \n",
       "4338270         ...                          0.0                   0.0   \n",
       "4338263         ...                          0.0                   1.0   \n",
       "4338200         ...                          0.0                   0.0   \n",
       "\n",
       "         lc_ind_pres_fin_ult1  lc_ind_reca_fin_ult1  lc_ind_tjcr_fin_ult1  \\\n",
       "4338251                   0.0                   1.0                   1.0   \n",
       "4338261                   0.0                   1.0                   0.0   \n",
       "4338284                   0.0                   0.0                   1.0   \n",
       "4338282                   0.0                   0.0                   0.0   \n",
       "4338278                   0.0                   0.0                   0.0   \n",
       "4338273                   0.0                   1.0                   0.0   \n",
       "4338271                   0.0                   0.0                   0.0   \n",
       "4338270                   0.0                   0.0                   0.0   \n",
       "4338263                   0.0                   0.0                   0.0   \n",
       "4338200                   0.0                   0.0                   0.0   \n",
       "\n",
       "         lc_ind_valo_fin_ult1  lc_ind_viv_fin_ult1  lc_ind_nomina_ult1  \\\n",
       "4338251                   1.0                  0.0                 0.0   \n",
       "4338261                   1.0                  0.0                 0.0   \n",
       "4338284                   0.0                  0.0                 0.0   \n",
       "4338282                   0.0                  0.0                 0.0   \n",
       "4338278                   1.0                  0.0                 0.0   \n",
       "4338273                   1.0                  0.0                 0.0   \n",
       "4338271                   0.0                  0.0                 0.0   \n",
       "4338270                   1.0                  0.0                 0.0   \n",
       "4338263                   0.0                  0.0                 0.0   \n",
       "4338200                   0.0                  0.0                 0.0   \n",
       "\n",
       "         lc_ind_nom_pens_ult1  lc_ind_recibo_ult1  \n",
       "4338251                   1.0                 1.0  \n",
       "4338261                   0.0                 1.0  \n",
       "4338284                   0.0                 1.0  \n",
       "4338282                   0.0                 1.0  \n",
       "4338278                   0.0                 0.0  \n",
       "4338273                   0.0                 1.0  \n",
       "4338271                   0.0                 1.0  \n",
       "4338270                   0.0                 1.0  \n",
       "4338263                   0.0                 0.0  \n",
       "4338200                   0.0                 1.0  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print clients_last_choice.shape\n",
    "clients_last_choice.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another train/predict + CV implementation\n",
    "\n",
    "### Input\n",
    "\n",
    "- `X` : `[nb_samples, nb_features]` shaped pd.DataFrame\n",
    "    - `features_masks_list` : `{fm1_name: features_mask_1, fm2_name: features_mask_2, ...]` with `features_mask_i` is a list of feature column names. They can oversect.\n",
    "    \n",
    "- `Y` : `[nb_samples, nb_labels]` shaped pd.DataFrame\n",
    "    - `labels_masks_list` : `{lm1_name: labels_mask_1, lm2_name: labels_mask_2, ...}` with `labels_mask_i` is a list of labels column names. They can oversect.\n",
    "\n",
    "- `samples_masks_list` : `[samples_mask_1, samples_mask_2, ...]` with samples_mask_i is a function to produce a boolean pd.DataFrame . Used only for training. \n",
    "\n",
    "\n",
    "- Set of models `models` : list of functions to create a model, e.g. `[create_RF, create_NN, create_GBT]`\n",
    "\n",
    "\n",
    "### Training phase\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_masks_list = [\n",
    "#    lambda x:  ~(x['targets_diff'].isin([0])), \n",
    "#     lambda x, y:  x['targets_diff'] > 0, \n",
    "#     lambda x, y:  x['targets_diff'] < 0, \n",
    "    lambda x, y:  (x['targets_diff'] > 0) | (y['targets_diff'] > 0), \n",
    "    lambda x, y:  (x['targets_diff'] < 0) | (y['targets_diff'] < 0), \n",
    "#     lambda x, y:  (y['targets_diff'] > 0), \n",
    "#     lambda x, y:  y['targets_diff'] < 0, \n",
    "]\n",
    "\n",
    "TARGET_LABELS_FRQ_PREV = [c + '_prev' for c in TARGET_LABELS_FRQ]\n",
    "\n",
    "features_masks_dict = {\n",
    "#     'fm_all': None,\n",
    "    'fm0': features + target_features + TARGET_LABELS_FRQ.tolist() + TARGET_LABELS_FRQ_PREV,\n",
    "#     'fm1': ['pais_residencia', 'sexo', 'age', 'ind_nuevo', 'segmento', 'ind_empleado', 'ind_actividad_cliente', 'indresi'],\n",
    "    'fm2': target_features,\n",
    "#     'fm3': ['pais_residencia', 'sexo', 'age', 'segmento', 'renta'],\n",
    "#     'fm4': ['pais_residencia', 'sexo', 'age', 'renta', 'targets_logdiff', 'targets_logcount2_diff','targets_logcount2','targets_logcount1'],\n",
    "    'fm5': ['nomprov', 'ind_nuevo', 'renta', 'ind_actividad_cliente', 'canal_entrada'],\n",
    "#     'fm6': TARGET_LABELS_FRQ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "def create_RF(input_shape, output_shape):        \n",
    "    # https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "    return RandomForestClassifier(n_estimators=100, \n",
    "#                                   min_samples_split=100,\n",
    "#                                   min_samples_leaf=25,\n",
    "#                                   max_depth=10\n",
    "                                  max_features=1.0, \n",
    "#                                   oob_score=True,\n",
    "#                                   bootstrap=True,\n",
    "                                  n_jobs=-1\n",
    "                                 )\n",
    "\n",
    "def create_ET(input_shape, output_shape):\n",
    "    return ExtraTreesClassifier(n_estimators=100,\n",
    "#                                   min_samples_leaf=25,\n",
    "#                                   max_depth=10\n",
    "                                  max_features=1.0, \n",
    "                                  oob_score=True,\n",
    "                                  bootstrap=True,\n",
    "                                  n_jobs=-1\n",
    "\n",
    "                               )\n",
    "\n",
    "def create_GB(input_shape, output_shape):\n",
    "    return GradientBoostingClassifier(n_estimators=75)\n",
    "\n",
    "models_dict = {\n",
    "    'rf': create_RF,\n",
    "    'et': create_ET,\n",
    "    'gb': create_GB,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_0  <=>  [2, 3, 4, 5] <==> ['Current Accounts' 'Derivada Account' 'Payroll Account' 'Junior Account']\n",
      "lm_1  <=>  [2, 6, 7, 8] <==> ['Current Accounts' 'Mas particular Account' 'particular Account'\n",
      " 'particular Plus Account']\n",
      "lm_2  <=>  [2, 18, 23, 12] <==> ['Current Accounts' 'Credit Card' 'Direct Debit' 'e-account']\n",
      "lm_3  <=>  [21, 22] <==> ['Payroll' 'Pensions']\n",
      "lm_4  <=>  [2, 12, 18] <==> ['Current Accounts' 'e-account' 'Credit Card']\n",
      "lm_5  <=>  [2, 12, 23] <==> ['Current Accounts' 'e-account' 'Direct Debit']\n",
      "lm_6  <=>  [2, 18, 23] <==> ['Current Accounts' 'Credit Card' 'Direct Debit']\n",
      "lm_7  <=>  [18, 23, 21, 22] <==> ['Credit Card' 'Direct Debit' 'Payroll' 'Pensions']\n",
      "lm_8  <=>  [21, 23, 22, 4] <==> ['Payroll' 'Direct Debit' 'Pensions' 'Payroll Account']\n",
      "lm_9  <=>  [3, 4] <==> ['Derivada Account' 'Payroll Account']\n",
      "lm_10  <=>  [22, 7, 8, 23] <==> ['Pensions' 'particular Account' 'particular Plus Account' 'Direct Debit']\n",
      "lm_11  <=>  [0, 1, 14, 15, 17] <==> ['Saving Account' 'Guarantees' 'Mortgage' 'Pensions (plan fin)' 'Taxes']\n",
      "lm_others <=> [9, 10, 11, 13, 16, 19, 20] <==> ['Short-term deposits' 'Medium-term deposits' 'Long-term deposits' 'Funds'\n",
      " 'Loans' 'Securities' 'Home Account']\n",
      "{'lm_10': array(['ind_nom_pens_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n",
      "       'ind_recibo_ult1'], \n",
      "      dtype='|S17'), 'lm_11': array(['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_hip_fin_ult1',\n",
      "       'ind_plan_fin_ult1', 'ind_reca_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_8': array(['ind_nomina_ult1', 'ind_recibo_ult1', 'ind_nom_pens_ult1',\n",
      "       'ind_cno_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_9': array(['ind_cder_fin_ult1', 'ind_cno_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_others': array(['ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n",
      "       'ind_fond_fin_ult1', 'ind_pres_fin_ult1', 'ind_valo_fin_ult1',\n",
      "       'ind_viv_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_0': array(['ind_cco_fin_ult1', 'ind_cder_fin_ult1', 'ind_cno_fin_ult1',\n",
      "       'ind_ctju_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_1': array(['ind_cco_fin_ult1', 'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1',\n",
      "       'ind_ctpp_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_2': array(['ind_cco_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_recibo_ult1',\n",
      "       'ind_ecue_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_3': array(['ind_nomina_ult1', 'ind_nom_pens_ult1'], \n",
      "      dtype='|S17'), 'lm_4': array(['ind_cco_fin_ult1', 'ind_ecue_fin_ult1', 'ind_tjcr_fin_ult1'], \n",
      "      dtype='|S17'), 'lm_5': array(['ind_cco_fin_ult1', 'ind_ecue_fin_ult1', 'ind_recibo_ult1'], \n",
      "      dtype='|S17'), 'lm_6': array(['ind_cco_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_recibo_ult1'], \n",
      "      dtype='|S17'), 'lm_7': array(['ind_tjcr_fin_ult1', 'ind_recibo_ult1', 'ind_nomina_ult1',\n",
      "       'ind_nom_pens_ult1'], \n",
      "      dtype='|S17')}\n"
     ]
    }
   ],
   "source": [
    "NP_TARGET_LABELS = np.array(TARGET_LABELS)\n",
    "target_labels = NP_TARGET_LABELS\n",
    "\n",
    "common_groups = [\n",
    "##    [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "#     [2, ],\n",
    "    [2, 3, 4, 5],\n",
    "    [2, 6, 7, 8],\n",
    "    [2, 18, 23, 12], \n",
    "    [21, 22],\n",
    "    [2, 12, 18],\n",
    "    [2, 12, 23],\n",
    "    [2, 18, 23],\n",
    "    [18, 23, 21, 22],\n",
    "    [21, 23, 22, 4],\n",
    "#     [18, ],\n",
    "#     [12, ],\n",
    "#     [21, ],\n",
    "#     [22, ],\n",
    "#     [23, ],\n",
    "    [3, 4], \n",
    "    [22, 7, 8, 23],\n",
    "    [0, 1, 14, 15, 17]\n",
    "#     [17, ],\n",
    "#     [i] for i in range(24)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def flatten(array):\n",
    "    out = []\n",
    "    for item in array:\n",
    "        out += item\n",
    "    return out\n",
    "\n",
    "others = list(set(range(24)) - set(flatten(common_groups)))\n",
    "\n",
    "# for i, a in enumerate(zip(TARGET_LABELS2, TARGET_LABELS)):\n",
    "#     print i, a\n",
    "    \n",
    "s = set({})\n",
    "labels_masks_dict = {}\n",
    "for i, g in enumerate(common_groups):\n",
    "    print 'lm_%i' % i, \" <=> \", g, \"<==>\", TARGET_LABELS2[g]\n",
    "    labels_masks_dict['lm_%i' % i] = target_labels[g]\n",
    "    s |= set(g)\n",
    "print 'lm_others', \"<=>\", others, \"<==>\", TARGET_LABELS2[others]\n",
    "labels_masks_dict['lm_others'] = target_labels[others]\n",
    "s |= set(others)\n",
    "\n",
    "assert len(s) == len(target_labels), \"Sum is not equal 24, s=%i\" % s\n",
    "print labels_masks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'et': [(None, None, 'lm_10'),\n",
       "  (None, None, 'lm_11'),\n",
       "  (None, None, 'lm_8'),\n",
       "  (None, None, 'lm_9'),\n",
       "  (None, None, 'lm_others'),\n",
       "  (None, None, 'lm_0'),\n",
       "  (None, None, 'lm_1'),\n",
       "  (None, None, 'lm_2'),\n",
       "  (None, None, 'lm_3'),\n",
       "  (None, None, 'lm_4'),\n",
       "  (None, None, 'lm_5'),\n",
       "  (None, None, 'lm_6'),\n",
       "  (None, None, 'lm_7')],\n",
       " 'gb': [(None, None, 'lm_10'),\n",
       "  (None, None, 'lm_11'),\n",
       "  (None, None, 'lm_8'),\n",
       "  (None, None, 'lm_9'),\n",
       "  (None, None, 'lm_others'),\n",
       "  (None, None, 'lm_0'),\n",
       "  (None, None, 'lm_1'),\n",
       "  (None, None, 'lm_2'),\n",
       "  (None, None, 'lm_3'),\n",
       "  (None, None, 'lm_4'),\n",
       "  (None, None, 'lm_5'),\n",
       "  (None, None, 'lm_6'),\n",
       "  (None, None, 'lm_7')],\n",
       " 'rf': [(None, None, 'lm_10'),\n",
       "  (None, None, 'lm_11'),\n",
       "  (None, None, 'lm_8'),\n",
       "  (None, None, 'lm_9'),\n",
       "  (None, None, 'lm_others'),\n",
       "  (None, None, 'lm_0'),\n",
       "  (None, None, 'lm_1'),\n",
       "  (None, None, 'lm_2'),\n",
       "  (None, None, 'lm_3'),\n",
       "  (None, None, 'lm_4'),\n",
       "  (None, None, 'lm_5'),\n",
       "  (None, None, 'lm_6'),\n",
       "  (None, None, 'lm_7')]}"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {model_name: [(samples_mask_code, features_mask_name, labels_mask_name), ...]}\n",
    "models_pipelines = {\n",
    "    'gb' : [(None, None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) > 1],\n",
    "    'rf' : [(None, None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) > 1],\n",
    "    'et' : [(None, None, key) for key in labels_masks_dict if len(labels_masks_dict[key]) > 1],\n",
    "}\n",
    "models_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gb': [(None, 'fm5', 'lm_11'),\n",
       "  (None, 'fm5', 'lm_9'),\n",
       "  (None, 'fm5', 'lm_others'),\n",
       "  (None, 'fm5', 'lm_0'),\n",
       "  (None, 'fm5', 'lm_3'),\n",
       "  (None, 'fm5', 'lm_4'),\n",
       "  (None, 'fm2', 'lm_11'),\n",
       "  (None, 'fm2', 'lm_9'),\n",
       "  (None, 'fm2', 'lm_others'),\n",
       "  (None, 'fm2', 'lm_0'),\n",
       "  (None, 'fm2', 'lm_3'),\n",
       "  (None, 'fm3', 'lm_11'),\n",
       "  (None, 'fm3', 'lm_others'),\n",
       "  (None, 'fm3', 'lm_3'),\n",
       "  (None, 'fm0', 'lm_11'),\n",
       "  (None, 'fm0', 'lm_8'),\n",
       "  (None, 'fm0', 'lm_others'),\n",
       "  (None, 'fm0', 'lm_0'),\n",
       "  (None, 'fm0', 'lm_2'),\n",
       "  (None, 'fm0', 'lm_3'),\n",
       "  (None, 'fm0', 'lm_4'),\n",
       "  (None, 'fm0', 'lm_5'),\n",
       "  (None, 'fm0', 'lm_6'),\n",
       "  (None, 'fm0', 'lm_7'),\n",
       "  (None, 'fm1', 'lm_11'),\n",
       "  (None, 'fm1', 'lm_others'),\n",
       "  (None, 'fm1', 'lm_3')]}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    'gb': [(None, 'fm5', _lm) for _lm in ['lm_11', 'lm_9', 'lm_others', 'lm_0', 'lm_3', 'lm_4']] + \\\n",
    "    [(None, 'fm2', _lm) for _lm in ['lm_11', 'lm_9', 'lm_others', 'lm_0', 'lm_3']] + \\\n",
    "    [(None, 'fm3', _lm) for _lm in ['lm_11', 'lm_others', 'lm_3']] + \\\n",
    "    [(None, 'fm0', _lm) for _lm in ['lm_11', 'lm_8', 'lm_others', 'lm_0', 'lm_2', 'lm_3', 'lm_4', 'lm_5', 'lm_6', 'lm_7']] + \\\n",
    "    [(None, 'fm1', _lm) for _lm in ['lm_11', 'lm_others', 'lm_3']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from trainval import train_all, predict_all, probas_to_indices, score_estimators\n",
    "from utils import map7_score0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 75) (110000, 53) (110000, 27)\n",
      "(39988, 75) (39988, 53) (39988, 27)\n"
     ]
    }
   ],
   "source": [
    "ll = 110000\n",
    "# ll = 1100\n",
    "\n",
    "mask = X.index.isin(X.index[:ll])\n",
    "\n",
    "X1 = X[mask]\n",
    "Y1 = Y[mask]\n",
    "clc = clients_last_choice[mask]\n",
    "print X1.shape, Y1.shape, clc.shape\n",
    "\n",
    "mask = X.index.isin(X.index[ll:ll+ll//2])\n",
    "X2 = X[mask]\n",
    "Y2 = Y[mask]\n",
    "clc2 = clients_last_choice[mask]\n",
    "print X2.shape, Y2.shape, clc2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import dummies_to_decimal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def prepare_to_fit(X_train, Y_train):    \n",
    "    x_train = X_train.values\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    y_train = Y_train.apply(dummies_to_decimal, axis=1)\n",
    "    y_train = y_train.values    \n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def prepare_to_test(X_val, Y_val=None):\n",
    "    x_val = X_val.values\n",
    "    x_val = StandardScaler().fit_transform(x_val)\n",
    "    if Y_val is not None:\n",
    "        y_val = Y_val.apply(dummies_to_decimal, axis=1)\n",
    "        y_val = y_val.values \n",
    "    else:\n",
    "        y_val = None\n",
    "    return x_val, y_val\n",
    "\n",
    "\n",
    "def probas_to_labels_probas(y_probas, class_indices, labels):\n",
    "    l = len(labels)\n",
    "    out = np.zeros((len(y_probas), l))\n",
    "    _y_probas = class_indices[np.argmax(y_probas, axis=1)]\n",
    "    max_pr = np.max(y_probas, axis=1)\n",
    "    i = 0\n",
    "    for index, pr in zip(_y_probas, max_pr):\n",
    "        dummies_str = decimal_to_dummies(index, l)\n",
    "        out[i, :] = np.array([pr*float(v) for v in dummies_str])\n",
    "        i += 1\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_kwargs = {\n",
    "    'samples_masks_list': samples_masks_list, \n",
    "    'features_masks_dict': features_masks_dict, \n",
    "    'labels_masks_dict': labels_masks_dict, \n",
    "    'models_dict': models_dict,\n",
    "    'labels': target_labels,\n",
    "    'transform_proba_func': probas_to_indices,\n",
    "    'prepare_to_fit_func': prepare_to_fit,\n",
    "    'prepare_to_test_func': prepare_to_test,   \n",
    "    'probas_to_labels_probas_func': probas_to_labels_probas,\n",
    "    'threshold': 0.1,\n",
    "    'n_highest': 7,\n",
    "    'mode': 'sum',\n",
    "    'verbose': False,\n",
    "    'models_pipelines': models_pipelines,\n",
    "    'return_probas': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.441972\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.441972\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.303171\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.751287\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.751287\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.709779\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.508717\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.508717\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.362444\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.696663\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.696663\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.608501\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.768886\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.768886\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.737672\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.651835\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.652001\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.558360\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.509215\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.509215\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.389341\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.399137\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.399303\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.235431\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.731529\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.731529\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.671094\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.467209\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.467209\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.333555\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.451436\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.451436\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.289889\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.456085\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.456085\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.298522\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.471858\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.471858\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.325253\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.791632\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.791632\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.682384\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.973767\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.973767\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.897891\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.789806\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.789806\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.654823\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.943052\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.943052\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.907355\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.950523\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.950689\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.868006\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.914328\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.914328\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.870995\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.900216\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.900216\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.853395\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.720737\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.720737\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.583430\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.920305\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.920472\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.832309\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.803420\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.803420\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.680392\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.783165\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.783165\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.664785\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.744147\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.744147\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.580110\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.740827\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.740827\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.560518\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.350988\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.350988\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.286236\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.723062\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.723062\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.709779\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.422049\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.422049\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.357961\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.653661\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.653661\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.616636\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.756268\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.756268\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.739831\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.595052\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.595052\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.557197\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.441806\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.441806\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.388677\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.295036\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.295036\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.230616\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.703304\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.703304\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.675743\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.394156\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.394156\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.337539\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.359289\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.359289\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.292711\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.353810\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.353810\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.289225\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.375726\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.375892\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.313631\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999834\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.847418\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.999004\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999834\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.836128\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.988544\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.997177\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.970115\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999668\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.973601\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999668\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.803752\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.937573\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.871161\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999834\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.846754\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.792628\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.784991\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.294870\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.294870\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.288561\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.710609\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.710609\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.709115\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.372074\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.372074\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.363938\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.621949\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.621949\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.618795\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.738834\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.738834\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.736344\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.562012\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.562012\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.559024\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.386850\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.386850\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.377885\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.233771\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.233937\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.227129\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.679230\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.679064\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.673751\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.342686\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.342686\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.334717\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.300681\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.300681\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.292711\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.299352\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.299352\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.292877\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.327246\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.327246\n",
      "INFO:root:-- Process : sample_mask=6023/110000, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.322099\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.461187\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.461056\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.352588\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.762807\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.762938\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.730659\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.521302\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.521302\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.420674\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.684788\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.684658\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.598406\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.793779\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.793779\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.769211\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.591871\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.591871\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.476869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.516336\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.516336\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.404208\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.397020\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.397020\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.257318\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.752091\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.752091\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.708834\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.468636\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.468636\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.340695\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.430084\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.430084\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.280450\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.458312\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.458442\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.321354\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.521171\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.521171\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm5, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.427862\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.790382\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.790382\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.666362\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.976869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.976869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.897804\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.786722\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.786853\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.649111\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.926817\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.926817\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.892316\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.944590\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.944720\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.856116\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.850497\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.850366\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.802666\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.850497\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.850366\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.798745\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.719812\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.719681\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.575144\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.914401\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.914401\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.829979\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.792865\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.792865\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.674987\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.765682\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.765682\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.630946\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.740852\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.740852\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.574490\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.772739\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.772739\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.605463\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.393100\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.393100\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.345531\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.741636\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.741636\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.729744\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.461187\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.461187\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.421066\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.655384\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.655254\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.614349\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.783717\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.783717\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.770256\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.526529\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.526660\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.478045\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.446550\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.446681\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.393100\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.309592\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.309592\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.246472\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.739153\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.739153\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.716545\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.406952\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.406952\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.349059\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.349712\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.349712\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.286984\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.374412\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.374412\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.305802\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.463800\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.463800\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm3, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.426555\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.816257\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.999869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.821093\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.969812\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.985102\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.935311\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.941976\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.795086\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.926163\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.873366\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.834553\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.788683\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.782148\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.348798\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.348798\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.342656\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.731051\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.731051\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.730005\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.425379\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.425379\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.421197\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.620491\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.620491\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.616702\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.769733\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.769733\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.768557\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.497386\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.496994\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.489284\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.398719\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.398719\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.394668\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.260324\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.260193\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.249869\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.718244\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.718244\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.716414\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.357946\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.358207\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.349582\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.303189\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.303189\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.294302\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.327757\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.327757\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.315865\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.427339\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.427339\n",
      "INFO:root:-- Process : sample_mask=7652/110000, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='gb', fit accuracy : 0.423027\n"
     ]
    }
   ],
   "source": [
    "estimators = train_all(X1, Y1, **_kwargs)\n",
    "\n",
    "#print estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'et': 0.6738840369626834,\n",
       " 'gb': 0.59930223946783467,\n",
       " 'rf': 0.67389580325027343}"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = defaultdict(list)\n",
    "for e in estimators:\n",
    "    accuracies[e[0][2]].append(e[2])\n",
    "\n",
    "mean_accuracy = {}\n",
    "for key in accuracies:\n",
    "    accuracy_list = accuracies[key]\n",
    "    mean_accuracy[key] = sum(accuracy_list)/len(accuracy_list)\n",
    "    \n",
    "mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_10 -> 0.374562368711\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_10 -> 0.223066920076\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_10 -> 0.241197359208\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_11 -> 0.928528558568\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_11 -> 0.856006802041\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_11 -> 0.9711163349\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_8 -> 0.474742422727\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_8 -> 0.283785135541\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_8 -> 0.255801740522\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_9 -> 0.690007002101\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_9 -> 0.598729618886\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_9 -> 0.832549764929\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_others -> 0.917175152546\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_others -> 0.871511453436\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_others -> 0.936831049315\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_0 -> 0.64389316795\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_0 -> 0.623387016105\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_0 -> 0.759127738321\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_1 -> 0.521956586976\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_1 -> 0.464514354306\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_1 -> 0.676452935881\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_2 -> 0.100455136541\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_2 -> 0.073422026608\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_2 -> 0.0737471241372\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_3 -> 0.863934180254\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_3 -> 0.738071421426\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_3 -> 0.941282384715\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_4 -> 0.567295188557\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_4 -> 0.452935880764\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_4 -> 0.798264479344\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_5 -> 0.214289286786\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_5 -> 0.187881364409\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_5 -> 0.116685005502\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_6 -> 0.266930079024\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_6 -> 0.162673802141\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_6 -> 0.123387016105\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_7 -> 0.340827248174\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_7 -> 0.22921876563\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_7 -> 0.188381514454\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_10 -> 0.109857957387\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_10 -> 0.832424727418\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_10 -> 0.124262278684\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_11 -> 0.971391417425\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_11 -> 0.970866259878\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_11 -> 0.97066619986\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_8 -> 0.776407922377\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_8 -> 0.0502400720216\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_8 -> 0.0157797339202\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_9 -> 0.826397919376\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_9 -> 0.812393718115\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_9 -> 0.779633890167\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_others -> 0.960538161448\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_others -> 0.786285885766\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_others -> 0.94610883265\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_0 -> 0.818320496149\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_0 -> 0.0396869060718\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_0 -> 0.791687506252\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_1 -> 0.110458137441\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_1 -> 0.139616885066\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_1 -> 0.143643092928\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_2 -> 0.0661698509553\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_2 -> 0.0306591977593\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_2 -> 0.0383365009503\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_3 -> 0.941457437231\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_3 -> 0.83900170051\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_3 -> 0.847729318796\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_4 -> 0.803541062319\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_4 -> 0.0797989396819\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_4 -> 0.103155946784\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_5 -> 0.754451335401\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_5 -> 0.0462638791637\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_5 -> 0.0703461038311\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_6 -> 0.0829748924677\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_6 -> 0.0672451735521\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_6 -> 0.0623937181154\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_7 -> 0.0896268880664\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_7 -> 0.787386215865\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_7 -> 0.00920276082825\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_10 -> 0.294713414024\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_10 -> 0.262753826148\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_10 -> 0.233895168551\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_11 -> 0.750675202561\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_11 -> 0.659147744323\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_11 -> 0.855406621987\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_8 -> 0.168700610183\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_8 -> 0.163474042213\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_8 -> 0.129138741622\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_9 -> 0.425327598279\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_9 -> 0.429753926178\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_9 -> 0.306742022607\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_others -> 0.829048714614\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_others -> 0.705761728519\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_others -> 0.956762028609\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_0 -> 0.477243172952\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_0 -> 0.391142342703\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_0 -> 0.309717915375\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_1 -> 0.446408922677\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_1 -> 0.347154146244\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_1 -> 0.46411423427\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_2 -> 0.0744473342003\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_2 -> 0.0980294088226\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_2 -> 0.0901520456137\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_3 -> 0.639041712514\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_3 -> 0.614134240272\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_3 -> 0.908697609283\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_4 -> 0.292387716315\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_4 -> 0.23967190157\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_4 -> 0.397944383315\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_5 -> 0.0865009502851\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_5 -> 0.0994048214464\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_5 -> 0.0714714414324\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_6 -> 0.144393317995\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_6 -> 0.136490947284\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_6 -> 0.0840752225668\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_7 -> 0.218965689707\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_7 -> 0.252575772732\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_7 -> 0.18855656697\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_10 -> 0.323196959088\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_10 -> 0.0\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_10 -> 2.50075022507e-05\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_11 -> 0.999699909973\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_11 -> 0.999549864959\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_11 -> 0.999249774932\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_8 -> 0.930729218766\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_8 -> 0.939756927078\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_8 -> 0.942132639792\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_9 -> 0.983294988497\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_9 -> 0.994423326998\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_9 -> 0.993873161949\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_others -> 0.998474542363\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_others -> 0.997724317295\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_others -> 0.982919875963\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_0 -> 0.971141342403\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_0 -> 0.983395018506\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_0 -> 0.97566770031\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_1 -> 0.000100030009003\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_1 -> 0.0\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_1 -> 0.000100030009003\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_2 -> 0.697209162749\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_2 -> 0.938981694508\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_2 -> 0.93955686706\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_3 -> 0.983470041012\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_3 -> 0.982544763429\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_3 -> 0.983745123537\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_4 -> 0.976517955387\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_4 -> 0.966189856957\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_4 -> 0.979168750625\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_5 -> 0.533385015505\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_5 -> 0.944208262479\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_5 -> 0.946508952686\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_6 -> 0.492347704311\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_6 -> 0.945358607582\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_6 -> 0.947084125238\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_7 -> 0.619785935781\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_7 -> 0.938081424427\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_7 -> 0.938706611984\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_10 -> 0.545538661598\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_10 -> 0.125762728819\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_10 -> 0.112083625088\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_11 -> 0.915724717415\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_11 -> 0.595253576073\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_11 -> 0.973617085126\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_8 -> 0.196158847654\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_8 -> 0.108432529759\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_8 -> 0.117210163049\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_9 -> 0.710788236471\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_9 -> 0.515679703911\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_9 -> 0.307967390217\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_others -> 0.973342002601\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_others -> 0.936530959288\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_others -> 0.957262178654\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_0 -> 0.65622186656\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_0 -> 0.464589376813\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_0 -> 0.258377513254\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_1 -> 0.64344303291\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_1 -> 0.592577773332\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_1 -> 0.597579273782\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_2 -> 0.0702710813244\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_2 -> 0.0656196859058\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_2 -> 0.0506401920576\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_3 -> 0.934455336601\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_3 -> 0.750625187556\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_3 -> 0.944608382515\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_4 -> 0.285385615685\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_4 -> 0.235470641192\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_4 -> 0.350605181554\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_5 -> 0.0860508152446\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_5 -> 0.0669950985296\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_5 -> 0.0576923076923\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_6 -> 0.170176052816\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_6 -> 0.084600380114\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_6 -> 0.0749724917475\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_7 -> 0.458462538762\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_7 -> 0.104581374412\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_7 -> 0.101830549165\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_10 -> 0.592052615785\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_10 -> 0.231494448335\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_10 -> 0.468740622187\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_11 -> 0.940182054616\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_11 -> 0.886540962289\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_11 -> 0.972566770031\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_8 -> 0.645193558067\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_8 -> 0.361708512554\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_8 -> 0.760703210963\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_9 -> 0.7477743323\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_9 -> 0.641292387716\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_9 -> 0.906947084125\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_others -> 0.936630989297\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_others -> 0.846528958688\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_others -> 0.955211563469\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_0 -> 0.449659897969\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_0 -> 0.445858757627\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_0 -> 0.840527158147\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_1 -> 0.255251575473\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_1 -> 0.292512753826\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_1 -> 0.257177153146\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_2 -> 0.142992897869\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_2 -> 0.147094128238\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_2 -> 0.485920776233\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_3 -> 0.9200010003\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_3 -> 0.846653996199\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_3 -> 0.944308292488\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_4 -> 0.276657997399\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_4 -> 0.303466039812\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_4 -> 0.639841952586\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_5 -> 0.154871461438\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_5 -> 0.174052215665\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_5 -> 0.408197459238\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_6 -> 0.211738521556\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_6 -> 0.2522256677\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_6 -> 0.473392017605\n",
      "INFO:root:-- Score : model=et, features_mask=fm5, labels_mask=lm_7 -> 0.665599679904\n",
      "INFO:root:-- Score : model=rf, features_mask=fm5, labels_mask=lm_7 -> 0.397269180754\n",
      "INFO:root:-- Score : model=gb, features_mask=fm5, labels_mask=lm_7 -> 0.766579973992\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_10 -> 0.0303090927278\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_10 -> 0.798514554366\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_10 -> 0.736946083825\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_11 -> 0.944458337501\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_11 -> 0.924152245674\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_11 -> 0.783184955487\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_8 -> 0.755351605482\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_8 -> 0.789236771031\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_8 -> 0.733044913474\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_9 -> 0.829148744623\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_9 -> 0.900545163549\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_9 -> 0.909972991898\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_others -> 0.948384515355\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_others -> 0.880414124237\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_others -> 0.779008702611\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_0 -> 0.778708612584\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_0 -> 0.763854156247\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_0 -> 0.0794238271481\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_1 -> 0.0712713814144\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_1 -> 0.0799739921977\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_1 -> 0.0394618385516\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_2 -> 0.722991897569\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_2 -> 0.0290837251175\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_2 -> 0.0331849554866\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_3 -> 0.819645893768\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_3 -> 0.893993197959\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_3 -> 0.775807742323\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_4 -> 0.765604681404\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_4 -> 0.0654196258878\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_4 -> 0.736370911273\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_5 -> 0.0513904171251\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_5 -> 0.0276833049915\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_5 -> 0.744348304491\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_6 -> 0.757702310693\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_6 -> 0.0554166249875\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_6 -> 0.709937981394\n",
      "INFO:root:-- Score : model=et, features_mask=fm2, labels_mask=lm_7 -> 0.790087026108\n",
      "INFO:root:-- Score : model=rf, features_mask=fm2, labels_mask=lm_7 -> 0.80644193258\n",
      "INFO:root:-- Score : model=gb, features_mask=fm2, labels_mask=lm_7 -> 0.735695708713\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_10 -> 0.512903871161\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_10 -> 0.390967290187\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_10 -> 0.747724317295\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_11 -> 0.858857657297\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_11 -> 0.769655896769\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_11 -> 0.931879563869\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_8 -> 0.476242872862\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_8 -> 0.456211863559\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_8 -> 0.805416624987\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_9 -> 0.582824847454\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_9 -> 0.528733620086\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_9 -> 0.442932879864\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_others -> 0.895693708112\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_others -> 0.704186255877\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_others -> 0.965289586876\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_0 -> 0.438456536961\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_0 -> 0.382714814444\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_0 -> 0.269830949285\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_1 -> 0.342277683305\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_1 -> 0.265529658898\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_1 -> 0.18055416625\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_2 -> 0.197059117735\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_2 -> 0.158672601781\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_2 -> 0.481194358307\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_3 -> 0.86633490047\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_3 -> 0.751050315095\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_3 -> 0.930529158748\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_4 -> 0.267805341602\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_4 -> 0.234320296089\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_4 -> 0.471091327398\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_5 -> 0.18255476643\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_5 -> 0.210438131439\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_5 -> 0.172676803041\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_6 -> 0.320046013804\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_6 -> 0.292687806342\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_6 -> 0.402420726218\n",
      "INFO:root:-- Score : model=et, features_mask=fm3, labels_mask=lm_7 -> 0.582074622387\n",
      "INFO:root:-- Score : model=rf, features_mask=fm3, labels_mask=lm_7 -> 0.490272081624\n",
      "INFO:root:-- Score : model=gb, features_mask=fm3, labels_mask=lm_7 -> 0.78188456537\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_10 -> 0.339201760528\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_10 -> 7.5022506752e-05\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_10 -> 2.50075022507e-05\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_11 -> 0.999724917475\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_11 -> 0.999349804941\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_11 -> 0.999599879964\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_8 -> 0.943132939882\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_8 -> 0.943958187456\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_8 -> 0.94233269981\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_9 -> 0.993573071922\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_9 -> 0.992172651796\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_9 -> 0.993948184455\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_others -> 0.998499549865\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_others -> 0.997624287286\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_others -> 0.997399219766\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_0 -> 0.977568270481\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_0 -> 0.973717115135\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_0 -> 0.961538461538\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_1 -> 0.0194808442533\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_1 -> 0.00107532259678\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_1 -> 0.000150045013504\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_2 -> 0.921401420426\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_2 -> 0.936230869261\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_2 -> 0.919625887766\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_3 -> 0.985870761228\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_3 -> 0.980544163249\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_3 -> 0.984145243573\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_4 -> 0.973291987596\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_4 -> 0.970266079824\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_4 -> 0.972691807542\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_5 -> 0.921176352906\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_5 -> 0.945783735121\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_5 -> 0.943583074922\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_6 -> 0.93400520156\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_6 -> 0.942407722317\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_6 -> 0.941357407222\n",
      "INFO:root:-- Score : model=et, features_mask=fm0, labels_mask=lm_7 -> 0.918100430129\n",
      "INFO:root:-- Score : model=rf, features_mask=fm0, labels_mask=lm_7 -> 0.940932279684\n",
      "INFO:root:-- Score : model=gb, features_mask=fm0, labels_mask=lm_7 -> 0.887041112334\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_10 -> 0.806316895069\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_10 -> 0.500100030009\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_10 -> 0.669625887766\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_11 -> 0.973617085126\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_11 -> 0.973567070121\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_11 -> 0.973517055117\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_8 -> 0.77310693208\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_8 -> 0.451410423127\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_8 -> 0.789586876063\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_9 -> 0.784460338101\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_9 -> 0.474842452736\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_9 -> 0.310493147944\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_others -> 0.968365509653\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_others -> 0.804791437431\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_others -> 0.965089526858\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_0 -> 0.262028608583\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_0 -> 0.406296889067\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_0 -> 0.215064519356\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_1 -> 0.227068120436\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_1 -> 0.378938681604\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_1 -> 0.154321296389\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_2 -> 0.196283885166\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_2 -> 0.331249374812\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_2 -> 0.671251375413\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_3 -> 0.919325797739\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_3 -> 0.887016104831\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_3 -> 0.895918775633\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_4 -> 0.241422426728\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_4 -> 0.475792737821\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_4 -> 0.604256276883\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_5 -> 0.154246273882\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_5 -> 0.171851555467\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_5 -> 0.0960038011403\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_6 -> 0.225592677803\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_6 -> 0.388716614984\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_6 -> 0.26733019906\n",
      "INFO:root:-- Score : model=et, features_mask=fm1, labels_mask=lm_7 -> 0.824797439232\n",
      "INFO:root:-- Score : model=rf, features_mask=fm1, labels_mask=lm_7 -> 0.794763429029\n",
      "INFO:root:-- Score : model=gb, features_mask=fm1, labels_mask=lm_7 -> 0.833500050015\n"
     ]
    }
   ],
   "source": [
    "_ = score_estimators(estimators, X2, Y2, **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[(None, 'fm5', _lm) for _lm in ['lm_11', 'lm_9', 'lm_others', 'lm_0', 'lm_3', 'lm_4']] + \\\n",
    "[(None, 'fm2', _lm) for _lm in ['lm_11', 'lm_9', 'lm_others', 'lm_0', 'lm_3']] + \\\n",
    "[(None, 'fm3', _lm) for _lm in ['lm_11', 'lm_others', 'lm_3']] + \\\n",
    "[(None, 'fm0', _lm) for _lm in ['lm_11', 'lm_8', 'lm_others', 'lm_0', 'lm_2', 'lm_3', 'lm_4', 'lm_5', 'lm_6', 'lm_7']] + \\\n",
    "[(None, 'fm1', _lm) for _lm in ['lm_11', 'lm_others', 'lm_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:-- Predict all --\n"
     ]
    }
   ],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X2, **_kwargs)\n",
    "#print y_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind_ahor_fin_ult1</th>\n",
       "      <th>ind_aval_fin_ult1</th>\n",
       "      <th>ind_cco_fin_ult1</th>\n",
       "      <th>ind_cder_fin_ult1</th>\n",
       "      <th>ind_cno_fin_ult1</th>\n",
       "      <th>ind_ctju_fin_ult1</th>\n",
       "      <th>ind_ctma_fin_ult1</th>\n",
       "      <th>ind_ctop_fin_ult1</th>\n",
       "      <th>ind_ctpp_fin_ult1</th>\n",
       "      <th>ind_deco_fin_ult1</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>670965</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>0.105646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.014944</td>\n",
       "      <td>0.194729</td>\n",
       "      <td>0.011217</td>\n",
       "      <td>0.004091</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>0.132310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670966</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476599</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.067432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104801</td>\n",
       "      <td>0.082653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024866</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.012525</td>\n",
       "      <td>0.111593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670968</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125717</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.098438</td>\n",
       "      <td>0.118432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.039078</td>\n",
       "      <td>0.020597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050595</td>\n",
       "      <td>0.108789</td>\n",
       "      <td>0.177258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670972</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064459</td>\n",
       "      <td>0.066157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>0.032353</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120621</td>\n",
       "      <td>0.173420</td>\n",
       "      <td>0.224267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670973</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115957</td>\n",
       "      <td>0.008305</td>\n",
       "      <td>0.012054</td>\n",
       "      <td>0.105563</td>\n",
       "      <td>0.119755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015252</td>\n",
       "      <td>0.041141</td>\n",
       "      <td>0.020854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026096</td>\n",
       "      <td>0.082942</td>\n",
       "      <td>0.173538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ind_ahor_fin_ult1  ind_aval_fin_ult1  ind_cco_fin_ult1  \\\n",
       "670965                0.0                0.0          0.185343   \n",
       "670966                0.0                0.0          0.476599   \n",
       "670968                0.0                0.0          0.096411   \n",
       "670972                0.0                0.0          0.086864   \n",
       "670973                0.0                0.0          0.148467   \n",
       "\n",
       "        ind_cder_fin_ult1  ind_cno_fin_ult1  ind_ctju_fin_ult1  \\\n",
       "670965           0.000000          0.069844           0.000000   \n",
       "670966           0.004411          0.067432           0.000000   \n",
       "670968           0.000000          0.125717           0.008202   \n",
       "670972           0.000000          0.172941           0.000000   \n",
       "670973           0.000000          0.115957           0.008305   \n",
       "\n",
       "        ind_ctma_fin_ult1  ind_ctop_fin_ult1  ind_ctpp_fin_ult1  \\\n",
       "670965           0.000000           0.097959           0.105646   \n",
       "670966           0.000000           0.104801           0.082653   \n",
       "670968           0.003539           0.098438           0.118432   \n",
       "670972           0.000000           0.064459           0.066157   \n",
       "670973           0.012054           0.105563           0.119755   \n",
       "\n",
       "        ind_deco_fin_ult1       ...         ind_hip_fin_ult1  \\\n",
       "670965                0.0       ...                 0.011159   \n",
       "670966                0.0       ...                 0.000000   \n",
       "670968                0.0       ...                 0.010928   \n",
       "670972                0.0       ...                 0.005903   \n",
       "670973                0.0       ...                 0.011065   \n",
       "\n",
       "        ind_plan_fin_ult1  ind_pres_fin_ult1  ind_reca_fin_ult1  \\\n",
       "670965           0.011159           0.004247           0.014944   \n",
       "670966           0.000000           0.000000           0.000000   \n",
       "670968           0.010928           0.000000           0.015064   \n",
       "670972           0.005903           0.000000           0.008398   \n",
       "670973           0.011065           0.000000           0.015252   \n",
       "\n",
       "        ind_tjcr_fin_ult1  ind_valo_fin_ult1  ind_viv_fin_ult1  \\\n",
       "670965           0.194729           0.011217          0.004091   \n",
       "670966           0.024866           0.012335          0.018421   \n",
       "670968           0.039078           0.020597          0.000000   \n",
       "670972           0.032353           0.003635          0.000000   \n",
       "670973           0.041141           0.020854          0.000000   \n",
       "\n",
       "        ind_nomina_ult1  ind_nom_pens_ult1  ind_recibo_ult1  \n",
       "670965         0.036780           0.085365         0.132310  \n",
       "670966         0.009636           0.012525         0.111593  \n",
       "670968         0.050595           0.108789         0.177258  \n",
       "670972         0.120621           0.173420         0.224267  \n",
       "670973         0.026096           0.082942         0.173538  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18] [2] [5] [4, 21, 22, 23] [5] [2] [4, 23] [2] [4, 17, 21, 22, 23]\n",
      " [2, 4, 12, 18, 21, 22, 23]]\n",
      "[[18, 2, 23, 8] [2, 23, 7] [23, 4, 8, 22, 12] [23, 22, 4, 21]\n",
      " [23, 2, 8, 4, 7] [2, 23, 7] [23, 4, 2] [2, 23] [23, 22, 2, 4, 21]\n",
      " [23, 2, 4, 22]]\n"
     ]
    }
   ],
   "source": [
    "y_val = targets_str_to_indices(Y2[target_labels].values)\n",
    "print y_val[:10]\n",
    "print y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Compute max map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0295838751625\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.0142559434497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.014255943449701576"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = targets_str_to_indices(Y2[target_labels].values)\n",
    "\n",
    "logging.info(\"- Compute max map7 score\")\n",
    "map7_score(y_val, y_val, clc2[LC_TARGET_LABELS].values)\n",
    "# map7_score0(y_val, y_val)\n",
    "logging.info(\"- Compute map7 score\")\n",
    "map7_score(y_val, y_preds, clc2[LC_TARGET_LABELS].values)\n",
    "# map7_score0(y_val, y_preds)\n",
    "#logging.info(\"- Compute AUC ROC : \")\n",
    "#print roc_auc_score(y_val, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.021295269099703414 (GB on 'all')\n",
    "\n",
    "0.021271936353906683 (RF tunning)\n",
    "\n",
    "0.021668245671284416 (RF tunning)\n",
    "\n",
    "0.02136609107928888\n",
    "\n",
    "0.0211362663776694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print labels_masks_dict[estimators[0][0][1]]\n",
    "# print estimators[0][1].classes_\n",
    "# print estimators[0][1].n_classes_\n",
    "# print estimators[0][1].n_features_\n",
    "# print estimators[0][1].n_outputs_\n",
    "# print estimators[0][1].estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import targets_to_labels, targets_indices_to_labels, remove_last_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Count =  1\n",
      "['Credit Card']\n",
      "[]\n",
      "--- Count =  2\n",
      "['e-account']\n",
      "['Direct Debit']\n",
      "--- Count =  3\n",
      "['Current Accounts', 'e-account']\n",
      "[]\n",
      "--- Count =  4\n",
      "['Credit Card']\n",
      "[]\n",
      "--- Count =  5\n",
      "['Credit Card']\n",
      "['Pensions', 'Payroll']\n",
      "--- Count =  6\n",
      "['Payroll', 'Pensions']\n",
      "['Current Accounts', 'particular Account', 'particular Plus Account', 'Direct Debit']\n",
      "--- Count =  7\n",
      "['Payroll', 'Pensions']\n",
      "['Direct Debit', 'Current Accounts', 'particular Account', 'particular Plus Account']\n",
      "--- Count =  8\n",
      "['Payroll Account']\n",
      "['Current Accounts']\n",
      "--- Count =  9\n",
      "['e-account']\n",
      "[]\n",
      "--- Count =  10\n",
      "['Payroll', 'Pensions']\n",
      "['Current Accounts', 'particular Account', 'particular Plus Account']\n",
      "--- Count =  11\n",
      "['e-account', 'Payroll', 'Pensions']\n",
      "[]\n",
      "--- Count =  12\n",
      "['Credit Card']\n",
      "[]\n",
      "--- Count =  13\n",
      "['e-account']\n",
      "['Direct Debit', 'particular Account', 'particular Plus Account']\n",
      "--- Count =  14\n",
      "['Credit Card']\n",
      "[]\n",
      "--- Count =  15\n",
      "['Payroll Account', 'Payroll', 'Pensions']\n",
      "[]\n",
      "--- Count =  16\n",
      "['Payroll Account']\n",
      "['Direct Debit']\n",
      "--- Count =  17\n",
      "['Payroll', 'Pensions']\n",
      "[]\n",
      "--- Count =  18\n",
      "['Credit Card']\n",
      "[]\n",
      "--- Count =  19\n",
      "['Credit Card']\n",
      "[]\n",
      "--- Count =  20\n",
      "['Payroll Account']\n",
      "[]\n",
      "--- Count =  21\n",
      "['Payroll Account', 'Payroll', 'Pensions']\n",
      "['Direct Debit', 'particular Account']\n",
      "--- Count =  22\n",
      "['Payroll Account', 'Payroll', 'Pensions']\n",
      "['particular Account']\n",
      "--- Count =  23\n",
      "['Current Accounts']\n",
      "['Payroll Account']\n",
      "--- Count =  24\n",
      "['Credit Card']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "limit = 25\n",
    "count = 0\n",
    "\n",
    "not_predicted_predicted = defaultdict(int)\n",
    "for last_choice, targets, products, proba in zip(clc2[LC_TARGET_LABELS].values, y_val, y_preds, Y_probas.values):\n",
    "    added_products = remove_last_choice(targets, last_choice)\n",
    "    predictions = remove_last_choice(products, last_choice)\n",
    "#     print \"---\", count, last_choice\n",
    "#     print targets, '->', added_products\n",
    "#     print products, '->', predictions\n",
    "#     if count == 3:\n",
    "#         break\n",
    "    \n",
    "    if len(added_products) == 0:\n",
    "        continue\n",
    "        \n",
    "    if len(set(added_products) & set(predictions)) > 0:\n",
    "#         print \"Predicted : \", added_products, predictions\n",
    "#         print set(added_products) & set(predictions)\n",
    "        continue\n",
    "\n",
    "    count += 1\n",
    "    if count < limit:\n",
    "        print \"--- Count = \", count\n",
    "        print targets_indices_to_labels(added_products, TARGET_LABELS2)#, targets_indices_to_labels(targets, TARGET_LABELS2)\n",
    "        print targets_indices_to_labels(predictions, TARGET_LABELS2)#, targets_indices_to_labels(products, TARGET_LABELS2)#, proba\n",
    "    \n",
    "    for p in added_products:\n",
    "        not_predicted_predicted[TARGET_LABELS2[p]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'Securities': 1, 'Direct Debit': 19, 'e-account': 56, 'Payroll': 247, 'Pensions': 247, 'Taxes': 11, 'Payroll Account': 91, 'Long-term deposits': 1, 'Credit Card': 79, 'Current Accounts': 49}) 39988\n"
     ]
    }
   ],
   "source": [
    "print not_predicted_predicted, y_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print y_probas[:10, target_groups[0]]\n",
    "#print Y[np.array(TARGET_LABELS)[target_groups[0]]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFold Cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from trainval import cross_val_score0, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:- Cross validation : \n",
      "INFO:root:\n",
      "\n",
      "\t\t-- Fold : 1 / 5\n",
      "\n",
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.725203\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.725203\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.967224\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.967224\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.712610\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.712782\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.907366\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.907366\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.961359\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.961359\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.873900\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.873900\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.870623\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.870623\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.664309\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.664309\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.877005\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.877005\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.781784\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.781784\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.714680\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.714680\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.687942\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.687942\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.684319\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.684492\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999655\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999655\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999655\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999655\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999827\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.381577\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.381577\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.810937\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.810937\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.382267\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.382094\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.626186\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.626013\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.848370\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.848370\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.568397\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.568570\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.490426\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.490254\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.279627\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.279627\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.694842\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.694842\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.405037\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.405037\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.320511\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.320683\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.335518\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.335346\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.377954\n",
      "INFO:root:-- Process : sample_mask=5797/119990, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.377954\n",
      "INFO:root:-- Predict all --\n",
      "INFO:root:- Compute map7 score\n",
      "INFO:root:-- Predicted map7 score: 0.568432089161\n",
      "INFO:root:\n",
      "\n",
      "\t\t-- Fold : 2 / 5\n",
      "\n",
      "INFO:root:-- Train all --\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.748125\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.747965\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.966800\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.966800\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.735036\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.735036\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.912051\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.912051\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.957063\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.957063\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.880607\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.880447\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.879170\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.879170\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.690024\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.690184\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.885555\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.885555\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.798085\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.798085\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.735834\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.735994\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.712530\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.712530\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.709657\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm2, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.709657\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999681\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999681\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999362\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 1.000000\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999681\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.999840\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm0, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.999521\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.358659\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_10\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.358819\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.773025\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_11\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.773025\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.381644\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_8\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.381644\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.625379\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_9\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.625539\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.800160\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_others\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.800160\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.568236\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_0\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.568236\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.465922\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_1\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.465762\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.273743\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_2\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.273743\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.687630\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_3\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.687630\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.393136\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_4\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.393136\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.323543\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_5\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.323703\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.331205\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_6\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.331205\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='et', fit accuracy : 0.368396\n",
      "INFO:root:-- Process : sample_mask=6265/119990, features_mask=fm1, labels_mask=lm_7\n",
      "INFO:root:--- Score : model='rf', fit accuracy : 0.368236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-370-517ab4f0970b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnb_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Cross-Validation \\n %i | %f | %f | %f | %f | %.5f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnb_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/SPR/trees/trainval.py\u001b[0m in \u001b[0;36mcross_val_score0\u001b[0;34m(data, nb_folds, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'return_probas'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/SPR/trees/trainval.py\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(X_train, Y_train, samples_masks_list, features_masks_dict, labels_masks_dict, models_dict, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mlabels_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_masks_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels_mask_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mY_train__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabels_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mY_train_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_to_fit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-354-4ea95ebba9ea>\u001b[0m in \u001b[0;36mprepare_to_fit\u001b[0;34m(X_train, Y_train)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummies_to_decimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4059\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4060\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4061\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4062\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4115\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_agg_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m                     result = lib.reduce(values, func, axis=axis, dummy=dummy,\n\u001b[0;32m-> 4117\u001b[0;31m                                         labels=labels)\n\u001b[0m\u001b[1;32m   4118\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4119\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas.lib.reduce (pandas/lib.c:43539)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas.lib.Reducer.get_result (pandas/lib.c:33736)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/SPR/common/utils.py\u001b[0m in \u001b[0;36mdummies_to_decimal\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdummies_to_decimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummies_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vfomin/Documents/TDS/SPR/common/utils.py\u001b[0m in \u001b[0;36mdummies_to_str\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_folds = 5\n",
    "results = cross_val_score0((X, Y), nb_folds=nb_folds, **_kwargs)\n",
    "\n",
    "print \"Cross-Validation \\n %i | %f | %f | %f | %f | %.5f \" % (nb_folds, results.min(), results.mean(), np.median(results), results.max(), results.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 201505 -> 201605 \n",
    "\n",
    "Cross-Validation \n",
    " 5 | 0.014585 | 0.018385 | 0.019147 | 0.022227 | 0.00294 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute cross-validation across several months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_folds = 3\n",
    "yms = [201504, 201505]\n",
    "#yms = [201505]\n",
    "\n",
    "for ym in yms:\n",
    "    logging.info(\"\\n-------------------------\")\n",
    "    logging.info(\"- Process month : %s\" % ym)\n",
    "    logging.info(\"-------------------------\\n\")\n",
    "    \n",
    "    ym1 = ym + 100    \n",
    "    df1 = train_df if months_ym_map[ym] in train_months else val_df\n",
    "    df2 = train_df if months_ym_map[ym1] in train_months else val_df\n",
    "    X, Y, clients_last_choice = get_XY(ym, df1, ym1, df2) \n",
    "    results = cross_val_score2((X, Y, clients_last_choice[LC_TARGET_LABELS].values), \n",
    "                                profiles=profiles,\n",
    "                                nb_folds=nb_folds)\n",
    "    print \"Cross-Validation \\n %i | %f | %f | %f | %.5f \" % (nb_folds, results.min(), results.mean(), results.max(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201505\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = train_df\n",
    "#df1 = val_df\n",
    "df2 = train_df #if months_ym_map[next_year_month] in train_months else val_df\n",
    "#df2 = val_df\n",
    "\n",
    "X, Y, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators = train_all(X, Y, **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X, **_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check score on the data 2016/05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.info(\"- Compute map7 score\")\n",
    "print map7_score(y_val, y_preds, clients_last_choice[LC_TARGET_LABELS].values)\n",
    "logging.info(\"- Compute max map7 score\")\n",
    "print map7_score(y_val, y_val, clients_last_choice[LC_TARGET_LABELS].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction for 2016/06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_train_df, test_df = load_train_test([201506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months_ym_map = {}\n",
    "months = list(set(full_train_df['fecha_dato'].unique()) | set(test_df['fecha_dato'].unique()))\n",
    "for m in months:\n",
    "    months_ym_map[to_yearmonth(m)] = m\n",
    "    \n",
    "full_train_months = full_train_df['fecha_dato'].unique()\n",
    "test_months = test_df['fecha_dato'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_month = 201506\n",
    "next_year_month = current_month + 100\n",
    "\n",
    "df1 = full_train_df\n",
    "df2 = test_df\n",
    "X, _, clients_last_choice = get_XY(current_month, df1, next_year_month, df2, months_ym_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clients_last_choice.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_submission(predicted_added_products, clients, clc, target_labels):\n",
    "    added_products_col = []\n",
    "    count = 0 \n",
    "    for products, last_choice in zip(predicted_added_products, clc):\n",
    "        predictions = remove_last_choice(products, last_choice)\n",
    "        added_products_col.append(' '.join([target_labels[i] for i in predictions]))\n",
    "        count+=1\n",
    "        if count % 100000 == 0:\n",
    "            logging.info(\"Elapsed : %i\", count)\n",
    "            \n",
    "    out = pd.DataFrame(data={'ncodpers': clients, 'added_products': added_products_col}, columns=['ncodpers', 'added_products'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_preds, Y_probas = predict_all(estimators, X, **_kwargs)\n",
    "\n",
    "logging.info(\"- Get submission dataframe:\")\n",
    "clients = X['ncodpers'].values\n",
    "submission = get_submission(y_pred, clients, clients_last_choice[TARGET_LABELS].values, TARGET_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_clients = set(submission['ncodpers'].unique())\n",
    "test_clients = set(test_df['ncodpers'].unique())\n",
    "if submission_clients != test_clients:\n",
    "    missing_clients = list(test_clients - submission_clients)\n",
    "        \n",
    "#     selected_estimators = []\n",
    "#     for e in estimators:\n",
    "#         if e[0]\n",
    "        \n",
    "    \n",
    "    missing_added_products = np.zeros((len(missing_clients)))\n",
    "    submission = pd.concat([submission, \n",
    "                            pd.DataFrame(data={\n",
    "                                'ncodpers': missing_clients, \n",
    "                                'added_products': missing_added_products\n",
    "                            }, columns=['ncodpers', 'added_products'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get submission DataFrame and write csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print submission.shape\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "logging.info('- Generate submission')\n",
    "submission_file = '../results/submission_' + \\\n",
    "                  str(datetime.now().strftime(\"%Y-%m-%d-%H-%M\")) + \\\n",
    "                  '.csv'\n",
    "\n",
    "submission.to_csv(submission_file, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../results/submission_2016-11-17-16-37.csv', 'r') as r:\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    print r.readline()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
