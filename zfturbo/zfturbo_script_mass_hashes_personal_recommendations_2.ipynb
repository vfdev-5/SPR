{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'ZFTurbo: https://kaggle.com/zfturbo'\n",
    "\n",
    "\"\"\"\n",
    "Fork of ZFTurbo 'Mass hashes' code : https://www.kaggle.com/zfturbo/santander-product-recommendation/mass-hashes/code\n",
    "\n",
    "Added personal recommendations based on previous user's choices\n",
    "\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Project\n",
    "from zfturbo_script_mass_hashes_personal_recommendations import read_data, get_profiles, personal_recommendations_to_proba, common_recommendations_to_proba, get_target_labels, predict_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train/test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = \"../data/train_ver2_201601-201605.csv\"\n",
    "# test_filename = \"../data/test_ver2.csv\"\n",
    "test_filename = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute recommendations from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logging.info('--- Run solution ---')\n",
    "reader = open(train_filename, \"r\")\n",
    "target_labels = get_target_labels(reader.readline())\n",
    "\n",
    "nb_months_validation = 4\n",
    "\n",
    "(personal_recommendations_validation,\n",
    " common_recommendations_validation,\n",
    " product_stats_validation) = read_data(reader, 201601, nb_months_validation, get_profiles)\n",
    "\n",
    "logging.debug(\"-- common_recommendations_validation : %s \" % len(common_recommendations_validation))\n",
    "logging.debug(\"-- personal_recommendations_validation : %s \" % len(personal_recommendations_validation))\n",
    "logging.debug(\"-- product_stats_validation : %s \" % len(product_stats_validation))\n",
    "\n",
    "personal_recommendations = deepcopy(personal_recommendations_validation)\n",
    "common_recommendations = deepcopy(common_recommendations_validation)\n",
    "product_stats = deepcopy(product_stats_validation)\n",
    "\n",
    "(personal_recommendations,\n",
    " common_recommendations,\n",
    " product_stats,\n",
    " validation_data) = read_data(reader, 201605, 1, get_profiles,\n",
    "                              return_raw_data=True,\n",
    "                              personal_recommendations=personal_recommendations,\n",
    "                              common_recommendations=common_recommendations,\n",
    "                              product_stats=product_stats)\n",
    "\n",
    "logging.debug(\"-- common_recommendations : %s \" % len(common_recommendations))\n",
    "logging.debug(\"-- personal_recommendations : %s \" % len(personal_recommendations))\n",
    "logging.debug(\"-- product_stats : %s \" % len(product_stats))\n",
    "\n",
    "reader.close()\n",
    "\n",
    "common_recommendations_to_proba(common_recommendations)\n",
    "common_recommendations_to_proba(common_recommendations_validation)\n",
    "\n",
    "# Sort product stats:\n",
    "product_stats_validation = sorted(product_stats_validation.items(), key=itemgetter(1), reverse=True)\n",
    "product_stats = sorted(product_stats.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#personal_recommendations_to_proba(personal_recommendations, nb_months_validation)\n",
    "personal_recommendations_to_proba(personal_recommendations_validation, nb_months_validation+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Hashes num: ', len(common_recommendations))\n",
    "print('Hashes valid num: ', len(common_recommendations_validation))\n",
    "print('Valid part: ', len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare methods :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PERSONAL_RECOMMENDATIONS_WEIGHT = 0.0\n",
    "COMMON_RECOMMENDATIONS_WEIGHT = 1.0 - PERSONAL_RECOMMENDATIONS_WEIGHT\n",
    "\n",
    "ZFTURBO_COMMON_WEIGHT = 1.0\n",
    "MINE_COMMON_WEIGHT = 1.0 - ZFTURBO_COMMON_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_VERBOSE = False\n",
    "__VERBOSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _compute_product_probas(user, personal_recommendations, profiles, common_recommendations, personal_recommendations_weight):\n",
    "\n",
    "    common_predictions = _compute_predictions_from_common(common_recommendations, profiles)\n",
    "    personal_predictions = _compute_predictions_from_personal(user, personal_recommendations)\n",
    "\n",
    "    if common_predictions is not None and personal_predictions is not None:\n",
    "        return (1.0 - personal_recommendations_weight) * common_predictions + personal_recommendations_weight * personal_predictions\n",
    "    elif personal_predictions is not None:\n",
    "        return personal_predictions\n",
    "    elif common_predictions is not None:\n",
    "        return common_predictions\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _compute_predictions_from_personal(user, personal_recommendations):\n",
    "    personal_predictions = None\n",
    "    if user in personal_recommendations:\n",
    "        personal_predictions = personal_recommendations[user]['recommendations']\n",
    "\n",
    "    if _VERBOSE:\n",
    "        print \"\\n\\n Personal predictions: \", personal_predictions\n",
    "    return personal_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _compute_predictions_from_common(common_recommendations, profiles):\n",
    "    \"\"\"\n",
    "    Compute suggestion using profiles\n",
    "    \n",
    "    if len(profile1) > len(profile2) -> profile1 is more important than profile2\n",
    "    \n",
    "    :return: list of target indices sorted by descreasing importance with proba\n",
    "    \n",
    "    \"\"\"\n",
    "    target_weights = None\n",
    "    total_length = 0.0\n",
    "    total_count = 0\n",
    "    max_length = 0\n",
    "    # compute a total length to of participating profiles to define profile weight\n",
    "    for profile in profiles:\n",
    "        if profile in common_recommendations:\n",
    "            total_length += len(profile)\n",
    "            max_length = max(len(profile), max_length)\n",
    "            total_count += 1\n",
    "\n",
    "    if total_length > 0:\n",
    "        target_weights = np.zeros(24)\n",
    "\n",
    "    for profile in profiles:\n",
    "        if profile in common_recommendations:\n",
    "            profile_weight = len(profile) * 1.0 / total_length\n",
    "            # _common_recommendations[profile].items() -> [(target, proba)]\n",
    "            target_probas = sorted(common_recommendations[profile].items(), key=itemgetter(1), reverse=True)\n",
    "            #target_total_score = (24.0 + len(profile)) * total_count\n",
    "            target_total_score = (25.0 + max_length) * total_count\n",
    "            for i, target_proba in enumerate(target_probas):\n",
    "                target_score = 25.0 - i + len(profile)  # 24 + 'total'\n",
    "                if __VERBOSE:\n",
    "                    print \"-- i, target_score\", i, target_score\n",
    "                target = target_proba[0]\n",
    "                proba = target_proba[1]\n",
    "                if isinstance(target, int):\n",
    "                    p1 = common_recommendations[profile][target] * profile_weight * MINE_COMMON_WEIGHT\n",
    "                    p2 = target_score * 1.0 / target_total_score * ZFTURBO_COMMON_WEIGHT\n",
    "                    if __VERBOSE:\n",
    "                        print \"--- i, target, p1, p2, p1 + p2 : \", i, target, p1, p2, p1 + p2\n",
    "                    target_weights[target] += p1 + p2\n",
    "            if _VERBOSE:\n",
    "                print \"--> scored labels : {}\".format(np.argsort(target_weights)[::-1])\n",
    "                print \"---> target_weights : {}\".format(target_weights)\n",
    "\n",
    "    if _VERBOSE:\n",
    "        print \"\\n\\n Common predictions : \", target_weights\n",
    "    return target_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _compute_predictions(row, get_profiles_func,\n",
    "                        personal_recommendations,\n",
    "                        common_recommendations,\n",
    "                        product_stats,\n",
    "                        personal_recommendations_weight):\n",
    "    predicted = []\n",
    "    user = get_user(row)\n",
    "    profiles = get_profiles_func(row)\n",
    "\n",
    "    last_choice = None\n",
    "    if user in personal_recommendations:\n",
    "        last_choice = personal_recommendations[user]['last_choice']\n",
    "\n",
    "    if _VERBOSE:\n",
    "        print \"Last choice : \", last_choice\n",
    "    #\n",
    "    probas = _compute_product_probas(user,\n",
    "                                    personal_recommendations, profiles,\n",
    "                                    common_recommendations,\n",
    "                                    personal_recommendations_weight)\n",
    "\n",
    "    # Remove the products from the last choice : \n",
    "    if last_choice is not None and len(probas) > 0:\n",
    "        mask = np.abs(last_choice - 1)\n",
    "        probas *= mask\n",
    "\n",
    "    if _VERBOSE:\n",
    "        print \"\\n Probas (-last_choice) : \", probas\n",
    "    \n",
    "    #suggestions = np.argsort(probas)[::-1]  # equal elements do not have the correct ordering, dunno why\n",
    "    # probas -> int( probas  * 1000 ) before ordering\n",
    "    probas_int = (probas * 1000).astype(np.int)\n",
    "    suggestions = sorted(range(len(probas_int)), key=probas_int.__getitem__, reverse=True)\n",
    "\n",
    "    if _VERBOSE:\n",
    "        print \"\\n Suggestions  (-last_choice) - sorted : \", suggestions\n",
    "\n",
    "    # add 7 to predicted:\n",
    "    if len(predicted) < 7:\n",
    "        l = min(7, len(suggestions))\n",
    "        #predicted = suggestions[:l].tolist()\n",
    "        predicted = suggestions[:l]\n",
    "\n",
    "    if _VERBOSE:\n",
    "        print \"\\n- PREDICTED : \", predicted\n",
    "    # add suggestions from product_stats:\n",
    "    if len(predicted) < 7:\n",
    "        for product in product_stats:\n",
    "            # If user is not new\n",
    "            if last_choice is not None and last_choice[product[0]] == 1:\n",
    "                continue\n",
    "            if product[0] not in predicted:\n",
    "                predicted.append(product[0])\n",
    "                if len(predicted) == 7:\n",
    "                    break\n",
    "\n",
    "    \n",
    "    if _VERBOSE:\n",
    "        print \"FINAL PREDICTED : \", predicted \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from common import get_user, apk, get_real_values, get_choices\n",
    "import heapq\n",
    "\n",
    "def zfturbo_compute_predictions(row, get_profiles_func,\n",
    "                        best,\n",
    "                        personal_recommendations,\n",
    "                        product_stats):\n",
    "    predicted = []\n",
    "    user = get_user(row)\n",
    "    profiles = get_profiles_func(row)\n",
    "\n",
    "    last_choice = None\n",
    "    if user in personal_recommendations:\n",
    "        last_choice = personal_recommendations[user]['last_choice']\n",
    "\n",
    "    if _VERBOSE:\n",
    "        print \"Last choice : \", last_choice\n",
    "        \n",
    "    def _get_next_best_prediction(best, profiles, predicted, last_choice):\n",
    "        score = [0] * 24\n",
    "        for h in profiles:\n",
    "            if h in best:\n",
    "                if __VERBOSE:\n",
    "                    print \"-- profile : \", h\n",
    "                for i in range(len(best[h])):\n",
    "                    sc = 24 - i + len(h)\n",
    "                    index = best[h][i][0]\n",
    "                    if last_choice is not None:\n",
    "                        if last_choice[index] == 1:\n",
    "                            continue\n",
    "                    if index not in predicted:\n",
    "                        if __VERBOSE:\n",
    "                            print \"-- i, index, sc\", i, index, sc\n",
    "                        score[index] += sc\n",
    "                if _VERBOSE:\n",
    "                    print \"--> scored labels: \", np.argsort(score)[::-1] #, np.sort(score)[::-1][:7]\n",
    "                    print \"---> target_weights : {}\".format(score)\n",
    "        \n",
    "        if _VERBOSE:\n",
    "            print \"\\n -- score : \", score, np.argsort(score)[::-1]\n",
    "        \n",
    "        final = []\n",
    "        pred = heapq.nlargest(7, range(len(score)), score.__getitem__)\n",
    "        if _VERBOSE:\n",
    "            print \"\\n -- pred : \", pred\n",
    "        for i in range(7):\n",
    "            if score[pred[i]] > 0:\n",
    "                final.append(pred[i])\n",
    "                \n",
    "        if _VERBOSE:\n",
    "            print \"\\n -- final : \", final\n",
    "        return final\n",
    "\n",
    "    predicted = _get_next_best_prediction(best, profiles, predicted, last_choice)\n",
    "\n",
    "    if _VERBOSE:\n",
    "        print \"\\n- PREDICTED : \", predicted\n",
    "    # add suggestions from product_stats:\n",
    "    if len(predicted) < 7:\n",
    "        for product in product_stats:\n",
    "            # If user is not new\n",
    "            if last_choice is not None and last_choice[product[0]] == 1:\n",
    "                continue\n",
    "\n",
    "            if product[0] not in predicted:\n",
    "                predicted.append(product[0])\n",
    "                if len(predicted) == 7:\n",
    "                    break\n",
    "\n",
    "    \n",
    "    if _VERBOSE:\n",
    "        print \"FINAL PREDICTED : \", predicted\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zfturbo_predict_score(validation_data, get_profiles_func,\n",
    "                  common_recommendations,\n",
    "                  personal_recommendations,\n",
    "                  product_stats):\n",
    "    \n",
    "    logging.debug(\"-- zfturbo_predict_score\")\n",
    "    map7 = 0.0    \n",
    "    count = 25\n",
    "    for i, row in enumerate(validation_data):\n",
    "        predicted = zfturbo_compute_predictions(row, get_profiles_func,\n",
    "                                        common_recommendations,\n",
    "                                        personal_recommendations,\n",
    "                                        product_stats)\n",
    "        real = get_real_values(row, personal_recommendations)\n",
    "        score = apk(real, predicted)\n",
    "        if count > 0:\n",
    "            print \"-- i : \", i, row[1], \" score : \", score, \" | predicted : \", predicted, \", real : \", real\n",
    "        map7 += score\n",
    "    \n",
    "        count-=1\n",
    "        if count == 0:\n",
    "            break\n",
    "        \n",
    "    if len(validation_data) > 0:\n",
    "        map7 /= len(validation_data)\n",
    "\n",
    "    logging.debug(\"--- predict_score : map7=%s\" % map7)\n",
    "    return map7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def sort_common_recommendations(common_recommendations):\n",
    "    out = dict()\n",
    "    for b in common_recommendations:\n",
    "        arr = common_recommendations[b]\n",
    "        srtd = sorted(arr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        # remove 'total'\n",
    "        out[b] = [item for item in srtd if item[0] != 'total']\n",
    "    return out\n",
    "best_validation = sort_common_recommendations(common_recommendations_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare prediction methods on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from zfturbo_script_mass_hashes_personal_recommendations import parse_line, process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ZFTURBO_COMMON_WEIGHT = 1.0\n",
    "MINE_COMMON_WEIGHT = 1.0 - ZFTURBO_COMMON_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_VERBOSE=False\n",
    "__VERBOSE=False\n",
    "# def _predict_score(validation_data, get_profiles_func,\n",
    "#                   personal_recommendations,\n",
    "#                   common_recommendations,\n",
    "#                   product_stats,\n",
    "#                   personal_recommendations_weight):\n",
    "\n",
    "get_profiles_func = get_profiles\n",
    "_personal_recommendations = personal_recommendations_validation\n",
    "_common_recommendations = common_recommendations_validation\n",
    "_product_stats = product_stats_validation\n",
    "personal_recommendations_weight = 0.0\n",
    "\n",
    "logging.debug(\"-- predict_score : personal_recommendations_weight=%s\" % personal_recommendations_weight)\n",
    "map7_1 = 0.0   \n",
    "map7_2 = 0.0   \n",
    "count = 10000\n",
    "\n",
    "for i, row in enumerate(validation_data):\n",
    "\n",
    "    row = process_data(row)\n",
    "    predicted1 = _compute_predictions(row, get_profiles_func,\n",
    "                                    _personal_recommendations,\n",
    "                                    _common_recommendations,\n",
    "                                    _product_stats,\n",
    "                                    personal_recommendations_weight)\n",
    "\n",
    "    predicted2 = zfturbo_compute_predictions(row, get_profiles_func,\n",
    "                                        best_validation,\n",
    "                                        _personal_recommendations,\n",
    "                                        _product_stats)\n",
    "    \n",
    "    real = get_real_values(row, _personal_recommendations)\n",
    "    score1 = apk(real, predicted1)\n",
    "    score2 = apk(real, predicted2)\n",
    "    if count > 0 and score1 != score2:\n",
    "        print \"-- i : \", i, row[1]\n",
    "        print \"--- p1 : \", score1, predicted1, real\n",
    "        print \"--- p2 : \", score2, predicted2\n",
    "    map7_1 += score1\n",
    "    map7_2 += score2\n",
    "    \n",
    "    count -= 1\n",
    "    if count == 0:\n",
    "        break\n",
    "\n",
    "if len(validation_data) > 0:\n",
    "    map7_1 /= len(validation_data)\n",
    "    map7_2 /= len(validation_data)\n",
    "\n",
    "print map7_1, map7_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare predicitions on a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_VERBOSE=True\n",
    "__VERBOSE=False\n",
    "row = validation_data[1061]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zfturbo_compute_predictions(row, get_profiles, best_validation,\n",
    "                        personal_recommendations_validation,\n",
    "                        product_stats_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "personal_recommendations_weight = 0.0\n",
    "_compute_predictions(row, get_profiles,\n",
    "                        personal_recommendations_validation,\n",
    "                        common_recommendations_validation,\n",
    "                        product_stats_validation,\n",
    "                        personal_recommendations_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test = [0, 0, 302, 0, 0, 0, 173, 218, 0, 0, 86, 204, 255, 157, 80, 145, 61, 246, 256, 187, 125, 292, 291, 0]\n",
    "test = [ 0.  ,        0.   ,       0.80606061 , 0.  ,        0.72727273 , 0.80606061,\n",
    "  0.46666667 , 0.    ,      0.    ,      0.     ,     0.     ,     0.6030303,\n",
    "  0.77878788 , 0.46969697  ,0.  ,        0.       ,   0.26060606 , 0.54545455,\n",
    "  0.54242424 , 0.46969697 , 0.  ,        0.58787879  ,0.61212121 , 0.72727273,]\n",
    "\n",
    "print np.argsort(test)[::-1][:7]\n",
    "print heapq.nlargest(7, range(len(test)), test.__getitem__)\n",
    "print sorted(range(len(test)), key=test.__getitem__, reverse=True)[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print test[22], test[18], test[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = '../data/test_ver2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "reader = open(test_file, 'r')\n",
    "reader.readline()\n",
    "total = 0\n",
    "limit = 1000\n",
    "while 1:\n",
    "    line = reader.readline()[:-1]\n",
    "    total += 1\n",
    "\n",
    "    if line == '':\n",
    "        break\n",
    "\n",
    "    row = parse_line(line)    \n",
    "    test_data.append(row)\n",
    "\n",
    "    if total > limit:\n",
    "        break\n",
    "    \n",
    "    if total % 100000 == 0:\n",
    "        logging.info('Read {} lines'.format(total))\n",
    "    \n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_VERBOSE=False\n",
    "__VERBOSE=False\n",
    "# def _predict_score(validation_data, get_profiles_func,\n",
    "#                   personal_recommendations,\n",
    "#                   common_recommendations,\n",
    "#                   product_stats,\n",
    "#                   personal_recommendations_weight):\n",
    "\n",
    "get_profiles_func = get_profiles\n",
    "_personal_recommendations = personal_recommendations_validation\n",
    "_common_recommendations = common_recommendations_validation\n",
    "_product_stats = product_stats_validation\n",
    "personal_recommendations_weight = 0.0\n",
    "\n",
    "logging.debug(\"-- predict_score : personal_recommendations_weight=%s\" % personal_recommendations_weight)\n",
    "count = 5\n",
    "\n",
    "for i, row in enumerate(test_data):\n",
    "\n",
    "    prow = process_data(row)\n",
    "    predicted1 = _compute_predictions(prow, get_profiles_func,\n",
    "                                    _personal_recommendations,\n",
    "                                    _common_recommendations,\n",
    "                                    _product_stats,\n",
    "                                    personal_recommendations_weight)\n",
    "\n",
    "    predicted2 = zfturbo_compute_predictions(row, get_profiles_func,\n",
    "                                        best_validation,\n",
    "                                        _personal_recommendations,\n",
    "                                        _product_stats)\n",
    "    \n",
    "    #if count > 0 and predicted1 != predicted2:\n",
    "    if True:\n",
    "        print \"-- i : \", i, row[1]\n",
    "        print \"--- p1 : \", predicted1, [target_labels[i] for i in predicted1]\n",
    "        print \"--- p2 : \", predicted2, [target_labels[i] for i in predicted2]\n",
    "    \n",
    "    count -= 1\n",
    "    if count == 0:\n",
    "        break\n",
    "\n",
    "print \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
